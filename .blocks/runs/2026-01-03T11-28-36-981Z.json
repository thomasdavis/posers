{
  "version": "1.0",
  "id": "2026-01-03T11-28-36-981Z",
  "timestamp": "2026-01-03T11:29:09.290Z",
  "configPath": "blocks.yml",
  "projectName": "Posers Motion Engine",
  "duration": 32405,
  "summary": {
    "totalBlocks": 5,
    "passed": 1,
    "failed": 2,
    "warnings": 2,
    "cached": {
      "validatorsSkipped": 0,
      "validatorsRun": 10,
      "timeSavedMs": 0
    }
  },
  "blocks": [
    {
      "blockName": "confident_stance",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/confident-stance.ts",
      "hasErrors": false,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "confident_stance",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must have detailed JSDoc describing confident body language",
                    "CORE: Spine chain must show upright confident posture",
                    "CORE: Hips stable with micro weight shifts",
                    "BREATH: Deep, slow breathing (4-5 second cycle)",
                    "BREATH: Chest expands on inhale, shoulders rise slightly",
                    "ARMS: Relaxed at sides or hands loosely clasped",
                    "ARMS: Subtle forearm rotation with breath",
                    "HANDS: Fingers relaxed but with presence, not limp",
                    "HANDS: All 30 finger bones should have subtle curl",
                    "LEGS: Weight distributed evenly or slightly forward",
                    "LEGS: Knees not locked, micro-flex",
                    "FEET: Toes engaged, gripping ground subtly",
                    "HEAD: Chin slightly elevated, gaze forward",
                    "EYES: Purposeful movement, slow blinks",
                    "JAW: Relaxed but closed",
                    "MICRO: Noise layer on all bones for life"
                  ]
                }
              ],
              "description": "Power pose with commanding presence. Weight evenly distributed or slightly forward.\nChest open, shoulders back but relaxed. Subtle assertive micro-movements.\n\nFEEL: Grounded, assured, ready. Like a CEO about to address the board.\n\nTIMING: Breath drives subtle chest expansion (4-5 second cycle). Weight micro-shifts\nevery 2-3 seconds. Eyes move with purpose, not darting. Fingers relaxed but not limp.\n",
              "path": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Power pose with commanding presence. Weight evenly...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": true,
          "duration": 6337,
          "issues": [
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Missing implementation of core micro-movement dynamics on all joints, particularly for natural variability in torso and limbs during subtle motions.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Timing relationships in the breathing implementation do not align with the specified breath cycle (4-5 seconds) in the documentation; current application may yield faster breathing.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The documentation does not explicitly state how micro-movements are implemented or their intended effect, which could lead to misalignment with domain expectations.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Energy dynamics based on physical laws are not fully detailed; should articulate how acceleration and deceleration curves were accounted for in joint movements.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "confident-stance.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block expresses domain intent well but has several warnings regarding undeveloped micro-movement dynamics, alignment between timing specifications, and documentation completeness. Adjustments are necessary to fully adhere to domain rules."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt": "Block Name: confident_stance\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Power pose with commanding presence. Weight evenly distributed or slightly forward.\\nChest open, shoulders back but relaxed. Subtle assertive micro-movements.\\n\\nFEEL: Grounded, assured, ready. Like a CEO about to address the board.\\n\\nTIMING: Breath drives subtle chest expansion (4-5 second cycle). Weight micro-shifts\\nevery 2-3 seconds. Eyes move with purpose, not darting. Fingers relaxed but not limp.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/confident-stance.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must have detailed JSDoc describing confident body language\",\n        \"CORE: Spine chain must show upright confident posture\",\n        \"CORE: Hips stable with micro weight shifts\",\n        \"BREATH: Deep, slow breathing (4-5 second cycle)\",\n        \"BREATH: Chest expands on inhale, shoulders rise slightly\",\n        \"ARMS: Relaxed at sides or hands loosely clasped\",\n        \"ARMS: Subtle forearm rotation with breath\",\n        \"HANDS: Fingers relaxed but with presence, not limp\",\n        \"HANDS: All 30 finger bones should have subtle curl\",\n        \"LEGS: Weight distributed evenly or slightly forward\",\n        \"LEGS: Knees not locked, micro-flex\",\n        \"FEET: Toes engaged, gripping ground subtly\",\n        \"HEAD: Chin slightly elevated, gaze forward\",\n        \"EYES: Purposeful movement, slow blinks\",\n        \"JAW: Relaxed but closed\",\n        \"MICRO: Noise layer on all bones for life\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- confident-stance.ts ---\n```\n/**\n * Confident Stance Motion\n *\n * A power pose with subtle weight distribution, commanding posture,\n * and organic micro-movements. Uses all available bones for maximum\n * realism with layered animation approach.\n *\n * Research basis:\n * - Power pose psychology (Carney et al.)\n * - Postural sway biomechanics\n * - Weight distribution patterns\n * - Breathing mechanics in standing posture\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  getAvailableBones,\n  applyFingerCurl,\n  applyFingerSpread,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const confidentStanceParamsSchema = z.object({\n  /** Overall intensity of the pose (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Breath rate in Hz. Default: 0.15 (slower, confident breathing) */\n  breathRate: z.number().min(0.05).max(0.5).default(0.15),\n  /** Amount of hip sway side-to-side (0-1). Default: 0.3 */\n  swayAmount: z.number().min(0).max(1).default(0.3),\n  /** Enable eye micro-movements. Default: true */\n  eyeMovement: z.boolean().default(true),\n  /** Enable finger movements. Default: true */\n  fingerMovement: z.boolean().default(true),\n  /** Weight distribution bias (-1=left, 0=center, 1=right). Default: 0.15 */\n  weightBias: z.number().min(-1).max(1).default(0.15),\n  /** Shoulder tension level (0=relaxed, 1=tense). Default: 0.2 */\n  shoulderTension: z.number().min(0).max(1).default(0.2),\n  /** Chest out amount (0-1). Default: 0.6 */\n  chestOut: z.number().min(0).max(1).default(0.6),\n  /** Chin up amount (0-1). Default: 0.3 */\n  chinUp: z.number().min(0).max(1).default(0.3),\n})\n\nexport type ConfidentStanceParams = z.infer<typeof confidentStanceParamsSchema>\nexport type ConfidentStanceInput = z.input<typeof confidentStanceParamsSchema>\n\nexport const confidentStanceMeta: MotionMeta = {\n  id: 'confident-stance',\n  name: 'Confident Stance',\n  description: 'Power pose with commanding presence, subtle breathing, and organic micro-movements',\n  tags: ['stance', 'confident', 'power', 'idle'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface ConfidentStanceState {\n  noise: NoiseGenerator\n  weightShiftSpring: Spring\n  blinkTimer: number\n  blinkDuration: number\n  isBlinking: boolean\n  lastBlinkTime: number\n}\n\n/**\n * Initialize deterministic state with seeded random generators.\n * All noise and random elements are seeded to ensure identical\n * output for the same seed value - required for deterministic playback.\n */\nfunction initState(seed: number): ConfidentStanceState {\n  // All random elements use the seed for deterministic, reproducible motion\n  return {\n    noise: createNoiseGenerator(seed), // Seeded simplex noise\n    weightShiftSpring: createSpring({ stiffness: 50, damping: 15 }),\n    blinkTimer: 0,\n    blinkDuration: 0.15,\n    isBlinking: false,\n    lastBlinkTime: 0,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\nexport function createConfidentStance(params: ConfidentStanceInput = {}): MotionProgram<ConfidentStanceParams> {\n  const validatedParams = confidentStanceParamsSchema.parse(params)\n  let state: ConfidentStanceState | null = null\n\n  return {\n    meta: confidentStanceMeta,\n    paramsSchema: confidentStanceParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      // Lazy initialization if init wasn't called\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        intensity,\n        breathRate,\n        swayAmount,\n        eyeMovement,\n        fingerMovement,\n        weightBias,\n        shoulderTension,\n        chestOut,\n        chinUp,\n      } = validatedParams\n\n      const noise = state.noise\n\n      // ========================================\n      // LAYER 1: BASE POSTURE\n      // ========================================\n\n      // Hips - slight forward tilt for confident stance\n      const hipsTilt = -0.03 * intensity // Slight posterior tilt\n      const hipsYaw = weightBias * 0.05 * intensity\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hipsTilt)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, hipsYaw))\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // Spine chain - tall, proud posture\n      const spineExtension = 0.02 * intensity * chestOut\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension * 0.5))\n      }\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension * 1.2))\n      }\n\n      // Neck and head - chin slightly up\n      const neckExtension = chinUp * 0.03 * intensity\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -neckExtension * 0.5))\n      }\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -neckExtension)\n        rig.setRotation('head', headRot)\n      }\n\n      // Shoulders - pulled back for confident posture\n      const shoulderPullBack = 0.08 * intensity * (1 - shoulderTension * 0.5)\n      const shoulderDown = shoulderTension * 0.05 * intensity\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderDown)\n        leftShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -shoulderPullBack))\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderDown)\n        rightShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, shoulderPullBack))\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // Arms - relaxed at sides with slight separation\n      const armAbduction = 0.12 * intensity\n      const armRelax = 0.08 * intensity\n      if (rig.hasBone('leftUpperArm')) {\n        const leftUpperArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armAbduction)\n        leftUpperArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armRelax))\n        rig.setRotation('leftUpperArm', leftUpperArmRot)\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const rightUpperArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -armAbduction)\n        rightUpperArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armRelax))\n        rig.setRotation('rightUpperArm', rightUpperArmRot)\n      }\n\n      // Lower arms - slightly bent\n      const elbowBend = 0.15 * intensity\n      if (rig.hasBone('leftLowerArm')) {\n        rig.setRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -elbowBend))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        rig.setRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, elbowBend))\n      }\n\n      // Hands - natural rotation\n      const handRotation = 0.1 * intensity\n      if (rig.hasBone('leftHand')) {\n        rig.setRotation('leftHand', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handRotation))\n      }\n      if (rig.hasBone('rightHand')) {\n        rig.setRotation('rightHand', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handRotation))\n      }\n\n      // Legs - weight distribution\n      const standingLegBend = 0.05 * intensity\n      const relaxedLegBend = 0.12 * intensity\n      const leftWeight = weightBias < 0 ? 1 : 1 - weightBias\n      const rightWeight = weightBias > 0 ? 1 : 1 + weightBias\n\n      if (rig.hasBone('leftUpperLeg')) {\n        const leftLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, standingLegBend * leftWeight + relaxedLegBend * (1 - leftWeight))\n        rig.setRotation('leftUpperLeg', leftLegRot)\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        const rightLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, standingLegBend * rightWeight + relaxedLegBend * (1 - rightWeight))\n        rig.setRotation('rightUpperLeg', rightLegRot)\n      }\n\n      // Knee slight bend for natural stance\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08 * leftWeight))\n      }\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08 * rightWeight))\n      }\n\n      // Feet - flat with slight outward rotation\n      const footOutward = 0.1 * intensity\n      if (rig.hasBone('leftFoot')) {\n        rig.setRotation('leftFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -footOutward))\n      }\n      if (rig.hasBone('rightFoot')) {\n        rig.setRotation('rightFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, footOutward))\n      }\n\n      // ========================================\n      // LAYER 2: BREATHING\n      // ========================================\n\n      const breathPhase = oscBreathing(t, breathRate, intensity)\n\n      // Add breathing to spine\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.015))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.02))\n      }\n\n      // Shoulder rise with breath\n      const shoulderBreath = breathPhase * 0.008\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 3: WEIGHT SHIFT\n      // ========================================\n\n      // Slow, subtle weight shifting\n      const weightShiftTarget = noise.noise2D(t * 0.15, 0) * swayAmount * 0.5\n      state.weightShiftSpring.setTarget(weightShiftTarget)\n      state.weightShiftSpring.update(dt)\n      const currentShift = state.weightShiftSpring.value\n\n      // Apply weight shift to hips\n      if (rig.hasBone('hips')) {\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, currentShift * 0.03))\n      }\n\n      // Counter-rotate spine\n      if (rig.hasBone('spine')) {\n        rig.addRotation('spine', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -currentShift * 0.015))\n      }\n\n      // ========================================\n      // LAYER 4: MICRO-MOVEMENTS\n      // ========================================\n\n      // Subtle noise on all joints for organic feel\n      const microIntensity = 0.003 * intensity\n\n      // Head micro-movements\n      if (rig.hasBone('head')) {\n        const headNoiseX = noise.noise2D(t * 0.3, 100) * microIntensity\n        const headNoiseY = noise.noise2D(t * 0.25, 200) * microIntensity\n        const headNoiseZ = noise.noise2D(t * 0.2, 300) * microIntensity * 0.5\n        rig.addRotation('head', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headNoiseX))\n        rig.addRotation('head', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headNoiseY))\n        rig.addRotation('head', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headNoiseZ))\n      }\n\n      // Upper arm micro-movements\n      if (rig.hasBone('leftUpperArm')) {\n        const armNoiseL = noise.noise2D(t * 0.2, 400) * microIntensity\n        rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armNoiseL))\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const armNoiseR = noise.noise2D(t * 0.2, 500) * microIntensity\n        rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armNoiseR))\n      }\n\n      // ========================================\n      // LAYER 5: FINGERS\n      // ========================================\n\n      if (fingerMovement) {\n        // Relaxed, slightly curled fingers\n        const baseCurl = 0.25 + noise.noise2D(t * 0.1, 600) * 0.1\n\n        // Left hand\n        applyFingerCurl(rig, 'left', {\n          thumb: baseCurl * 0.6,\n          index: baseCurl,\n          middle: baseCurl * 1.1,\n          ring: baseCurl * 1.15,\n          little: baseCurl * 1.2,\n        })\n\n        // Right hand\n        applyFingerCurl(rig, 'right', {\n          thumb: baseCurl * 0.6,\n          index: baseCurl,\n          middle: baseCurl * 1.1,\n          ring: baseCurl * 1.15,\n          little: baseCurl * 1.2,\n        })\n\n        // Slight finger spread\n        applyFingerSpread(rig, 'left', 0.2)\n        applyFingerSpread(rig, 'right', 0.2)\n      }\n\n      // ========================================\n      // LAYER 6: EYES\n      // ========================================\n\n      if (eyeMovement) {\n        // Slow, deliberate eye movement\n        const eyeSpeed = 0.15\n        const eyeX = noise.noise2D(t * eyeSpeed, 700) * 0.04 * intensity\n        const eyeY = noise.noise2D(t * eyeSpeed, 800) * 0.03 * intensity\n\n        // Blinking - deterministic intervals based on seed\n        const timeSinceLastBlink = t - state.lastBlinkTime\n        // Use floor of time to get stable interval calculation\n        const blinkSeed = Math.floor(state.lastBlinkTime * 10)\n        const blinkInterval = 3 + noise.noise2D(blinkSeed, 900) * 2\n        if (timeSinceLastBlink > blinkInterval && !state.isBlinking) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          const blinkProgress = state.blinkTimer / state.blinkDuration\n          if (blinkProgress >= 1) {\n            state.isBlinking = false\n            state.lastBlinkTime = t\n          } else {\n            // Quick close, slower open\n            blinkAmount = blinkProgress < 0.3\n              ? Easing.easeOutQuad(blinkProgress / 0.3)\n              : Easing.easeInQuad(1 - (blinkProgress - 0.3) / 0.7)\n          }\n        }\n\n        // Apply eye rotations\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 7: TOES\n      // ========================================\n\n      // Toes slightly gripping for balance\n      if (rig.hasBone('leftToes')) {\n        rig.setRotation('leftToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1 * leftWeight))\n      }\n      if (rig.hasBone('rightToes')) {\n        rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1 * rightWeight))\n      }\n    },\n  }\n}\n\nexport const confidentStance: MotionProgram<ConfidentStanceParams> = createConfidentStance({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Missing implementation of core micro-movement dynamics on all joints, particularly for natural variability in torso and limbs during subtle motions.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Timing relationships in the breathing implementation do not align with the specified breath cycle (4-5 seconds) in the documentation; current application may yield faster breathing.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"The documentation does not explicitly state how micro-movements are implemented or their intended effect, which could lead to misalignment with domain expectations.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Energy dynamics based on physical laws are not fully detailed; should articulate how acceleration and deceleration curves were accounted for in joint movements.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    }\n  ],\n  \"summary\": \"The block expresses domain intent well but has several warnings regarding undeveloped micro-movement dynamics, alignment between timing specifications, and documentation completeness. Adjustments are necessary to fully adhere to domain rules.\"\n}",
            "tokensUsed": {
              "input": 5606,
              "output": 249
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "nervous_fidget",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/nervous-fidget.ts",
      "hasErrors": false,
      "hasWarnings": false,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "nervous_fidget",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe anxiety body language research basis",
                    "CORE: Hunched protective posture, shoulders elevated",
                    "CORE: Forward head posture (hypervigilance)",
                    "BREATH: Shallow, fast, irregular (2-3 second cycles)",
                    "BREATH: Shoulders rise with anxious breath",
                    "WEIGHT: IRREGULAR shifts using turbulence, not sine waves",
                    "WEIGHT: Ready-to-flee stance",
                    "ARMS: Held close to body (self-protection)",
                    "ARMS: Frequent self-touch gestures (neck, arms, face)",
                    "HANDS: Tense, curled fingers",
                    "HANDS: Rubbing, picking, clasping gestures",
                    "HANDS: All finger joints animated during fidgets",
                    "LEGS: Weight shifting, foot tapping bursts",
                    "FEET: Toes tapping or curled with tension",
                    "HEAD: Quick darting looks, then freeze, then dart",
                    "EYES: Rapid saccades, wide, frequent blinks",
                    "JAW: Clenched with occasional release",
                    "TIMING: Fidget BURSTS not continuous",
                    "TIMING: Quiet moments between fidget clusters"
                  ]
                }
              ],
              "description": "Anxiety-driven fidgeting with involuntary self-soothing behaviors.\nWeight shifts irregularly. Hands seek comfort (touching face, arms, clothing).\nShallow rapid breathing. Hypervigilant eye movement.\n\nFEEL: Uncomfortable, restless, seeking escape. Like waiting for bad news.\n\nTIMING: Breath is faster (2-3 second shallow cycles). Weight shifts are IRREGULAR\n(not sine waves - use turbulence). Fidget gestures happen in bursts with quiet\nperiods. Eyes dart quickly, then lock, then dart again.\n",
              "path": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Anxiety-driven fidgeting with involuntary self-soo...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": true,
          "duration": 2936,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "nervous-fidget.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The `nervous_fidget` block passed validation as it thoroughly expresses domain intent in its source code, incorporating well-defined animations that reflect the behavioral patterns associated with anxiety. It effectively utilizes specified inputs and outputs, correctly implements domain rules, and does not introduce any undocumented concepts. The documentation clearly states the motion's purpose, the expected feel, and cites relevant research, fulfilling all requirements."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt": "Block Name: nervous_fidget\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Anxiety-driven fidgeting with involuntary self-soothing behaviors.\\nWeight shifts irregularly. Hands seek comfort (touching face, arms, clothing).\\nShallow rapid breathing. Hypervigilant eye movement.\\n\\nFEEL: Uncomfortable, restless, seeking escape. Like waiting for bad news.\\n\\nTIMING: Breath is faster (2-3 second shallow cycles). Weight shifts are IRREGULAR\\n(not sine waves - use turbulence). Fidget gestures happen in bursts with quiet\\nperiods. Eyes dart quickly, then lock, then dart again.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe anxiety body language research basis\",\n        \"CORE: Hunched protective posture, shoulders elevated\",\n        \"CORE: Forward head posture (hypervigilance)\",\n        \"BREATH: Shallow, fast, irregular (2-3 second cycles)\",\n        \"BREATH: Shoulders rise with anxious breath\",\n        \"WEIGHT: IRREGULAR shifts using turbulence, not sine waves\",\n        \"WEIGHT: Ready-to-flee stance\",\n        \"ARMS: Held close to body (self-protection)\",\n        \"ARMS: Frequent self-touch gestures (neck, arms, face)\",\n        \"HANDS: Tense, curled fingers\",\n        \"HANDS: Rubbing, picking, clasping gestures\",\n        \"HANDS: All finger joints animated during fidgets\",\n        \"LEGS: Weight shifting, foot tapping bursts\",\n        \"FEET: Toes tapping or curled with tension\",\n        \"HEAD: Quick darting looks, then freeze, then dart\",\n        \"EYES: Rapid saccades, wide, frequent blinks\",\n        \"JAW: Clenched with occasional release\",\n        \"TIMING: Fidget BURSTS not continuous\",\n        \"TIMING: Quiet moments between fidget clusters\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- nervous-fidget.ts ---\n```\n/**\n * Nervous Fidget Motion\n *\n * Anxiety-driven motion with rapid weight shifts, self-soothing gestures,\n * and tense posture. Captures the biomechanics of nervousness and\n * displacement behavior.\n *\n * Research basis:\n * - Anxiety body language (Navarro)\n * - Self-soothing behaviors (pacifying gestures)\n * - Displacement activities in stress\n * - Hypervigilance postural patterns\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  getAvailableBones,\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const nervousFidgetParamsSchema = z.object({\n  /** Overall anxiety level (0-1). Default: 0.6 */\n  anxietyLevel: z.number().min(0).max(1).default(0.6),\n  /** Intensity of fidgeting movements (0-1). Default: 0.5 */\n  fidgetIntensity: z.number().min(0).max(1).default(0.5),\n  /** Speed of looking around (0-1). Default: 0.4 */\n  lookAroundSpeed: z.number().min(0).max(1).default(0.4),\n  /** Breath rate multiplier (nervous = faster). Default: 1.5 */\n  breathRateMultiplier: z.number().min(1).max(3).default(1.5),\n  /** Enable hand fidgeting. Default: true */\n  handFidget: z.boolean().default(true),\n  /** Enable foot tapping. Default: true */\n  footTap: z.boolean().default(true),\n  /** Enable rapid eye movement. Default: true */\n  rapidEyeMovement: z.boolean().default(true),\n  /** Shoulder tension (0-1). Default: 0.7 */\n  shoulderTension: z.number().min(0).max(1).default(0.7),\n})\n\nexport type NervousFidgetParams = z.infer<typeof nervousFidgetParamsSchema>\nexport type NervousFidgetInput = z.input<typeof nervousFidgetParamsSchema>\n\nexport const nervousFidgetMeta: MotionMeta = {\n  id: 'nervous-fidget',\n  name: 'Nervous Fidget',\n  description: 'Anxiety-driven fidgeting with weight shifts, tense posture, and self-soothing gestures',\n  tags: ['nervous', 'anxiety', 'fidget', 'tension'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface NervousFidgetState {\n  noise: NoiseGenerator\n  weightSpring: Spring\n  headSpring: Spring\n  armSpring: Spring\n  fidgetTimer: number\n  fidgetType: 'none' | 'hand_rub' | 'arm_touch' | 'neck_touch'\n  fidgetHand: 'left' | 'right'\n  lastFidgetTime: number\n  blinkTimer: number\n  isBlinking: boolean\n  footTapPhase: number\n  footTapActive: boolean\n  lookTarget: { x: number; y: number }\n  lookChangeTimer: number\n}\n\nfunction initState(seed: number): NervousFidgetState {\n  return {\n    noise: createNoiseGenerator(seed),\n    weightSpring: createSpring({ stiffness: 150, damping: 12 }),\n    headSpring: createSpring({ stiffness: 200, damping: 18 }),\n    armSpring: createSpring({ stiffness: 100, damping: 15 }),\n    fidgetTimer: 0,\n    fidgetType: 'none',\n    fidgetHand: 'right',\n    lastFidgetTime: 0,\n    blinkTimer: 0,\n    isBlinking: false,\n    footTapPhase: 0,\n    footTapActive: false,\n    lookTarget: { x: 0, y: 0 },\n    lookChangeTimer: 0,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a nervous fidget motion with proper handling of optional bones.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (jaw, eyes, fingers, toes) are gracefully skipped\n * - The animation degrades gracefully when bones are unavailable\n * - Core bones (hips, spine, chest) are checked before use\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - No use of Math.random() or Date.now()\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - No allocations in the update loop except for quaternion creation\n * - Pre-computed values used where possible\n * - Springs provide efficient physics simulation\n */\nexport function createNervousFidget(params: NervousFidgetInput = {}): MotionProgram<NervousFidgetParams> {\n  const validatedParams = nervousFidgetParamsSchema.parse(params)\n  let state: NervousFidgetState | null = null\n\n  return {\n    meta: nervousFidgetMeta,\n    paramsSchema: nervousFidgetParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        anxietyLevel,\n        fidgetIntensity,\n        lookAroundSpeed,\n        breathRateMultiplier,\n        handFidget,\n        footTap,\n        rapidEyeMovement,\n        shoulderTension,\n      } = validatedParams\n\n      const noise = state.noise\n      const anxiety = anxietyLevel\n\n      // ========================================\n      // LAYER 1: TENSE BASE POSTURE\n      // ========================================\n\n      // Hunched, protective stance\n      const hunch = 0.05 * anxiety\n      const forwardLean = 0.03 * anxiety\n\n      // Hips - slightly tucked\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, forwardLean)\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // Spine - forward hunch\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch))\n      }\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch * 0.8))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch * 0.5))\n      }\n\n      // Shoulders - raised and tense\n      const shoulderRaise = shoulderTension * 0.08\n      const shoulderForward = shoulderTension * 0.06\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderRaise)\n        leftShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -shoulderForward))\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderRaise)\n        rightShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, shoulderForward))\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // Neck - forward head posture (hypervigilance)\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.08 * anxiety))\n      }\n\n      // Arms - held closer to body\n      const armProtect = 0.05 * anxiety\n      if (rig.hasBone('leftUpperArm')) {\n        const leftArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.15 - armProtect)\n        leftArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1))\n        rig.setRotation('leftUpperArm', leftArmRot)\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const rightArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.15 + armProtect)\n        rightArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1))\n        rig.setRotation('rightUpperArm', rightArmRot)\n      }\n\n      // Bent elbows\n      if (rig.hasBone('leftLowerArm')) {\n        rig.setRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.4 * anxiety))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        rig.setRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.4 * anxiety))\n      }\n\n      // Legs - weight on one leg, ready to move\n      if (rig.hasBone('leftUpperLeg')) {\n        rig.setRotation('leftUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        rig.setRotation('rightUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.05))\n      }\n\n      // ========================================\n      // LAYER 2: SHALLOW BREATHING\n      // ========================================\n\n      const breathRate = 0.25 * breathRateMultiplier\n      const breathPhase = Math.sin(t * breathRate * Math.PI * 2)\n      const shallowBreath = breathPhase * 0.015 * anxiety\n\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, shallowBreath))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, shallowBreath * 1.2))\n      }\n\n      // Shoulder rise with anxious breath\n      const anxiousShoulderBreath = breathPhase * 0.01 * anxiety\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -anxiousShoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, anxiousShoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 3: IRREGULAR WEIGHT SHIFTING\n      // ========================================\n\n      // Quick, irregular weight shifts\n      const weightShiftNoise = noise.turbulence(t * 0.8, 0) * 2 - 1\n      const weightJitter = noise.noise2D(t * 3, 100) * fidgetIntensity * 0.3\n      state.weightSpring.setTarget(weightShiftNoise * 0.5 + weightJitter)\n      state.weightSpring.update(dt)\n      const weightShift = state.weightSpring.value * fidgetIntensity\n\n      if (rig.hasBone('hips')) {\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, weightShift * 0.04))\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightShift * 0.02))\n      }\n\n      // Counter in spine\n      if (rig.hasBone('spine')) {\n        rig.addRotation('spine', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -weightShift * 0.02))\n      }\n\n      // ========================================\n      // LAYER 4: HEAD MOVEMENT (LOOKING AROUND)\n      // ========================================\n\n      // Quick, darting looks\n      state.lookChangeTimer += dt\n      const lookChangeInterval = 1.5 - lookAroundSpeed * 1.2\n      if (state.lookChangeTimer > lookChangeInterval) {\n        state.lookChangeTimer = 0\n        state.lookTarget = {\n          x: (noise.noise2D(t, 200) * 2 - 1) * 0.15 * lookAroundSpeed,\n          y: (noise.noise2D(t, 300) * 2 - 1) * 0.1 * lookAroundSpeed,\n        }\n      }\n\n      state.headSpring.setTarget(state.lookTarget.x)\n      state.headSpring.update(dt)\n      const headYaw = state.headSpring.value\n\n      // Add nervous micro-movements\n      const headJitterX = noise.noise2D(t * 4, 400) * 0.02 * fidgetIntensity\n      const headJitterY = noise.noise2D(t * 4, 500) * 0.015 * fidgetIntensity\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headYaw + headJitterY)\n        headRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, state.lookTarget.y + headJitterX))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        rig.addRotation('neck', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headYaw * 0.3))\n      }\n\n      // ========================================\n      // LAYER 5: HAND FIDGETING\n      // ========================================\n\n      if (handFidget) {\n        // Decide on fidget type\n        const timeSinceFidget = t - state.lastFidgetTime\n        const fidgetInterval = 2 + noise.noise2D(t * 0.1, 600) * 2\n\n        if (state.fidgetType === 'none' && timeSinceFidget > fidgetInterval) {\n          const fidgetRoll = noise.noise2D(t, 700)\n          if (fidgetRoll > 0.6) {\n            state.fidgetType = 'hand_rub'\n          } else if (fidgetRoll > 0.3) {\n            state.fidgetType = 'arm_touch'\n          } else {\n            state.fidgetType = 'neck_touch'\n          }\n          state.fidgetHand = noise.noise2D(t, 800) > 0.5 ? 'left' : 'right'\n          state.fidgetTimer = 0\n        }\n\n        if (state.fidgetType !== 'none') {\n          state.fidgetTimer += dt\n          const fidgetDuration = 1.5 + noise.noise2D(t * 0.2, 900) * 1\n          const fidgetProgress = state.fidgetTimer / fidgetDuration\n\n          if (fidgetProgress >= 1) {\n            state.fidgetType = 'none'\n            state.lastFidgetTime = t\n          } else {\n            // Ease in and out of fidget\n            const fidgetWeight = Math.sin(fidgetProgress * Math.PI)\n\n            // Apply fidget based on type\n            switch (state.fidgetType) {\n              case 'hand_rub':\n                // Bring hands together in front\n                if (rig.hasBone('leftUpperArm')) {\n                  rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.3 * fidgetWeight))\n                  rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.2 * fidgetWeight))\n                }\n                if (rig.hasBone('rightUpperArm')) {\n                  rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.3 * fidgetWeight))\n                  rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.2 * fidgetWeight))\n                }\n                if (rig.hasBone('leftLowerArm')) {\n                  rig.addRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.5 * fidgetWeight))\n                }\n                if (rig.hasBone('rightLowerArm')) {\n                  rig.addRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.5 * fidgetWeight))\n                }\n                // Rubbing motion\n                const rubPhase = Math.sin(state.fidgetTimer * 8) * fidgetWeight\n                if (rig.hasBone('leftHand')) {\n                  rig.addRotation('leftHand', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rubPhase * 0.2))\n                }\n                break\n\n              case 'arm_touch':\n                // Touch opposite arm\n                const touchArm = state.fidgetHand === 'left' ? 'left' : 'right'\n                if (rig.hasBone(`${touchArm}UpperArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${touchArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.4 * fidgetWeight))\n                }\n                if (rig.hasBone(`${touchArm}LowerArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${touchArm}LowerArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 1, z: 0 }, (touchArm === 'left' ? -1 : 1) * 0.8 * fidgetWeight))\n                }\n                break\n\n              case 'neck_touch':\n                // Touch back of neck\n                const neckArm = state.fidgetHand\n                if (rig.hasBone(`${neckArm}UpperArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${neckArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.5 * fidgetWeight))\n                  rig.addRotation(`${neckArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 0, z: 1 }, (neckArm === 'left' ? -1 : 1) * 0.3 * fidgetWeight))\n                }\n                if (rig.hasBone(`${neckArm}LowerArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${neckArm}LowerArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 1, z: 0 }, (neckArm === 'left' ? -1 : 1) * 1.2 * fidgetWeight))\n                }\n                break\n            }\n          }\n        }\n\n        // Fingers - tense, curled\n        const fingerTension = 0.4 + noise.noise2D(t * 2, 1000) * 0.2\n        applyFingerCurl(rig, 'left', {\n          thumb: fingerTension * 0.5,\n          index: fingerTension,\n          middle: fingerTension * 1.1,\n          ring: fingerTension * 1.15,\n          little: fingerTension * 1.2,\n        })\n        applyFingerCurl(rig, 'right', {\n          thumb: fingerTension * 0.5,\n          index: fingerTension,\n          middle: fingerTension * 1.1,\n          ring: fingerTension * 1.15,\n          little: fingerTension * 1.2,\n        })\n      }\n\n      // ========================================\n      // LAYER 6: FOOT TAPPING\n      // ========================================\n\n      if (footTap) {\n        // Decide when to tap\n        const tapTrigger = noise.noise2D(t * 0.5, 1100)\n        if (!state.footTapActive && tapTrigger > 0.7) {\n          state.footTapActive = true\n          state.footTapPhase = 0\n        }\n\n        if (state.footTapActive) {\n          state.footTapPhase += dt * 6 // Fast tapping\n          const tapCycle = Math.sin(state.footTapPhase * Math.PI * 2)\n          const tapUp = Math.max(0, tapCycle) * fidgetIntensity\n\n          // Right foot tap\n          if (rig.hasBone('rightFoot')) {\n            rig.addRotation('rightFoot', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, tapUp * 0.15))\n          }\n          if (rig.hasBone('rightToes')) {\n            rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -tapUp * 0.2))\n          }\n\n          // Stop after a few taps\n          if (state.footTapPhase > 8) {\n            state.footTapActive = false\n          }\n        }\n      }\n\n      // ========================================\n      // LAYER 7: RAPID EYE MOVEMENT\n      // ========================================\n\n      if (rapidEyeMovement) {\n        // Quick, darting eye movements\n        const eyeSpeed = 3 + lookAroundSpeed * 4\n        const eyeX = noise.noise2D(t * eyeSpeed, 1200) * 0.08 * anxiety\n        const eyeY = noise.noise2D(t * eyeSpeed, 1300) * 0.06 * anxiety\n\n        // Frequent blinking\n        const blinkInterval = 1.5 - anxiety * 0.8\n        if (!state.isBlinking && noise.noise2D(t * 0.5, 1400) > 0.8) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          if (state.blinkTimer > 0.12) {\n            state.isBlinking = false\n          } else {\n            blinkAmount = Math.sin(state.blinkTimer / 0.12 * Math.PI)\n          }\n        }\n\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 8: JAW TENSION\n      // ========================================\n\n      if (rig.hasBone('jaw')) {\n        // Slight jaw clench with occasional release\n        const jawTension = anxiety * 0.02\n        const jawRelease = noise.noise2D(t * 0.3, 1500) > 0.8 ? 0.01 : 0\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawRelease - jawTension))\n      }\n    },\n  }\n}\n\nexport const nervousFidget: MotionProgram<NervousFidgetParams> = createNervousFidget({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": true,\n  \"issues\": [],\n  \"summary\": \"The `nervous_fidget` block passed validation as it thoroughly expresses domain intent in its source code, incorporating well-defined animations that reflect the behavioral patterns associated with anxiety. It effectively utilizes specified inputs and outputs, correctly implements domain rules, and does not introduce any undocumented concepts. The documentation clearly states the motion's purpose, the expected feel, and cites relevant research, fulfilling all requirements.\"\n}",
            "tokensUsed": {
              "input": 6708,
              "output": 91
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "smoking_cigarette",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/smoking-cigarette.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "smoking_cigarette",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe smoking biomechanics and timing offsets",
                    "PHASE ENVELOPES: NO discrete state machine",
                    "PHASE ENVELOPES: Shoulder, elbow, wrist have OVERLAPPING phases",
                    "PHASE ENVELOPES: Shoulder LEADS, wrist TRAILS",
                    "ANTICIPATION: Body telegraphs before arm rises",
                    "ANTICIPATION: Weight shifts before arm movement",
                    "ARM TRAJECTORY: Curved natural path, not linear",
                    "ARM TRAJECTORY: Spring-smoothed with overshoot/settle",
                    "HAND POSITION: Must accurately reach mouth (needs IK or precise targeting)",
                    "FINGERS: Precise cigarette grip (index + middle pinch)",
                    "FINGERS: Subtle adjustments throughout",
                    "INHALE: Chest expands, shoulders rise",
                    "INHALE: Eyes squint slightly",
                    "INHALE: Cheeks may hollow (jaw bone)",
                    "HOLD: Brief stillness, micro-tension",
                    "EXHALE: Jaw opens, lips purse",
                    "EXHALE: Head may tilt up or to side",
                    "EXHALE: Slow controlled breath",
                    "LOWER: Wrist leads, shoulder follows (opposite of raise)",
                    "LOWER: Follow-through past rest position, then settle",
                    "SUPPORT ARM: Crossed under, or relaxed at side",
                    "BREATH: Couples to entire motion cycle",
                    "STYLE: Casual/Stressed/Seductive affect ALL timing"
                  ]
                }
              ],
              "description": "Complete smoking animation with naturalistic hand-to-mouth coordination.\nNOT a state machine - overlapping phase envelopes for each body part.\n\nFEEL: Depends on style param. Casual = relaxed habit. Stressed = need for relief.\nSeductive = deliberate, slow, eye contact maintained.\n\nTIMING RELATIONSHIPS:\n- Shoulder rotation BEGINS first (leads by ~80ms)\n- Elbow extension follows (~50ms after shoulder)\n- Wrist rotation follows (~30ms after elbow)\n- Fingers adjust grip throughout\n- Inhale: chest expands, shoulders rise, eyes squint\n- Hold: brief stillness, smoke held\n- Exhale: jaw opens, head may tilt, slow breath out\n- Lower: reverse order (wrist leads, shoulder follows)\n\nCRITICAL: Hand MUST reach mouth accurately. This requires end-effector\ntargeting, not blind rotation values.\n",
              "path": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Complete smoking animation with naturalistic hand-...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 4735,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The implementation uses a state machine, which violates the specified requirement for overlapping phase envelopes instead of a state machine.",
              "file": "smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Describes lung phases but doesn't explicitly establish clear anticipation relationships between body parts as documented in the requirements.",
              "file": "smoking-cigarette.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The main motion implementation documentation states 'Complete smoking animation with state machine for different phases,' conflicting with the block definition's requirement for phase envelopes instead of a state machine.",
              "file": "smoking-cigarette.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The PHASE_DURATIONS section and state management logic imply fixed state transitions rather than overlapping temporal envelopes between movements.",
              "file": "smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Incomplete explanation of how micro-movements and noise functions are used to create organic motion in the comments.",
              "file": "smoking-cigarette.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "smoking-cigarette.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block fails validation due to the use of a state machine instead of overlapping phase envelopes, a direct violation of the specified requirements. Additionally, there are unclear anticipation relationships and limitations in the documentation, creating ambiguity around how movements correlate. Explicit adherence to phase definitions and a focus on smooth, continuous motion is not sufficiently demonstrated in the source code."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt": "Block Name: smoking_cigarette\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Complete smoking animation with naturalistic hand-to-mouth coordination.\\nNOT a state machine - overlapping phase envelopes for each body part.\\n\\nFEEL: Depends on style param. Casual = relaxed habit. Stressed = need for relief.\\nSeductive = deliberate, slow, eye contact maintained.\\n\\nTIMING RELATIONSHIPS:\\n- Shoulder rotation BEGINS first (leads by ~80ms)\\n- Elbow extension follows (~50ms after shoulder)\\n- Wrist rotation follows (~30ms after elbow)\\n- Fingers adjust grip throughout\\n- Inhale: chest expands, shoulders rise, eyes squint\\n- Hold: brief stillness, smoke held\\n- Exhale: jaw opens, head may tilt, slow breath out\\n- Lower: reverse order (wrist leads, shoulder follows)\\n\\nCRITICAL: Hand MUST reach mouth accurately. This requires end-effector\\ntargeting, not blind rotation values.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe smoking biomechanics and timing offsets\",\n        \"PHASE ENVELOPES: NO discrete state machine\",\n        \"PHASE ENVELOPES: Shoulder, elbow, wrist have OVERLAPPING phases\",\n        \"PHASE ENVELOPES: Shoulder LEADS, wrist TRAILS\",\n        \"ANTICIPATION: Body telegraphs before arm rises\",\n        \"ANTICIPATION: Weight shifts before arm movement\",\n        \"ARM TRAJECTORY: Curved natural path, not linear\",\n        \"ARM TRAJECTORY: Spring-smoothed with overshoot/settle\",\n        \"HAND POSITION: Must accurately reach mouth (needs IK or precise targeting)\",\n        \"FINGERS: Precise cigarette grip (index + middle pinch)\",\n        \"FINGERS: Subtle adjustments throughout\",\n        \"INHALE: Chest expands, shoulders rise\",\n        \"INHALE: Eyes squint slightly\",\n        \"INHALE: Cheeks may hollow (jaw bone)\",\n        \"HOLD: Brief stillness, micro-tension\",\n        \"EXHALE: Jaw opens, lips purse\",\n        \"EXHALE: Head may tilt up or to side\",\n        \"EXHALE: Slow controlled breath\",\n        \"LOWER: Wrist leads, shoulder follows (opposite of raise)\",\n        \"LOWER: Follow-through past rest position, then settle\",\n        \"SUPPORT ARM: Crossed under, or relaxed at side\",\n        \"BREATH: Couples to entire motion cycle\",\n        \"STYLE: Casual/Stressed/Seductive affect ALL timing\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- smoking-cigarette.ts ---\n```\n/**\n * Smoking Cigarette Motion\n *\n * Complete smoking animation with state machine for different phases:\n * idle holding, bring to mouth, inhale, hold, exhale, lower, ash tap.\n *\n * Research basis:\n * - Smoking biomechanics studies\n * - Hand-to-mouth coordination patterns\n * - Respiratory mechanics during smoking\n * - Habitual gesture timing\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  SpringPresets,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  applyCigaretteGrip,\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const smokingCigaretteParamsSchema = z.object({\n  /** Smoking style: casual, stressed, or seductive. Default: casual */\n  smokingStyle: z.enum(['casual', 'stressed', 'seductive']).default('casual'),\n  /** Which hand holds the cigarette. Default: right */\n  handedness: z.enum(['left', 'right']).default('right'),\n  /** Depth of inhale (0-1). Default: 0.7 */\n  inhaleDepth: z.number().min(0).max(1).default(0.7),\n  /** Variation in pacing (0-1). Default: 0.3 */\n  paceVariation: z.number().min(0).max(1).default(0.3),\n  /** Overall animation intensity (0-1). Default: 0.8 */\n  intensity: z.number().min(0).max(1).default(0.8),\n  /** Enable eye squint during inhale. Default: true */\n  eyeSquint: z.boolean().default(true),\n  /** Enable jaw animation during exhale. Default: true */\n  jawAnimation: z.boolean().default(true),\n  /** Time between puffs in seconds. Default: 8 */\n  puffInterval: z.number().min(3).max(20).default(8),\n})\n\nexport type SmokingCigaretteParams = z.infer<typeof smokingCigaretteParamsSchema>\nexport type SmokingCigaretteInput = z.input<typeof smokingCigaretteParamsSchema>\n\nexport const smokingCigaretteMeta: MotionMeta = {\n  id: 'smoking-cigarette',\n  name: 'Smoking Cigarette',\n  description: 'Complete smoking animation with hand-to-mouth, inhale, exhale phases',\n  tags: ['smoking', 'gesture', 'complex', 'state-machine'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MACHINE\n// ============================================================================\n\ntype SmokingPhase =\n  | 'idle'           // Holding cigarette at side\n  | 'bring_to_mouth' // Raising arm to mouth\n  | 'inhale'         // Taking a drag\n  | 'hold'           // Holding smoke\n  | 'exhale'         // Breathing out\n  | 'lower'          // Lowering arm\n  | 'ash_tap'        // Tapping ash off\n\ninterface SmokingState {\n  noise: NoiseGenerator\n  phase: SmokingPhase\n  phaseTime: number\n  phaseDuration: number\n  armSpringX: Spring\n  armSpringY: Spring\n  armSpringZ: Spring\n  wristSpring: Spring\n  chestSpring: Spring\n  lastPuffTime: number\n  ashTapPending: boolean\n  blinkTimer: number\n  isBlinking: boolean\n}\n\nconst PHASE_DURATIONS = {\n  idle: { base: 6, variance: 2 },\n  bring_to_mouth: { base: 0.8, variance: 0.2 },\n  inhale: { base: 1.5, variance: 0.3 },\n  hold: { base: 0.8, variance: 0.3 },\n  exhale: { base: 2.0, variance: 0.4 },\n  lower: { base: 0.6, variance: 0.15 },\n  ash_tap: { base: 0.4, variance: 0.1 },\n}\n\nfunction initState(seed: number): SmokingState {\n  return {\n    noise: createNoiseGenerator(seed),\n    phase: 'idle',\n    phaseTime: 0,\n    phaseDuration: PHASE_DURATIONS.idle.base,\n    armSpringX: createSpring(SpringPresets.smooth),\n    armSpringY: createSpring(SpringPresets.smooth),\n    armSpringZ: createSpring(SpringPresets.smooth),\n    wristSpring: createSpring({ stiffness: 250, damping: 22 }),\n    chestSpring: createSpring({ stiffness: 100, damping: 18 }),\n    lastPuffTime: -10,\n    ashTapPending: false,\n    blinkTimer: 0,\n    isBlinking: false,\n  }\n}\n\nfunction getNextPhase(current: SmokingPhase, ashTapPending: boolean): SmokingPhase {\n  switch (current) {\n    case 'idle': return 'bring_to_mouth'\n    case 'bring_to_mouth': return 'inhale'\n    case 'inhale': return 'hold'\n    case 'hold': return 'exhale'\n    case 'exhale': return ashTapPending ? 'ash_tap' : 'lower'\n    case 'lower': return 'idle'\n    case 'ash_tap': return 'lower'\n  }\n}\n\nfunction getPhaseDuration(phase: SmokingPhase, variation: number, noise: NoiseGenerator, t: number): number {\n  const { base, variance } = PHASE_DURATIONS[phase]\n  return base + noise.noise2D(t, phase.length * 100) * variance * variation\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a smoking cigarette motion with complete state machine.\n *\n * STATE MACHINE PHASES:\n * - idle: Holding cigarette at side, relaxed posture\n * - bring_to_mouth: Spring-animated arm raise to face\n * - inhale: Taking a drag, chest expansion, eye squint\n * - hold: Brief pause holding smoke, slight arm lower\n * - exhale: Slow breath out, jaw animation, arm stays high\n * - lower: Return arm to idle position\n * - ash_tap: Optional wrist flick to tap ash (random trigger)\n *\n * TRANSITIONS:\n * - All transitions are time-based with phase duration + variance\n * - Spring physics ensure smooth arm movement between states\n * - Phase progress is used for easing within each state\n * - idle → bring_to_mouth → inhale → hold → exhale → (ash_tap?) → lower → idle\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Dynamic bone names (smokingArm, supportArm) are cast and checked\n * - Missing optional bones (jaw, eyes, fingers) are gracefully skipped\n * - The animation degrades gracefully when bones are unavailable\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Ash tap triggering uses seeded noise\n * - Phase duration variance uses seeded noise\n *\n * SECONDARY MOTION:\n * - Finger positions for cigarette grip via applyCigaretteGrip\n * - Wrist rotation during state transitions\n * - Chest expansion/contraction with breathing phases\n */\nexport function createSmokingCigarette(params: SmokingCigaretteInput = {}): MotionProgram<SmokingCigaretteParams> {\n  const validatedParams = smokingCigaretteParamsSchema.parse(params)\n  let state: SmokingState | null = null\n\n  return {\n    meta: smokingCigaretteMeta,\n    paramsSchema: smokingCigaretteParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        smokingStyle,\n        handedness,\n        inhaleDepth,\n        paceVariation,\n        intensity,\n        eyeSquint,\n        jawAnimation,\n        puffInterval,\n      } = validatedParams\n\n      const noise = state.noise\n      const isRightHanded = handedness === 'right'\n      const handSide = isRightHanded ? 1 : -1\n\n      // Style modifiers\n      const styleModifiers = {\n        casual: { speed: 1, tension: 0.3, lean: 0 },\n        stressed: { speed: 1.3, tension: 0.7, lean: 0.05 },\n        seductive: { speed: 0.7, tension: 0.2, lean: -0.03 },\n      }[smokingStyle]\n\n      // ========================================\n      // STATE MACHINE UPDATE\n      // ========================================\n\n      state.phaseTime += dt\n\n      // Check for phase transition\n      if (state.phaseTime >= state.phaseDuration) {\n        const nextPhase = getNextPhase(state.phase, state.ashTapPending)\n        state.phase = nextPhase\n        state.phaseTime = 0\n        state.phaseDuration = getPhaseDuration(nextPhase, paceVariation, noise, t) / styleModifiers.speed\n\n        if (nextPhase === 'idle') {\n          state.lastPuffTime = t\n          // Randomly decide if next cycle should include ash tap\n          state.ashTapPending = noise.noise2D(t, 500) > 0.7\n        }\n      }\n\n      // Force transition if puff interval exceeded during idle\n      if (state.phase === 'idle' && (t - state.lastPuffTime) > puffInterval) {\n        state.phase = 'bring_to_mouth'\n        state.phaseTime = 0\n        state.phaseDuration = getPhaseDuration('bring_to_mouth', paceVariation, noise, t) / styleModifiers.speed\n      }\n\n      const phaseProgress = Math.min(1, state.phaseTime / state.phaseDuration)\n\n      // ========================================\n      // LAYER 1: BASE POSTURE\n      // ========================================\n\n      // Style-based lean\n      if (rig.hasBone('hips')) {\n        const leanAmount = styleModifiers.lean * intensity\n        rig.setRotation('hips', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leanAmount))\n      }\n\n      // Relaxed spine\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n      }\n\n      // ========================================\n      // LAYER 2: ARM MOVEMENT (SPRING-BASED)\n      // ========================================\n\n      // Define arm targets for each phase\n      let armTargetX = 0 // Forward/back rotation\n      let armTargetY = 0 // Abduction\n      let armTargetZ = 0 // Twist\n      let elbowBend = 0.2 * intensity\n      let wristRotation = 0\n\n      const smokingArm = isRightHanded ? 'right' : 'left'\n      const supportArm = isRightHanded ? 'left' : 'right'\n\n      switch (state.phase) {\n        case 'idle':\n          armTargetX = 0.1 * intensity\n          armTargetY = 0.15 * intensity\n          armTargetZ = 0\n          elbowBend = 0.3 * intensity\n          wristRotation = 0.1 * handSide\n          break\n\n        case 'bring_to_mouth':\n          const raiseEase = Easing.armRaise(phaseProgress)\n          armTargetX = -0.5 * intensity * raiseEase\n          armTargetY = 0.4 * intensity * raiseEase\n          armTargetZ = handSide * 0.2 * intensity * raiseEase\n          elbowBend = 1.4 * intensity * raiseEase\n          wristRotation = handSide * 0.3 * raiseEase\n          break\n\n        case 'inhale':\n          armTargetX = -0.5 * intensity\n          armTargetY = 0.4 * intensity\n          armTargetZ = handSide * 0.2 * intensity\n          elbowBend = 1.4 * intensity\n          wristRotation = handSide * 0.3\n          break\n\n        case 'hold':\n          // Slight arm lower while holding + SECONDARY MOTION\n          // During hold, add subtle micro-movements for organic feel\n          const holdProgress = Easing.easeInOutCubic(phaseProgress)\n\n          // Secondary motion: subtle arm drift and finger adjustments\n          const holdMicroX = noise.noise2D(t * 0.5, 250) * 0.015 * intensity\n          const holdMicroY = noise.noise2D(t * 0.4, 260) * 0.01 * intensity\n          const holdMicroZ = noise.noise2D(t * 0.3, 270) * 0.008 * intensity\n          const holdWristMicro = noise.noise2D(t * 0.6, 280) * 0.02 * intensity\n\n          armTargetX = -0.4 * intensity + holdMicroX\n          armTargetY = 0.35 * intensity + holdMicroY\n          armTargetZ = handSide * 0.18 * intensity + holdMicroZ\n          elbowBend = 1.3 * intensity\n          wristRotation = handSide * 0.25 + holdWristMicro\n          break\n\n        case 'exhale':\n          // Arm stays relatively high during exhale\n          const exhaleProgress = Easing.easeInOutCubic(phaseProgress)\n          armTargetX = -0.35 * intensity * (1 - exhaleProgress * 0.5)\n          armTargetY = 0.3 * intensity * (1 - exhaleProgress * 0.3)\n          armTargetZ = handSide * 0.15 * intensity\n          elbowBend = 1.1 * intensity * (1 - exhaleProgress * 0.3)\n          wristRotation = handSide * 0.2\n          break\n\n        case 'lower':\n          const lowerEase = Easing.easeInCubic(phaseProgress)\n          armTargetX = 0.1 * intensity * lowerEase\n          armTargetY = 0.15 * intensity * lowerEase\n          armTargetZ = 0\n          elbowBend = 0.3 * intensity * lowerEase + 1.1 * intensity * (1 - lowerEase)\n          wristRotation = handSide * 0.1 * lowerEase\n          break\n\n        case 'ash_tap':\n          // Quick wrist flick\n          const tapEase = Math.sin(phaseProgress * Math.PI * 2)\n          armTargetX = 0.1 * intensity\n          armTargetY = 0.15 * intensity\n          armTargetZ = 0\n          elbowBend = 0.4 * intensity\n          wristRotation = handSide * 0.1 + tapEase * 0.3 * handSide\n          break\n      }\n\n      // Apply spring smoothing to arm movements\n      state.armSpringX.setTarget(armTargetX)\n      state.armSpringY.setTarget(armTargetY)\n      state.armSpringZ.setTarget(armTargetZ)\n      state.wristSpring.setTarget(wristRotation)\n\n      state.armSpringX.update(dt)\n      state.armSpringY.update(dt)\n      state.armSpringZ.update(dt)\n      state.wristSpring.update(dt)\n\n      const smoothArmX = state.armSpringX.value\n      const smoothArmY = state.armSpringY.value\n      const smoothArmZ = state.armSpringZ.value\n      const smoothWrist = state.wristSpring.value\n\n      // Apply smoking arm\n      const upperArmBone = `${smokingArm}UpperArm` as VRMHumanBoneName\n      const lowerArmBone = `${smokingArm}LowerArm` as VRMHumanBoneName\n      const handBone = `${smokingArm}Hand` as VRMHumanBoneName\n      const shoulderBone = `${smokingArm}Shoulder` as VRMHumanBoneName\n\n      if (rig.hasBone(shoulderBone)) {\n        rig.setRotation(shoulderBone, quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handSide * smoothArmY * 0.15))\n      }\n\n      if (rig.hasBone(upperArmBone)) {\n        const upperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, smoothArmX)\n        upperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handSide * smoothArmY))\n        upperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, smoothArmZ))\n        rig.setRotation(upperArmBone, upperArmRot)\n      }\n\n      if (rig.hasBone(lowerArmBone)) {\n        rig.setRotation(lowerArmBone, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, handSide * elbowBend))\n      }\n\n      if (rig.hasBone(handBone)) {\n        const handRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, smoothWrist)\n        // Slight wrist extension when holding cigarette up\n        const wristExtension = (state.phase === 'inhale' || state.phase === 'hold') ? 0.15 : 0\n        handRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, wristExtension))\n        rig.setRotation(handBone, handRot)\n      }\n\n      // Apply cigarette grip to smoking hand\n      applyCigaretteGrip(rig, smokingArm as 'left' | 'right', 'between')\n\n      // Support arm - relaxed or crossed\n      const supportUpperArm = `${supportArm}UpperArm` as VRMHumanBoneName\n      const supportLowerArm = `${supportArm}LowerArm` as VRMHumanBoneName\n\n      if (smokingStyle === 'seductive') {\n        // Arm crossed under\n        if (rig.hasBone(supportUpperArm)) {\n          const supportRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.4 * intensity)\n          supportRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handSide * 0.3 * intensity))\n          rig.setRotation(supportUpperArm, supportRot)\n        }\n        if (rig.hasBone(supportLowerArm)) {\n          rig.setRotation(supportLowerArm, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -handSide * 1.2 * intensity))\n        }\n      } else {\n        // Relaxed at side\n        if (rig.hasBone(supportUpperArm)) {\n          rig.setRotation(supportUpperArm, quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handSide * 0.08))\n        }\n        if (rig.hasBone(supportLowerArm)) {\n          rig.setRotation(supportLowerArm, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -handSide * 0.15))\n        }\n        // Relaxed fingers on support hand\n        applyFingerCurl(rig, supportArm as 'left' | 'right', {\n          thumb: 0.3,\n          index: 0.35,\n          middle: 0.4,\n          ring: 0.45,\n          little: 0.5,\n        })\n      }\n\n      // ========================================\n      // LAYER 3: BREATHING & CHEST\n      // ========================================\n\n      let chestExpansion = 0\n      const baseBreath = oscBreathing(t, 0.2, 0.3) * intensity\n\n      switch (state.phase) {\n        case 'inhale':\n          // Deep inhale - chest expands\n          chestExpansion = Easing.easeInCubic(phaseProgress) * inhaleDepth * 0.06\n          break\n        case 'hold':\n          // Held breath\n          chestExpansion = inhaleDepth * 0.06\n          break\n        case 'exhale':\n          // Slow exhale\n          chestExpansion = inhaleDepth * 0.06 * (1 - Easing.easeOutCubic(phaseProgress))\n          break\n        default:\n          chestExpansion = baseBreath * 0.02\n      }\n\n      state.chestSpring.setTarget(chestExpansion)\n      state.chestSpring.update(dt)\n      const smoothChest = state.chestSpring.value\n\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -smoothChest))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -smoothChest * 1.5))\n      }\n\n      // Shoulders rise with inhale\n      const shoulderRise = state.phase === 'inhale' ? phaseProgress * 0.02 * inhaleDepth : 0\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderRise))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderRise))\n      }\n\n      // ========================================\n      // LAYER 4: HEAD MOVEMENT\n      // ========================================\n\n      let headTiltX = 0\n      let headTiltY = 0\n\n      switch (state.phase) {\n        case 'inhale':\n          // Slight head tilt back during inhale\n          headTiltX = -0.05 * phaseProgress * intensity\n          break\n        case 'exhale':\n          // Head forward/up for exhale\n          headTiltX = 0.03 * (1 - phaseProgress) * intensity\n          headTiltY = handSide * 0.02 * phaseProgress * intensity\n          break\n        default:\n          // Subtle idle movement\n          headTiltX = noise.noise2D(t * 0.2, 600) * 0.02 * intensity\n          headTiltY = noise.noise2D(t * 0.15, 700) * 0.025 * intensity\n      }\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headTiltY))\n        rig.setRotation('head', headRot)\n      }\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX * 0.4))\n      }\n\n      // ========================================\n      // LAYER 5: EYES\n      // ========================================\n\n      if (eyeSquint) {\n        let eyeSquintAmount = 0\n\n        if (state.phase === 'inhale') {\n          eyeSquintAmount = phaseProgress * 0.15 * intensity\n        } else if (state.phase === 'hold') {\n          eyeSquintAmount = 0.15 * intensity\n        } else if (state.phase === 'exhale') {\n          eyeSquintAmount = 0.15 * intensity * (1 - phaseProgress)\n        }\n\n        // Eye rotation for squint effect\n        if (rig.hasBone('leftEye')) {\n          rig.setRotation('leftEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeSquintAmount))\n        }\n        if (rig.hasBone('rightEye')) {\n          rig.setRotation('rightEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeSquintAmount))\n        }\n      }\n\n      // Blinking\n      const blinkChance = noise.noise2D(t * 0.3, 800)\n      if (!state.isBlinking && blinkChance > 0.95) {\n        state.isBlinking = true\n        state.blinkTimer = 0\n      }\n\n      if (state.isBlinking) {\n        state.blinkTimer += dt\n        if (state.blinkTimer > 0.15) {\n          state.isBlinking = false\n        } else {\n          const blinkProgress = Math.sin(state.blinkTimer / 0.15 * Math.PI)\n          if (rig.hasBone('leftEye')) {\n            rig.addRotation('leftEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -blinkProgress * 0.25))\n          }\n          if (rig.hasBone('rightEye')) {\n            rig.addRotation('rightEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -blinkProgress * 0.25))\n          }\n        }\n      }\n\n      // ========================================\n      // LAYER 6: JAW (EXHALE)\n      // ========================================\n\n      if (jawAnimation && rig.hasBone('jaw')) {\n        let jawOpen = 0\n\n        if (state.phase === 'exhale') {\n          // Open slightly for exhale\n          const exhaleJaw = Math.sin(phaseProgress * Math.PI) * 0.08 * intensity\n          jawOpen = exhaleJaw\n        }\n\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawOpen))\n      }\n\n      // ========================================\n      // LAYER 7: LEGS & WEIGHT\n      // ========================================\n\n      // Relaxed stance with slight weight shift\n      const weightShift = noise.noise2D(t * 0.1, 900) * 0.03 * intensity\n\n      if (rig.hasBone('leftUpperLeg')) {\n        rig.setRotation('leftUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 + weightShift))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        rig.setRotation('rightUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03 - weightShift))\n      }\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.05))\n      }\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08))\n      }\n\n      // Feet slightly turned out\n      if (rig.hasBone('leftFoot')) {\n        rig.setRotation('leftFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.1))\n      }\n      if (rig.hasBone('rightFoot')) {\n        rig.setRotation('rightFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.15))\n      }\n    },\n  }\n}\n\nexport const smokingCigarette: MotionProgram<SmokingCigaretteParams> = createSmokingCigarette({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"The implementation uses a state machine, which violates the specified requirement for overlapping phase envelopes instead of a state machine.\",\n      \"severity\": \"error\",\n      \"file\": \"smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Describes lung phases but doesn't explicitly establish clear anticipation relationships between body parts as documented in the requirements.\",\n      \"severity\": \"warning\",\n      \"file\": \"smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"The main motion implementation documentation states 'Complete smoking animation with state machine for different phases,' conflicting with the block definition's requirement for phase envelopes instead of a state machine.\",\n      \"severity\": \"error\",\n      \"file\": \"smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"The PHASE_DURATIONS section and state management logic imply fixed state transitions rather than overlapping temporal envelopes between movements.\",\n      \"severity\": \"error\",\n      \"file\": \"smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Incomplete explanation of how micro-movements and noise functions are used to create organic motion in the comments.\",\n      \"severity\": \"warning\",\n      \"file\": \"smoking-cigarette.ts\"\n    }\n  ],\n  \"summary\": \"The block fails validation due to the use of a state machine instead of overlapping phase envelopes, a direct violation of the specified requirements. Additionally, there are unclear anticipation relationships and limitations in the documentation, creating ambiguity around how movements correlate. Explicit adherence to phase definitions and a focus on smooth, continuous motion is not sufficiently demonstrated in the source code.\"\n}",
            "tokensUsed": {
              "input": 7384,
              "output": 271
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "seductive_walk",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/seductive-walk.ts",
      "hasErrors": false,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "seductive_walk",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe runway walk biomechanics",
                    "DOCUMENTATION: Must specify timing delays between body parts",
                    "GAIT: Full cycle with stance/swing phases per leg",
                    "GAIT: Crossover step pattern (feet cross midline)",
                    "HIPS: Exaggerated lateral sway, hip drop on swing side",
                    "HIPS: Forward pelvic tilt for posture",
                    "HIPS: Twist opposite to shoulders",
                    "SPINE: Counter-rotation balances hip sway",
                    "SPINE: S-curve through spine chain",
                    "SHOULDERS: Counter-rotate opposite to hips",
                    "SHOULDERS: Drop with arm swing",
                    "ARMS: Fluid swing with DELAY (secondary motion)",
                    "ARMS: Wrist trails elbow",
                    "HANDS: Graceful, slightly spread fingers",
                    "HANDS: Wrist rotation adds elegance",
                    "LEGS: Full hip/knee/ankle coordination",
                    "LEGS: Toe point during swing phase",
                    "FEET: Heel-toe roll, slight turn out",
                    "TOES: Point during swing, grip during stance",
                    "HEAD: Stable (vestibular reflex) with slight tilt",
                    "HEAD: Can include flirtatious glances",
                    "EYES: Confident forward gaze, slow blinks",
                    "BREATH: Couples to gait cycle",
                    "WEIGHT: Visible transfer, hip bounce"
                  ]
                }
              ],
              "description": "Runway-style walk with exaggerated hip sway and fluid full-body coordination.\nEvery step is a performance. Weight transfer is deliberate and visible.\n\nFEEL: Confident, alluring, aware of being watched. Like a model on runway.\n\nGAIT CYCLE TIMING:\n- Hips lead the motion (lateral sway peaks at mid-stance)\n- Spine counter-rotates to balance hip sway\n- Shoulders counter-rotate opposite to hips\n- Arms swing opposite to legs with DELAY (secondary motion)\n- Wrists trail arms, fingers trail wrists\n- Head stays relatively stable (vestibular reflex)\n- Eyes can flirt with \"audience\"\n\nCROSSOVER: Feet cross midline for exaggerated hip motion.\nEach step places foot slightly past center.\n",
              "path": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Runway-style walk with exaggerated hip sway and fl...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": true,
          "duration": 12297,
          "issues": [
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The documentation does not sufficiently specify the timing delays between body parts as required by the output constraints.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The 'createSeductiveWalk' function does not explicitly define the biomechanical or observational research basis for the nuanced motions described, such as hip sway or shoulder counter-rotation.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The 'getGaitPhase' function does not account for any variations or delays in gait cycle, which may misrepresent the timing relationships specified in outputs.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Secondary motion for the shoulders and arms needs enhanced detail regarding how it flows with respect to the hips and spine, as this is a critical part of the animation.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "seductive-walk.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block generally adheres to the domain intent of creating a seductive walk animation with a realistic gait cycle. However, it failed validation due to insufficient documentation regarding timing relationships and biomechanical foundations. Additionally, some animations lacked explicit clarity on secondary motions, which is crucial for achieving the intended fluidity and believability of motion."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt": "Block Name: seductive_walk\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Runway-style walk with exaggerated hip sway and fluid full-body coordination.\\nEvery step is a performance. Weight transfer is deliberate and visible.\\n\\nFEEL: Confident, alluring, aware of being watched. Like a model on runway.\\n\\nGAIT CYCLE TIMING:\\n- Hips lead the motion (lateral sway peaks at mid-stance)\\n- Spine counter-rotates to balance hip sway\\n- Shoulders counter-rotate opposite to hips\\n- Arms swing opposite to legs with DELAY (secondary motion)\\n- Wrists trail arms, fingers trail wrists\\n- Head stays relatively stable (vestibular reflex)\\n- Eyes can flirt with \\\"audience\\\"\\n\\nCROSSOVER: Feet cross midline for exaggerated hip motion.\\nEach step places foot slightly past center.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/seductive-walk.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe runway walk biomechanics\",\n        \"DOCUMENTATION: Must specify timing delays between body parts\",\n        \"GAIT: Full cycle with stance/swing phases per leg\",\n        \"GAIT: Crossover step pattern (feet cross midline)\",\n        \"HIPS: Exaggerated lateral sway, hip drop on swing side\",\n        \"HIPS: Forward pelvic tilt for posture\",\n        \"HIPS: Twist opposite to shoulders\",\n        \"SPINE: Counter-rotation balances hip sway\",\n        \"SPINE: S-curve through spine chain\",\n        \"SHOULDERS: Counter-rotate opposite to hips\",\n        \"SHOULDERS: Drop with arm swing\",\n        \"ARMS: Fluid swing with DELAY (secondary motion)\",\n        \"ARMS: Wrist trails elbow\",\n        \"HANDS: Graceful, slightly spread fingers\",\n        \"HANDS: Wrist rotation adds elegance\",\n        \"LEGS: Full hip/knee/ankle coordination\",\n        \"LEGS: Toe point during swing phase\",\n        \"FEET: Heel-toe roll, slight turn out\",\n        \"TOES: Point during swing, grip during stance\",\n        \"HEAD: Stable (vestibular reflex) with slight tilt\",\n        \"HEAD: Can include flirtatious glances\",\n        \"EYES: Confident forward gaze, slow blinks\",\n        \"BREATH: Couples to gait cycle\",\n        \"WEIGHT: Visible transfer, hip bounce\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- seductive-walk.ts ---\n```\n/**\n * Seductive Walk Motion\n *\n * Runway-style walking with exaggerated hip sway, fluid arm movement,\n * and confident head carriage. Full gait cycle implementation with\n * crossover step pattern.\n *\n * Research basis:\n * - Runway/catwalk biomechanics\n * - Hip kinematics during walking\n * - Arm swing dynamics and coordination\n * - Weight transfer patterns\n * - Secondary motion physics\n */\n\nimport { z } from 'zod'\nimport { Vector3 } from 'three'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  walkPhase,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  getAvailableBones,\n  applyFingerCurl,\n  applyFingerSpread,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const seductiveWalkParamsSchema = z.object({\n  /** Overall intensity of exaggeration (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Walking speed (steps per second). Default: 0.8 */\n  speed: z.number().min(0.3).max(2).default(0.8),\n  /** Amount of hip sway (0-1). Default: 0.8 */\n  hipSwayAmount: z.number().min(0).max(1).default(0.8),\n  /** Fluidity of arm movement (0-1). Default: 0.7 */\n  armFlowiness: z.number().min(0).max(1).default(0.7),\n  /** Head tilt angle (0-1). Default: 0.3 */\n  headTilt: z.number().min(0).max(1).default(0.3),\n  /** Enable crossover step pattern. Default: true */\n  crossoverStep: z.boolean().default(true),\n  /** Enable toe point during swing. Default: true */\n  toePoint: z.boolean().default(true),\n  /** Enable secondary motion (follow-through). Default: true */\n  secondaryMotion: z.boolean().default(true),\n  /** Enable finger movement. Default: true */\n  fingerAnimation: z.boolean().default(true),\n})\n\nexport type SeductiveWalkParams = z.infer<typeof seductiveWalkParamsSchema>\nexport type SeductiveWalkInput = z.input<typeof seductiveWalkParamsSchema>\n\nexport const seductiveWalkMeta: MotionMeta = {\n  id: 'seductive-walk',\n  name: 'Seductive Walk',\n  description: 'Runway-style walk with exaggerated hip sway and fluid arm movement',\n  tags: ['walk', 'locomotion', 'seductive', 'runway'],\n  author: 'posers',\n}\n\n// ============================================================================\n// GAIT CYCLE HELPERS\n// ============================================================================\n\n/**\n * Gait cycle phases (0-1):\n * 0.0 - 0.1: Right heel strike / Left toe off\n * 0.1 - 0.3: Right loading response\n * 0.3 - 0.5: Right midstance / Left swing\n * 0.5 - 0.6: Left heel strike / Right toe off\n * 0.6 - 0.8: Left loading response\n * 0.8 - 1.0: Left midstance / Right swing\n */\n\nfunction getGaitPhase(t: number, speed: number): number {\n  const cycleTime = 1 / speed\n  return (t / cycleTime) % 1\n}\n\nfunction legSwingCurve(phase: number): number {\n  // Smooth leg swing with acceleration\n  return Easing.easeInOutSine(phase)\n}\n\nfunction hipDropCurve(phase: number): number {\n  // Hip drops during swing phase, rises during stance\n  return Math.sin(phase * Math.PI * 2)\n}\n\nfunction armSwingCurve(phase: number, flowiness: number): number {\n  // Fluid arm swing with follow-through\n  const base = Math.sin(phase * Math.PI * 2)\n  const secondary = Math.sin((phase * Math.PI * 2) - 0.3) * 0.2 * flowiness\n  return base + secondary\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface SeductiveWalkState {\n  noise: NoiseGenerator\n  hipSpring: Spring\n  shoulderSpring: Spring\n  headSpring: Spring\n  leftArmSpring: Spring\n  rightArmSpring: Spring\n  blinkTimer: number\n  isBlinking: boolean\n}\n\n/**\n * Initialize deterministic state with seeded random generators.\n * The noise generator uses the seed to ensure identical output\n * for the same seed value across all calls.\n */\nfunction initState(seed: number): SeductiveWalkState {\n  // Noise generator is seeded for deterministic, reproducible motion\n  return {\n    noise: createNoiseGenerator(seed),\n    hipSpring: createSpring({ stiffness: 150, damping: 15 }),\n    shoulderSpring: createSpring({ stiffness: 120, damping: 12 }),\n    headSpring: createSpring({ stiffness: 180, damping: 20 }),\n    leftArmSpring: createSpring({ stiffness: 80, damping: 10 }),\n    rightArmSpring: createSpring({ stiffness: 80, damping: 10 }),\n    blinkTimer: 0,\n    isBlinking: false,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a seductive walk motion with full gait cycle implementation.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (toes, eyes, fingers) are gracefully skipped\n * - The animation degrades gracefully when bones are unavailable\n * - Core bones (hips, spine, legs) are all checked before use\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - Gait phase is purely time-based with no random elements\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - Spring physics use efficient semi-implicit Euler integration\n * - Gait phase calculations are simple trigonometry\n * - No allocations in hot path except quaternion creation\n */\nexport function createSeductiveWalk(params: SeductiveWalkInput = {}): MotionProgram<SeductiveWalkParams> {\n  const validatedParams = seductiveWalkParamsSchema.parse(params)\n  let state: SeductiveWalkState | null = null\n\n  return {\n    meta: seductiveWalkMeta,\n    paramsSchema: seductiveWalkParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        intensity,\n        speed,\n        hipSwayAmount,\n        armFlowiness,\n        headTilt,\n        crossoverStep,\n        toePoint,\n        secondaryMotion,\n        fingerAnimation,\n      } = validatedParams\n\n      const noise = state.noise\n      const gaitPhase = getGaitPhase(t, speed)\n\n      // Which leg is in stance phase\n      const rightStance = gaitPhase < 0.5\n      const leftStance = !rightStance\n\n      // Swing phase for each leg (0-1 during their swing)\n      const rightSwingPhase = rightStance ? 0 : (gaitPhase - 0.5) * 2\n      const leftSwingPhase = leftStance ? 0 : gaitPhase * 2\n\n      // ========================================\n      // LAYER 1: HIP MOVEMENT (CORE OF THE WALK)\n      // ========================================\n\n      // Hip sway - lateral movement\n      const hipSwayTarget = Math.sin(gaitPhase * Math.PI * 2) * hipSwayAmount * 0.12 * intensity\n      state.hipSpring.setTarget(hipSwayTarget)\n      state.hipSpring.update(dt)\n      const hipSway = state.hipSpring.value\n\n      // Hip rotation (twist) - counter to shoulders\n      const hipTwist = Math.sin(gaitPhase * Math.PI * 2) * 0.08 * intensity\n\n      // Hip drop on swing side\n      const hipDrop = hipDropCurve(gaitPhase) * 0.05 * intensity\n\n      // Forward hip oscillation (pelvic tilt during gait)\n      const hipForward = Math.sin(gaitPhase * Math.PI * 4) * 0.02 * intensity\n\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, hipSway)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, hipTwist))\n        hipsRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hipForward))\n        rig.setRotation('hips', hipsRot)\n\n        // Hip translation for bounce\n        const hipBounce = Math.abs(Math.sin(gaitPhase * Math.PI * 2)) * 0.01 * intensity\n        rig.setHipsPositionOffset(new Vector3(hipSway * 0.1, hipBounce, 0))\n      }\n\n      // ========================================\n      // LAYER 2: SPINE COUNTER-ROTATION\n      // ========================================\n\n      // Spine opposes hip movement for balance\n      const spineCounter = -hipTwist * 0.4\n      const spineSway = -hipSway * 0.3\n\n      if (rig.hasBone('spine')) {\n        const spineRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter * 0.5)\n        spineRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.5))\n        rig.setRotation('spine', spineRot)\n      }\n\n      if (rig.hasBone('chest')) {\n        const chestRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter * 0.8)\n        chestRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.3))\n        // Slight chest forward for confidence\n        chestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n        rig.setRotation('chest', chestRot)\n      }\n\n      if (rig.hasBone('upperChest')) {\n        const upperChestRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter)\n        upperChestRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.2))\n        upperChestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.04 * intensity))\n        rig.setRotation('upperChest', upperChestRot)\n      }\n\n      // ========================================\n      // LAYER 3: SHOULDER MOVEMENT\n      // ========================================\n\n      // Shoulders counter-rotate to hips\n      const shoulderTarget = -hipTwist * 0.6\n      state.shoulderSpring.setTarget(shoulderTarget)\n      state.shoulderSpring.update(dt)\n      const shoulderTwist = state.shoulderSpring.value\n\n      // Shoulder drop with arm swing\n      const leftShoulderDrop = armSwingCurve(gaitPhase, armFlowiness) * 0.02 * intensity\n      const rightShoulderDrop = armSwingCurve(gaitPhase + 0.5, armFlowiness) * 0.02 * intensity\n\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -leftShoulderDrop)\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, rightShoulderDrop)\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // ========================================\n      // LAYER 4: ARM SWING\n      // ========================================\n\n      // Fluid arm swing with secondary motion\n      const leftArmSwingBase = armSwingCurve(gaitPhase, armFlowiness) * 0.35 * intensity * armFlowiness\n      const rightArmSwingBase = armSwingCurve(gaitPhase + 0.5, armFlowiness) * 0.35 * intensity * armFlowiness\n\n      // Apply spring for smoothness\n      state.leftArmSpring.setTarget(leftArmSwingBase)\n      state.rightArmSpring.setTarget(rightArmSwingBase)\n      state.leftArmSpring.update(dt)\n      state.rightArmSpring.update(dt)\n\n      const leftArmSwing = state.leftArmSpring.value\n      const rightArmSwing = state.rightArmSpring.value\n\n      // Secondary motion - wrist lag\n      const leftWristLag = secondaryMotion ? leftArmSwing * 0.3 : 0\n      const rightWristLag = secondaryMotion ? rightArmSwing * 0.3 : 0\n\n      if (rig.hasBone('leftUpperArm')) {\n        const leftUpperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftArmSwing)\n        leftUpperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.08 * intensity))\n        rig.setRotation('leftUpperArm', leftUpperArmRot)\n      }\n\n      if (rig.hasBone('rightUpperArm')) {\n        const rightUpperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightArmSwing)\n        rightUpperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.08 * intensity))\n        rig.setRotation('rightUpperArm', rightUpperArmRot)\n      }\n\n      // Lower arms - slight bend with secondary motion\n      if (rig.hasBone('leftLowerArm')) {\n        const leftLowerRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.1 - leftWristLag * 0.2)\n        rig.setRotation('leftLowerArm', leftLowerRot)\n      }\n\n      if (rig.hasBone('rightLowerArm')) {\n        const rightLowerRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.1 + rightWristLag * 0.2)\n        rig.setRotation('rightLowerArm', rightLowerRot)\n      }\n\n      // Wrists - graceful rotation\n      if (rig.hasBone('leftHand')) {\n        const leftHandRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.1 + leftWristLag * 0.15)\n        leftHandRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftWristLag * 0.1))\n        rig.setRotation('leftHand', leftHandRot)\n      }\n\n      if (rig.hasBone('rightHand')) {\n        const rightHandRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.1 - rightWristLag * 0.15)\n        rightHandRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightWristLag * 0.1))\n        rig.setRotation('rightHand', rightHandRot)\n      }\n\n      // ========================================\n      // LAYER 5: LEG MOVEMENT\n      // ========================================\n\n      // Right leg\n      const rightLegForward = rightStance\n        ? -0.1 * (gaitPhase * 2) * intensity // Pushing back during stance\n        : Math.sin(rightSwingPhase * Math.PI) * 0.4 * intensity // Swinging forward\n\n      const rightLegAbduction = crossoverStep\n        ? Math.sin(gaitPhase * Math.PI * 2 + Math.PI) * 0.05 * intensity // Crossover\n        : 0\n\n      if (rig.hasBone('rightUpperLeg')) {\n        const rightUpperLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightLegForward)\n        rightUpperLegRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, rightLegAbduction))\n        rig.setRotation('rightUpperLeg', rightUpperLegRot)\n      }\n\n      // Right knee bend\n      const rightKneeBend = rightStance\n        ? 0.08 * intensity // Slight bend in stance\n        : Math.sin(rightSwingPhase * Math.PI) * 0.6 * intensity // Bend during swing\n\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightKneeBend))\n      }\n\n      // Left leg\n      const leftLegForward = leftStance\n        ? -0.1 * ((gaitPhase - 0.5) * 2) * intensity\n        : Math.sin(leftSwingPhase * Math.PI) * 0.4 * intensity\n\n      const leftLegAbduction = crossoverStep\n        ? Math.sin(gaitPhase * Math.PI * 2) * 0.05 * intensity\n        : 0\n\n      if (rig.hasBone('leftUpperLeg')) {\n        const leftUpperLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftLegForward)\n        leftUpperLegRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, leftLegAbduction))\n        rig.setRotation('leftUpperLeg', leftUpperLegRot)\n      }\n\n      const leftKneeBend = leftStance\n        ? 0.08 * intensity\n        : Math.sin(leftSwingPhase * Math.PI) * 0.6 * intensity\n\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftKneeBend))\n      }\n\n      // ========================================\n      // LAYER 6: FEET & TOES\n      // ========================================\n\n      // Right foot\n      const rightFootAngle = rightStance\n        ? -0.1 * intensity // Flat/slight heel strike\n        : (toePoint ? -0.3 * Math.sin(rightSwingPhase * Math.PI) * intensity : 0) // Toe point during swing\n\n      if (rig.hasBone('rightFoot')) {\n        const rightFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightFootAngle)\n        rightFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.05)) // Slight turn out\n        rig.setRotation('rightFoot', rightFootRot)\n      }\n\n      if (rig.hasBone('rightToes') && toePoint) {\n        const rightToePoint = rightStance ? 0 : Math.sin(rightSwingPhase * Math.PI) * 0.4 * intensity\n        rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightToePoint))\n      }\n\n      // Left foot\n      const leftFootAngle = leftStance\n        ? -0.1 * intensity\n        : (toePoint ? -0.3 * Math.sin(leftSwingPhase * Math.PI) * intensity : 0)\n\n      if (rig.hasBone('leftFoot')) {\n        const leftFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftFootAngle)\n        leftFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.05))\n        rig.setRotation('leftFoot', leftFootRot)\n      }\n\n      if (rig.hasBone('leftToes') && toePoint) {\n        const leftToePoint = leftStance ? 0 : Math.sin(leftSwingPhase * Math.PI) * 0.4 * intensity\n        rig.setRotation('leftToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftToePoint))\n      }\n\n      // ========================================\n      // LAYER 7: HEAD (STABLE WITH TILT)\n      // ========================================\n\n      // Head stays relatively stable (vestibular reflex)\n      const headCompensation = -hipSway * 0.3\n      const headTiltAmount = headTilt * 0.08 * intensity\n\n      // Subtle look direction\n      const lookDirection = noise.noise2D(t * 0.1, 100) * 0.05 * intensity\n\n      state.headSpring.setTarget(headCompensation)\n      state.headSpring.update(dt)\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, state.headSpring.value + headTiltAmount)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, lookDirection))\n        // Slight chin up for confidence\n        headRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        const neckRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, state.headSpring.value * 0.3)\n        neckRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.02 * intensity))\n        rig.setRotation('neck', neckRot)\n      }\n\n      // ========================================\n      // LAYER 8: FINGERS\n      // ========================================\n\n      if (fingerAnimation) {\n        // Graceful, slightly spread fingers\n        const fingerCurl = 0.15 + noise.noise2D(t * 0.2, 200) * 0.05\n\n        applyFingerCurl(rig, 'left', {\n          thumb: fingerCurl * 0.4,\n          index: fingerCurl * 0.8,\n          middle: fingerCurl * 0.9,\n          ring: fingerCurl,\n          little: fingerCurl * 1.1,\n        })\n\n        applyFingerCurl(rig, 'right', {\n          thumb: fingerCurl * 0.4,\n          index: fingerCurl * 0.8,\n          middle: fingerCurl * 0.9,\n          ring: fingerCurl,\n          little: fingerCurl * 1.1,\n        })\n\n        // Elegant spread\n        applyFingerSpread(rig, 'left', 0.4 * intensity)\n        applyFingerSpread(rig, 'right', 0.4 * intensity)\n      }\n\n      // ========================================\n      // LAYER 9: EYES\n      // ========================================\n\n      // Confident forward gaze with subtle movement\n      const eyeX = noise.noise2D(t * 0.15, 300) * 0.03 * intensity\n      const eyeY = noise.noise2D(t * 0.12, 400) * 0.02 * intensity\n\n      // Blinking\n      const blinkChance = noise.noise2D(t * 0.25, 500)\n      if (!state.isBlinking && blinkChance > 0.95) {\n        state.isBlinking = true\n        state.blinkTimer = 0\n      }\n\n      let blinkAmount = 0\n      if (state.isBlinking) {\n        state.blinkTimer += dt\n        if (state.blinkTimer > 0.15) {\n          state.isBlinking = false\n        } else {\n          blinkAmount = Math.sin(state.blinkTimer / 0.15 * Math.PI) * 0.25\n        }\n      }\n\n      if (rig.hasBone('leftEye')) {\n        const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n        leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n        rig.setRotation('leftEye', leftEyeRot)\n      }\n\n      if (rig.hasBone('rightEye')) {\n        const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n        rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n        rig.setRotation('rightEye', rightEyeRot)\n      }\n    },\n  }\n}\n\nexport const seductiveWalk: MotionProgram<SeductiveWalkParams> = createSeductiveWalk({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"The documentation does not sufficiently specify the timing delays between body parts as required by the output constraints.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"The 'createSeductiveWalk' function does not explicitly define the biomechanical or observational research basis for the nuanced motions described, such as hip sway or shoulder counter-rotation.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"The 'getGaitPhase' function does not account for any variations or delays in gait cycle, which may misrepresent the timing relationships specified in outputs.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Secondary motion for the shoulders and arms needs enhanced detail regarding how it flows with respect to the hips and spine, as this is a critical part of the animation.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    }\n  ],\n  \"summary\": \"The block generally adheres to the domain intent of creating a seductive walk animation with a realistic gait cycle. However, it failed validation due to insufficient documentation regarding timing relationships and biomechanical foundations. Additionally, some animations lacked explicit clarity on secondary motions, which is crucial for achieving the intended fluidity and believability of motion.\"\n}",
            "tokensUsed": {
              "input": 6881,
              "output": 288
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "contemplative_lean",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/contemplative-lean.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "contemplative_lean",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe contemplative body language",
                    "DOCUMENTATION: Must specify pose variants and their meanings",
                    "ASYMMETRY: Clear weight distribution (one leg dominant)",
                    "ASYMMETRY: Hip drop on non-weight side",
                    "ASYMMETRY: Spine S-curve to balance",
                    "VARIANTS: chin_rest, crossed_arms, akimbo, hand_on_hip",
                    "VARIANTS: Each has specific arm/hand configuration",
                    "BREATH: Deep, slow (5-6 second cycles)",
                    "BREATH: Visible chest expansion",
                    "GESTURES: Thinking micro-movements (chin tap, lip touch)",
                    "GESTURES: Very slow and deliberate",
                    "GESTURES: Phase envelopes for gesture timing",
                    "HANDS: Detailed finger pose per variant",
                    "HANDS: Subtle finger adjustments",
                    "LEGS: Weight leg straight-ish, relaxed leg bent",
                    "FEET: Weight foot flat, relaxed foot on ball",
                    "HEAD: Tilted slightly, distant gaze angle",
                    "EYES: Slow movement, distant focus",
                    "EYES: Slow, contemplative blinks",
                    "JAW: Relaxed, occasional subtle movement",
                    "MICRO: Very slow noise layer (meditative)",
                    "TRANSITIONS: Smooth between gesture variants"
                  ]
                }
              ],
              "description": "Thoughtful asymmetric pose with weight on one leg and self-reflective gestures.\nMultiple variants: chin rest, crossed arms, akimbo, hand on hip.\n\nFEEL: Lost in thought, processing something, introspective. Like solving a puzzle.\n\nTIMING: Very slow micro-movements. Breath is deep and slow (5-6 seconds).\nThinking gestures are deliberate, not fidgety. Weight shifts are rare and slow.\nEyes have \"distant\" quality - not tracking environment.\n\nASYMMETRY: Weight mostly on one leg. Opposite hip drops. Spine curves.\nArms break symmetry with gesture variant.\n",
              "path": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Thoughtful asymmetric pose with weight on one leg ...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 5970,
          "issues": [
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Missing detailed timing relationships between body parts in the documentation.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Documented gestures are not explicitly tied to the animation states, which could lead to inconsistent behaviors.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The `createContemplativeLean` function should provide explicit mechanisms to handle missing optional bones in the documentation, which is currently not elaborated.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Documentation does not cite biomechanical or observational research bases for specific gestures and animations, as required by the domain rules.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The default values used in the parameters schema do not adequately express the contemplative feel as described in the documentation.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "contemplative-lean.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block fails validation due to insufficient documentation around timing relationships and the lack of explicit ties between documented gestures and their respective animation states. Additionally, the current parameter defaults do not sufficiently promote the contemplative feel that is essential for the motion, and there are warnings regarding missing biomechanical citations in the documentation."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt": "Block Name: contemplative_lean\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Thoughtful asymmetric pose with weight on one leg and self-reflective gestures.\\nMultiple variants: chin rest, crossed arms, akimbo, hand on hip.\\n\\nFEEL: Lost in thought, processing something, introspective. Like solving a puzzle.\\n\\nTIMING: Very slow micro-movements. Breath is deep and slow (5-6 seconds).\\nThinking gestures are deliberate, not fidgety. Weight shifts are rare and slow.\\nEyes have \\\"distant\\\" quality - not tracking environment.\\n\\nASYMMETRY: Weight mostly on one leg. Opposite hip drops. Spine curves.\\nArms break symmetry with gesture variant.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe contemplative body language\",\n        \"DOCUMENTATION: Must specify pose variants and their meanings\",\n        \"ASYMMETRY: Clear weight distribution (one leg dominant)\",\n        \"ASYMMETRY: Hip drop on non-weight side\",\n        \"ASYMMETRY: Spine S-curve to balance\",\n        \"VARIANTS: chin_rest, crossed_arms, akimbo, hand_on_hip\",\n        \"VARIANTS: Each has specific arm/hand configuration\",\n        \"BREATH: Deep, slow (5-6 second cycles)\",\n        \"BREATH: Visible chest expansion\",\n        \"GESTURES: Thinking micro-movements (chin tap, lip touch)\",\n        \"GESTURES: Very slow and deliberate\",\n        \"GESTURES: Phase envelopes for gesture timing\",\n        \"HANDS: Detailed finger pose per variant\",\n        \"HANDS: Subtle finger adjustments\",\n        \"LEGS: Weight leg straight-ish, relaxed leg bent\",\n        \"FEET: Weight foot flat, relaxed foot on ball\",\n        \"HEAD: Tilted slightly, distant gaze angle\",\n        \"EYES: Slow movement, distant focus\",\n        \"EYES: Slow, contemplative blinks\",\n        \"JAW: Relaxed, occasional subtle movement\",\n        \"MICRO: Very slow noise layer (meditative)\",\n        \"TRANSITIONS: Smooth between gesture variants\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- contemplative-lean.ts ---\n```\n/**\n * Contemplative Lean Motion\n *\n * Thoughtful asymmetric pose with weight on one leg and thinking gestures.\n * Multiple pose variants including chin rest, crossed arms, and akimbo.\n *\n * Research basis:\n * - Thoughtful posture psychology\n * - Asymmetric stance biomechanics\n * - Self-touch gestures in contemplation\n * - Weight-bearing postural patterns\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const contemplativeLeanParamsSchema = z.object({\n  /** Pose variant. Default: chin_rest */\n  poseVariant: z.enum(['chin_rest', 'crossed_arms', 'akimbo', 'hand_on_hip']).default('chin_rest'),\n  /** Intensity of thinking gestures (0-1). Default: 0.5 */\n  thoughtIntensity: z.number().min(0).max(1).default(0.5),\n  /** Depth of breathing (0-1). Default: 0.6 */\n  breathDepth: z.number().min(0).max(1).default(0.6),\n  /** Amount of subtle fidgeting (0-1). Default: 0.3 */\n  fidgetAmount: z.number().min(0).max(1).default(0.3),\n  /** Which leg bears weight. Default: right */\n  weightLeg: z.enum(['left', 'right']).default('right'),\n  /** Overall animation intensity (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Enable eye movement. Default: true */\n  eyeMovement: z.boolean().default(true),\n  /** Enable head tilts. Default: true */\n  headTilts: z.boolean().default(true),\n})\n\nexport type ContemplativeLeanParams = z.infer<typeof contemplativeLeanParamsSchema>\nexport type ContemplativeLeanInput = z.input<typeof contemplativeLeanParamsSchema>\n\nexport const contemplativeLeanMeta: MotionMeta = {\n  id: 'contemplative-lean',\n  name: 'Contemplative Lean',\n  description: 'Thoughtful asymmetric pose with weight on one leg and thinking gestures',\n  tags: ['contemplative', 'thinking', 'pose', 'idle'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface ContemplativeLeanState {\n  noise: NoiseGenerator\n  weightSpring: Spring\n  headSpring: Spring\n  armSpring: Spring\n  blinkTimer: number\n  isBlinking: boolean\n  thinkingGestureTimer: number\n  currentGesture: 'idle' | 'head_tilt' | 'chin_tap' | 'look_away'\n  gestureDuration: number\n}\n\nfunction initState(seed: number): ContemplativeLeanState {\n  return {\n    noise: createNoiseGenerator(seed),\n    weightSpring: createSpring({ stiffness: 30, damping: 12 }),\n    headSpring: createSpring({ stiffness: 50, damping: 15 }),\n    armSpring: createSpring({ stiffness: 40, damping: 10 }),\n    blinkTimer: 0,\n    isBlinking: false,\n    thinkingGestureTimer: 0,\n    currentGesture: 'idle',\n    gestureDuration: 3,\n  }\n}\n\n// ============================================================================\n// POSE VARIANTS\n// ============================================================================\n\ninterface ArmPose {\n  leftUpperArm: { x: number; y: number; z: number }\n  leftLowerArm: { x: number; y: number; z: number }\n  leftHand: { x: number; y: number; z: number }\n  rightUpperArm: { x: number; y: number; z: number }\n  rightLowerArm: { x: number; y: number; z: number }\n  rightHand: { x: number; y: number; z: number }\n  leftFingers: { thumb: number; index: number; middle: number; ring: number; little: number }\n  rightFingers: { thumb: number; index: number; middle: number; ring: number; little: number }\n}\n\nfunction getArmPose(variant: string, intensity: number): ArmPose {\n  switch (variant) {\n    case 'chin_rest':\n      return {\n        // Right arm: hand to chin\n        rightUpperArm: { x: -0.8 * intensity, y: 0, z: -0.4 * intensity },\n        rightLowerArm: { x: 0, y: 1.3 * intensity, z: 0 },\n        rightHand: { x: 0.2 * intensity, y: 0, z: 0.1 * intensity },\n        // Left arm: support under right elbow\n        leftUpperArm: { x: 0.3 * intensity, y: 0, z: 0.25 * intensity },\n        leftLowerArm: { x: 0, y: -1.0 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.15 * intensity },\n        // Fingers\n        rightFingers: { thumb: 0.2, index: 0.15, middle: 0.35, ring: 0.5, little: 0.55 },\n        leftFingers: { thumb: 0.3, index: 0.35, middle: 0.4, ring: 0.45, little: 0.5 },\n      }\n\n    case 'crossed_arms':\n      return {\n        rightUpperArm: { x: 0.5 * intensity, y: 0, z: -0.35 * intensity },\n        rightLowerArm: { x: 0, y: 1.4 * intensity, z: 0 },\n        rightHand: { x: 0, y: 0, z: -0.2 * intensity },\n        leftUpperArm: { x: 0.4 * intensity, y: 0, z: 0.25 * intensity },\n        leftLowerArm: { x: 0, y: -1.3 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.15 * intensity },\n        rightFingers: { thumb: 0.2, index: 0.25, middle: 0.3, ring: 0.35, little: 0.4 },\n        leftFingers: { thumb: 0.25, index: 0.3, middle: 0.35, ring: 0.4, little: 0.45 },\n      }\n\n    case 'akimbo':\n      return {\n        rightUpperArm: { x: 0.15 * intensity, y: 0, z: -0.5 * intensity },\n        rightLowerArm: { x: 0, y: 1.4 * intensity, z: 0 },\n        rightHand: { x: -0.3 * intensity, y: 0, z: 0 },\n        leftUpperArm: { x: 0.15 * intensity, y: 0, z: 0.5 * intensity },\n        leftLowerArm: { x: 0, y: -1.4 * intensity, z: 0 },\n        leftHand: { x: -0.3 * intensity, y: 0, z: 0 },\n        rightFingers: { thumb: 0.1, index: 0.15, middle: 0.2, ring: 0.25, little: 0.3 },\n        leftFingers: { thumb: 0.1, index: 0.15, middle: 0.2, ring: 0.25, little: 0.3 },\n      }\n\n    case 'hand_on_hip':\n      return {\n        // Right hand on hip\n        rightUpperArm: { x: 0.2 * intensity, y: 0, z: -0.45 * intensity },\n        rightLowerArm: { x: 0, y: 1.3 * intensity, z: 0 },\n        rightHand: { x: -0.25 * intensity, y: 0, z: -0.1 * intensity },\n        // Left arm relaxed\n        leftUpperArm: { x: 0.05 * intensity, y: 0, z: 0.12 * intensity },\n        leftLowerArm: { x: 0, y: -0.15 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.1 * intensity },\n        rightFingers: { thumb: 0.15, index: 0.2, middle: 0.25, ring: 0.3, little: 0.35 },\n        leftFingers: { thumb: 0.25, index: 0.3, middle: 0.35, ring: 0.4, little: 0.45 },\n      }\n\n    default:\n      return getArmPose('chin_rest', intensity)\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a contemplative lean motion with multiple pose variants.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (jaw, eyes, toes, fingers) are gracefully skipped\n * - Dynamic bone names (weightLeg, relaxLeg) are cast and checked\n * - The animation degrades gracefully when bones are unavailable\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - Gesture selection is deterministic via seeded noise\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - Spring physics use efficient semi-implicit Euler integration\n * - Pose variant lookup is constant time\n * - No allocations in hot path except quaternion creation\n *\n * TRANSITIONS:\n * - Thinking gesture state machine provides fluid transitions\n * - Gesture weights use sine easing for smooth in/out\n * - Springs smooth all micro-movements\n */\nexport function createContemplativeLean(params: ContemplativeLeanInput = {}): MotionProgram<ContemplativeLeanParams> {\n  const validatedParams = contemplativeLeanParamsSchema.parse(params)\n  let state: ContemplativeLeanState | null = null\n\n  return {\n    meta: contemplativeLeanMeta,\n    paramsSchema: contemplativeLeanParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        poseVariant,\n        thoughtIntensity,\n        breathDepth,\n        fidgetAmount,\n        weightLeg,\n        intensity,\n        eyeMovement,\n        headTilts,\n      } = validatedParams\n\n      const noise = state.noise\n      const isRightWeight = weightLeg === 'right'\n      const weightSide = isRightWeight ? 1 : -1\n\n      // ========================================\n      // THINKING GESTURE STATE MACHINE\n      // ========================================\n\n      state.thinkingGestureTimer += dt\n      if (state.thinkingGestureTimer > state.gestureDuration) {\n        state.thinkingGestureTimer = 0\n        state.gestureDuration = 2 + noise.noise2D(t, 50) * 3\n\n        // Pick next gesture\n        const gestureRoll = noise.noise2D(t, 100)\n        if (gestureRoll > 0.7 && headTilts) {\n          state.currentGesture = 'head_tilt'\n        } else if (gestureRoll > 0.4 && poseVariant === 'chin_rest') {\n          state.currentGesture = 'chin_tap'\n        } else if (gestureRoll > 0.2) {\n          state.currentGesture = 'look_away'\n        } else {\n          state.currentGesture = 'idle'\n        }\n      }\n\n      const gestureProgress = state.thinkingGestureTimer / state.gestureDuration\n      const gestureWeight = Math.sin(gestureProgress * Math.PI) * thoughtIntensity\n\n      // ========================================\n      // LAYER 1: ASYMMETRIC WEIGHT DISTRIBUTION\n      // ========================================\n\n      // Hip drop on non-weight-bearing side\n      const hipDrop = 0.06 * intensity * weightSide\n      const hipShift = 0.04 * intensity * weightSide\n\n      // Subtle weight shift noise\n      const weightNoise = noise.noise2D(t * 0.08, 200) * fidgetAmount * 0.02\n      state.weightSpring.setTarget(hipShift + weightNoise)\n      state.weightSpring.update(dt)\n\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, hipDrop + state.weightSpring.value)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightSide * 0.03 * intensity))\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // ========================================\n      // LAYER 2: SPINE (RELAXED, SLIGHT CURVE)\n      // ========================================\n\n      if (rig.hasBone('spine')) {\n        const spineRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.3)\n        spineRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03 * intensity))\n        rig.setRotation('spine', spineRot)\n      }\n\n      if (rig.hasBone('chest')) {\n        const chestRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.2)\n        chestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n        rig.setRotation('chest', chestRot)\n      }\n\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.1))\n      }\n\n      // ========================================\n      // LAYER 3: DEEP CONTEMPLATIVE BREATHING\n      // ========================================\n\n      const breathPhase = oscBreathing(t, 0.12, breathDepth)\n\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.025))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.035))\n      }\n\n      // Shoulder rise with breath\n      const shoulderBreath = breathPhase * 0.012\n      if (rig.hasBone('leftShoulder')) {\n        rig.setRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.setRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 4: LEGS (WEIGHT DISTRIBUTION)\n      // ========================================\n\n      // Weight-bearing leg: straight\n      // Non-weight leg: bent, relaxed\n\n      const weightLegBone = isRightWeight ? 'rightUpperLeg' : 'leftUpperLeg'\n      const relaxLegBone = isRightWeight ? 'leftUpperLeg' : 'rightUpperLeg'\n      const weightKnee = isRightWeight ? 'rightLowerLeg' : 'leftLowerLeg'\n      const relaxKnee = isRightWeight ? 'leftLowerLeg' : 'rightLowerLeg'\n      const weightFoot = isRightWeight ? 'rightFoot' : 'leftFoot'\n      const relaxFoot = isRightWeight ? 'leftFoot' : 'rightFoot'\n\n      if (rig.hasBone(weightLegBone as VRMHumanBoneName)) {\n        // Slight hip flexion on weight-bearing side\n        rig.setRotation(weightLegBone as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n      }\n\n      if (rig.hasBone(relaxLegBone as VRMHumanBoneName)) {\n        // Relaxed leg forward and slightly bent\n        const relaxRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.15 * intensity)\n        relaxRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -weightSide * 0.08 * intensity))\n        rig.setRotation(relaxLegBone as VRMHumanBoneName, relaxRot)\n      }\n\n      if (rig.hasBone(weightKnee as VRMHumanBoneName)) {\n        // Straight-ish weight-bearing knee\n        rig.setRotation(weightKnee as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n      }\n\n      if (rig.hasBone(relaxKnee as VRMHumanBoneName)) {\n        // Bent relaxed knee\n        rig.setRotation(relaxKnee as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.2 * intensity))\n      }\n\n      // Feet\n      if (rig.hasBone(weightFoot as VRMHumanBoneName)) {\n        rig.setRotation(weightFoot as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightSide * 0.08))\n      }\n\n      if (rig.hasBone(relaxFoot as VRMHumanBoneName)) {\n        // Relaxed foot on ball/toe\n        const relaxFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.15 * intensity)\n        relaxFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -weightSide * 0.1))\n        rig.setRotation(relaxFoot as VRMHumanBoneName, relaxFootRot)\n      }\n\n      // Toes\n      const weightToes = isRightWeight ? 'rightToes' : 'leftToes'\n      const relaxToes = isRightWeight ? 'leftToes' : 'rightToes'\n\n      if (rig.hasBone(weightToes as VRMHumanBoneName)) {\n        rig.setRotation(weightToes as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.05))\n      }\n      if (rig.hasBone(relaxToes as VRMHumanBoneName)) {\n        rig.setRotation(relaxToes as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.1 * intensity))\n      }\n\n      // ========================================\n      // LAYER 5: ARMS (POSE VARIANT)\n      // ========================================\n\n      const armPose = getArmPose(poseVariant, intensity)\n\n      // Add subtle arm movement/fidget\n      const armFidget = noise.noise2D(t * 0.15, 300) * fidgetAmount * 0.02\n      state.armSpring.setTarget(armFidget)\n      state.armSpring.update(dt)\n      const armNoise = state.armSpring.value\n\n      if (rig.hasBone('leftUpperArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftUpperArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftUpperArm.y + armNoise))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftUpperArm.z))\n        rig.setRotation('leftUpperArm', rot)\n      }\n\n      if (rig.hasBone('leftLowerArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftLowerArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftLowerArm.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftLowerArm.z))\n        rig.setRotation('leftLowerArm', rot)\n      }\n\n      if (rig.hasBone('leftHand')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftHand.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftHand.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftHand.z))\n        rig.setRotation('leftHand', rot)\n      }\n\n      if (rig.hasBone('rightUpperArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightUpperArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightUpperArm.y - armNoise))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightUpperArm.z))\n        rig.setRotation('rightUpperArm', rot)\n      }\n\n      if (rig.hasBone('rightLowerArm')) {\n        // Add chin tap gesture for chin_rest variant\n        let chinTapAdd = 0\n        if (poseVariant === 'chin_rest' && state.currentGesture === 'chin_tap') {\n          chinTapAdd = Math.sin(state.thinkingGestureTimer * 6) * 0.05 * gestureWeight\n        }\n\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightLowerArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightLowerArm.y + chinTapAdd))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightLowerArm.z))\n        rig.setRotation('rightLowerArm', rot)\n      }\n\n      if (rig.hasBone('rightHand')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightHand.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightHand.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightHand.z))\n        rig.setRotation('rightHand', rot)\n      }\n\n      // ========================================\n      // LAYER 5B: DETAILED FINGER ANIMATION\n      // ========================================\n      // Animate all 30 finger bones (15 per hand) with pose-specific positioning\n      // Each finger has 3 joints: Proximal, Intermediate, Distal\n      // Bones: thumb/index/middle/ring/little + Proximal/Intermediate/Distal\n      //\n      // Apply base finger curl from pose, plus subtle micro-movements\n\n      // Add subtle finger fidget for organic feel\n      const fingerFidget = noise.noise2D(t * 0.25, 1100) * fidgetAmount * 0.1\n\n      // Left hand finger bones - explicit animation of all joints\n      const leftFingerCurl = {\n        thumb: armPose.leftFingers.thumb + fingerFidget * 0.3,\n        index: armPose.leftFingers.index + fingerFidget * 0.5,\n        middle: armPose.leftFingers.middle + fingerFidget * 0.6,\n        ring: armPose.leftFingers.ring + fingerFidget * 0.7,\n        little: armPose.leftFingers.little + fingerFidget * 0.8,\n      }\n\n      // Right hand finger bones - with thinking gesture adjustments\n      const thinkingFingerMod = state.currentGesture === 'chin_tap' ? gestureWeight * 0.1 : 0\n      const rightFingerCurl = {\n        thumb: armPose.rightFingers.thumb + thinkingFingerMod,\n        index: armPose.rightFingers.index + thinkingFingerMod * 1.5, // Index finger more active in chin tap\n        middle: armPose.rightFingers.middle + thinkingFingerMod * 0.8,\n        ring: armPose.rightFingers.ring + fingerFidget * 0.5,\n        little: armPose.rightFingers.little + fingerFidget * 0.6,\n      }\n\n      // Apply finger curls - this internally handles all 15 bones per hand\n      // with hasBone() checks for each: Proximal, Intermediate, Distal joints\n      applyFingerCurl(rig, 'left', leftFingerCurl)\n      applyFingerCurl(rig, 'right', rightFingerCurl)\n\n      // ========================================\n      // LAYER 6: HEAD & NECK (THINKING)\n      // ========================================\n\n      let headTiltX = 0.02 * intensity // Slight down\n      let headTiltY = 0\n      let headTiltZ = 0.03 * intensity * weightSide // Tilt toward weight side\n\n      // Add thinking gestures\n      if (state.currentGesture === 'head_tilt') {\n        headTiltZ += gestureWeight * 0.05 * (noise.noise2D(t, 400) > 0 ? 1 : -1)\n        headTiltX += gestureWeight * 0.03\n      } else if (state.currentGesture === 'look_away') {\n        headTiltY = gestureWeight * 0.1 * (noise.noise2D(t, 500) > 0 ? 1 : -1)\n      }\n\n      // Micro-movement\n      const headMicroX = noise.noise2D(t * 0.2, 600) * 0.01 * intensity\n      const headMicroY = noise.noise2D(t * 0.15, 700) * 0.015 * intensity\n\n      state.headSpring.setTarget(headTiltY + headMicroY)\n      state.headSpring.update(dt)\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX + headMicroX)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, state.headSpring.value))\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headTiltZ))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        const neckRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX * 0.5)\n        neckRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, state.headSpring.value * 0.4))\n        neckRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headTiltZ * 0.3))\n        rig.setRotation('neck', neckRot)\n      }\n\n      // ========================================\n      // LAYER 7: EYES (CONTEMPLATIVE GAZE)\n      // ========================================\n\n      if (eyeMovement) {\n        // Slow, distant gaze with occasional focus shifts\n        let eyeX = 0\n        let eyeY = 0.02 * intensity // Slight downward gaze\n\n        if (state.currentGesture === 'look_away') {\n          eyeX = gestureWeight * 0.1 * (noise.noise2D(t, 800) > 0 ? 1 : -1)\n        } else {\n          eyeX = noise.noise2D(t * 0.1, 900) * 0.04 * intensity\n          eyeY += noise.noise2D(t * 0.08, 1000) * 0.02 * intensity\n        }\n\n        // Slow blinking (contemplative)\n        const blinkChance = noise.noise2D(t * 0.2, 1100)\n        if (!state.isBlinking && blinkChance > 0.92) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          const blinkDuration = 0.2 // Slower, contemplative blink\n          if (state.blinkTimer > blinkDuration) {\n            state.isBlinking = false\n          } else {\n            blinkAmount = Math.sin(state.blinkTimer / blinkDuration * Math.PI) * 0.25\n          }\n        }\n\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 8: JAW (OCCASIONAL SUBTLE MOVEMENT)\n      // ========================================\n\n      if (rig.hasBone('jaw')) {\n        // Very subtle jaw movement as if thinking words\n        const jawThink = noise.noise2D(t * 0.3, 1200) > 0.85\n          ? noise.noise2D(t * 2, 1300) * 0.02 * thoughtIntensity\n          : 0\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawThink))\n      }\n    },\n  }\n}\n\nexport const contemplativeLean: MotionProgram<ContemplativeLeanParams> = createContemplativeLean({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Missing detailed timing relationships between body parts in the documentation.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"Documented gestures are not explicitly tied to the animation states, which could lead to inconsistent behaviors.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"The `createContemplativeLean` function should provide explicit mechanisms to handle missing optional bones in the documentation, which is currently not elaborated.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"Documentation does not cite biomechanical or observational research bases for specific gestures and animations, as required by the domain rules.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"The default values used in the parameters schema do not adequately express the contemplative feel as described in the documentation.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    }\n  ],\n  \"summary\": \"The block fails validation due to insufficient documentation around timing relationships and the lack of explicit ties between documented gestures and their respective animation states. Additionally, the current parameter defaults do not sufficiently promote the contemplative feel that is essential for the motion, and there are warnings regarding missing biomechanical citations in the documentation.\"\n}",
            "tokensUsed": {
              "input": 8132,
              "output": 293
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    }
  ]
}