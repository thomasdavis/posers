{
  "version": "1.0",
  "id": "2026-01-03T11-32-12-325Z",
  "timestamp": "2026-01-03T11:36:53.945Z",
  "configPath": "blocks.yml",
  "projectName": "Posers Motion Engine",
  "duration": 281706,
  "summary": {
    "totalBlocks": 5,
    "passed": 0,
    "failed": 5,
    "warnings": 0,
    "cached": {
      "validatorsSkipped": 0,
      "validatorsRun": 10,
      "timeSavedMs": 0
    }
  },
  "blocks": [
    {
      "blockName": "confident_stance",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/confident-stance.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 1,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "confident_stance",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must have detailed JSDoc describing confident body language",
                    "CORE: Spine chain must show upright confident posture",
                    "CORE: Hips stable with micro weight shifts",
                    "BREATH: Deep, slow breathing (4-5 second cycle)",
                    "BREATH: Chest expands on inhale, shoulders rise slightly",
                    "ARMS: Relaxed at sides or hands loosely clasped",
                    "ARMS: Subtle forearm rotation with breath",
                    "HANDS: Fingers relaxed but with presence, not limp",
                    "HANDS: All 30 finger bones should have subtle curl",
                    "LEGS: Weight distributed evenly or slightly forward",
                    "LEGS: Knees not locked, micro-flex",
                    "FEET: Toes engaged, gripping ground subtly",
                    "HEAD: Chin slightly elevated, gaze forward",
                    "EYES: Purposeful movement, slow blinks",
                    "JAW: Relaxed but closed",
                    "MICRO: Noise layer on all bones for life"
                  ]
                }
              ],
              "description": "Power pose with commanding presence. Weight evenly distributed or slightly forward.\nChest open, shoulders back but relaxed. Subtle assertive micro-movements.\n\nFEEL: Grounded, assured, ready. Like a CEO about to address the board.\n\nTIMING: Breath drives subtle chest expansion (4-5 second cycle). Weight micro-shifts\nevery 2-3 seconds. Eyes move with purpose, not darting. Fingers relaxed but not limp.\n",
              "path": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Power pose with commanding presence. Weight evenly...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 58949,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Breath timing does not match domain requirement. The block requires a 4–5 second breathing cycle, but confidentStanceParamsSchema.breathRate defaults to 0.15 Hz (≈6.67 s period). This violates the 'BREATH: Deep, slow breathing (4-5 second cycle)' constraint.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Top-of-file documentation is present but not detailed enough. The domain requires every motion file to begin with detailed documentation that describes HOW the motion should FEEL and specifies timing relationships between body parts (breath cycle, weight shift cadence, phase relationships, citations). The header lacks explicit timing relationships (e.g. breath 4–5 s, weight micro-shifts every 2–3 s) and not enough biomechanical citation detail.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The implementation does not add forearm (lower arm) rotation coupled to breath. The 'ARMS: Subtle forearm rotation with breath' constraint expects lower-arm rotations modulated by breathing, but the breathing layer only modifies chest and shoulders (lower arms are set statically).",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Jaw bone is not driven. The constraint 'JAW: Relaxed but closed' is not implemented — there is no manipulation of a jaw bone (e.g. 'jaw' or 'lowerJaw') to ensure a relaxed/closed jaw posture.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Micro-noise is not applied to 'all bones'. The domain requires a noise layer across all bones (and consideration of all 69 VRM bones). The code applies micro-noise to head and upper arms only; many bones (spine segments beyond 'spine'/'chest'/'upperChest', pelvis details, clavicles, forearms, hands, finger joints, leg bones beyond setRotation entries, etc.) are not covered systematically.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Finger operations are not wrapped in explicit hasBone() checks. applyFingerCurl() and applyFingerSpread() are called without hasBone() guards in this file. The domain rule 'hasBone() checks wrap all optional bone access' is not demonstrably satisfied here (it may be satisfied internally by the helpers, but the source does not show explicit guards).",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "No explicit guarantee that all 30 finger bones receive subtle curl. The output constraint requires subtle curl on all 30 finger bones; the code calls applyFingerCurl() with per-finger factors but does not explicitly enumerate or assert that all finger bones are present/modified.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Weight bias logic appears incorrect and likely produces unintended weighting. The leftWeight/rightWeight computation is: leftWeight = weightBias < 0 ? 1 : 1 - weightBias; rightWeight = weightBias > 0 ? 1 : 1 + weightBias. For positive weightBias the rightWeight is forced to 1 regardless of magnitude, which does not correctly reflect a continuous bias range (-1..1). This is likely a bug that affects 'LEGS: Weight distributed evenly or slightly forward' and micro-weighting behaviors.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Some core bones are operated without explicit clamping to anatomical limits. The domain requires joint rotations to respect anatomical limits; the code uses small values (good practice) but does not enforce/clamp limits explicitly or document the ranges used.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Unused imports present: BoneChains and getAvailableBones are imported but not used. Remove or use them to avoid introducing unexplained concepts in the file header.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Blink interval calculation is unconventional and may produce unstable intervals. It uses floor(state.lastBlinkTime * 10) as a seed index into noise.noise2D, then multiplies noise by 2 and offsets 3 seconds. This is a behavioral note (not runtime testable here) but could be made clearer/documented in the file-level timing notes.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "confident-stance.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block is well-structured and expresses many aspects of a confident stance (spine/chest/head posture, shoulder/back, breathing influence on chest/shoulders, eye motion/blink logic, finger curl helpers). However it fails multiple domain constraints: breath rate default does not match the required 4–5s cycle; forearm/breath coupling is missing; jaw control is absent; micro-noise is not applied across all bones and the code does not demonstrate handling of all 69 VRM bones; finger bone coverage and hasBone guards are not explicitly enforced; weight-bias math appears incorrect. Additionally, the required detailed documentation with explicit timing relationships is inadequate. Because of these domain-rule violations, the block does not pass validation."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: confident_stance\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Power pose with commanding presence. Weight evenly distributed or slightly forward.\\nChest open, shoulders back but relaxed. Subtle assertive micro-movements.\\n\\nFEEL: Grounded, assured, ready. Like a CEO about to address the board.\\n\\nTIMING: Breath drives subtle chest expansion (4-5 second cycle). Weight micro-shifts\\nevery 2-3 seconds. Eyes move with purpose, not darting. Fingers relaxed but not limp.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/confident-stance.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must have detailed JSDoc describing confident body language\",\n        \"CORE: Spine chain must show upright confident posture\",\n        \"CORE: Hips stable with micro weight shifts\",\n        \"BREATH: Deep, slow breathing (4-5 second cycle)\",\n        \"BREATH: Chest expands on inhale, shoulders rise slightly\",\n        \"ARMS: Relaxed at sides or hands loosely clasped\",\n        \"ARMS: Subtle forearm rotation with breath\",\n        \"HANDS: Fingers relaxed but with presence, not limp\",\n        \"HANDS: All 30 finger bones should have subtle curl\",\n        \"LEGS: Weight distributed evenly or slightly forward\",\n        \"LEGS: Knees not locked, micro-flex\",\n        \"FEET: Toes engaged, gripping ground subtly\",\n        \"HEAD: Chin slightly elevated, gaze forward\",\n        \"EYES: Purposeful movement, slow blinks\",\n        \"JAW: Relaxed but closed\",\n        \"MICRO: Noise layer on all bones for life\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- confident-stance.ts ---\n```\n/**\n * Confident Stance Motion\n *\n * A power pose with subtle weight distribution, commanding posture,\n * and organic micro-movements. Uses all available bones for maximum\n * realism with layered animation approach.\n *\n * Research basis:\n * - Power pose psychology (Carney et al.)\n * - Postural sway biomechanics\n * - Weight distribution patterns\n * - Breathing mechanics in standing posture\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  getAvailableBones,\n  applyFingerCurl,\n  applyFingerSpread,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const confidentStanceParamsSchema = z.object({\n  /** Overall intensity of the pose (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Breath rate in Hz. Default: 0.15 (slower, confident breathing) */\n  breathRate: z.number().min(0.05).max(0.5).default(0.15),\n  /** Amount of hip sway side-to-side (0-1). Default: 0.3 */\n  swayAmount: z.number().min(0).max(1).default(0.3),\n  /** Enable eye micro-movements. Default: true */\n  eyeMovement: z.boolean().default(true),\n  /** Enable finger movements. Default: true */\n  fingerMovement: z.boolean().default(true),\n  /** Weight distribution bias (-1=left, 0=center, 1=right). Default: 0.15 */\n  weightBias: z.number().min(-1).max(1).default(0.15),\n  /** Shoulder tension level (0=relaxed, 1=tense). Default: 0.2 */\n  shoulderTension: z.number().min(0).max(1).default(0.2),\n  /** Chest out amount (0-1). Default: 0.6 */\n  chestOut: z.number().min(0).max(1).default(0.6),\n  /** Chin up amount (0-1). Default: 0.3 */\n  chinUp: z.number().min(0).max(1).default(0.3),\n})\n\nexport type ConfidentStanceParams = z.infer<typeof confidentStanceParamsSchema>\nexport type ConfidentStanceInput = z.input<typeof confidentStanceParamsSchema>\n\nexport const confidentStanceMeta: MotionMeta = {\n  id: 'confident-stance',\n  name: 'Confident Stance',\n  description: 'Power pose with commanding presence, subtle breathing, and organic micro-movements',\n  tags: ['stance', 'confident', 'power', 'idle'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface ConfidentStanceState {\n  noise: NoiseGenerator\n  weightShiftSpring: Spring\n  blinkTimer: number\n  blinkDuration: number\n  isBlinking: boolean\n  lastBlinkTime: number\n}\n\n/**\n * Initialize deterministic state with seeded random generators.\n * All noise and random elements are seeded to ensure identical\n * output for the same seed value - required for deterministic playback.\n */\nfunction initState(seed: number): ConfidentStanceState {\n  // All random elements use the seed for deterministic, reproducible motion\n  return {\n    noise: createNoiseGenerator(seed), // Seeded simplex noise\n    weightShiftSpring: createSpring({ stiffness: 50, damping: 15 }),\n    blinkTimer: 0,\n    blinkDuration: 0.15,\n    isBlinking: false,\n    lastBlinkTime: 0,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\nexport function createConfidentStance(params: ConfidentStanceInput = {}): MotionProgram<ConfidentStanceParams> {\n  const validatedParams = confidentStanceParamsSchema.parse(params)\n  let state: ConfidentStanceState | null = null\n\n  return {\n    meta: confidentStanceMeta,\n    paramsSchema: confidentStanceParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      // Lazy initialization if init wasn't called\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        intensity,\n        breathRate,\n        swayAmount,\n        eyeMovement,\n        fingerMovement,\n        weightBias,\n        shoulderTension,\n        chestOut,\n        chinUp,\n      } = validatedParams\n\n      const noise = state.noise\n\n      // ========================================\n      // LAYER 1: BASE POSTURE\n      // ========================================\n\n      // Hips - slight forward tilt for confident stance\n      const hipsTilt = -0.03 * intensity // Slight posterior tilt\n      const hipsYaw = weightBias * 0.05 * intensity\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hipsTilt)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, hipsYaw))\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // Spine chain - tall, proud posture\n      const spineExtension = 0.02 * intensity * chestOut\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension * 0.5))\n      }\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension * 1.2))\n      }\n\n      // Neck and head - chin slightly up\n      const neckExtension = chinUp * 0.03 * intensity\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -neckExtension * 0.5))\n      }\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -neckExtension)\n        rig.setRotation('head', headRot)\n      }\n\n      // Shoulders - pulled back for confident posture\n      const shoulderPullBack = 0.08 * intensity * (1 - shoulderTension * 0.5)\n      const shoulderDown = shoulderTension * 0.05 * intensity\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderDown)\n        leftShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -shoulderPullBack))\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderDown)\n        rightShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, shoulderPullBack))\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // Arms - relaxed at sides with slight separation\n      const armAbduction = 0.12 * intensity\n      const armRelax = 0.08 * intensity\n      if (rig.hasBone('leftUpperArm')) {\n        const leftUpperArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armAbduction)\n        leftUpperArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armRelax))\n        rig.setRotation('leftUpperArm', leftUpperArmRot)\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const rightUpperArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -armAbduction)\n        rightUpperArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armRelax))\n        rig.setRotation('rightUpperArm', rightUpperArmRot)\n      }\n\n      // Lower arms - slightly bent\n      const elbowBend = 0.15 * intensity\n      if (rig.hasBone('leftLowerArm')) {\n        rig.setRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -elbowBend))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        rig.setRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, elbowBend))\n      }\n\n      // Hands - natural rotation\n      const handRotation = 0.1 * intensity\n      if (rig.hasBone('leftHand')) {\n        rig.setRotation('leftHand', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handRotation))\n      }\n      if (rig.hasBone('rightHand')) {\n        rig.setRotation('rightHand', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handRotation))\n      }\n\n      // Legs - weight distribution\n      const standingLegBend = 0.05 * intensity\n      const relaxedLegBend = 0.12 * intensity\n      const leftWeight = weightBias < 0 ? 1 : 1 - weightBias\n      const rightWeight = weightBias > 0 ? 1 : 1 + weightBias\n\n      if (rig.hasBone('leftUpperLeg')) {\n        const leftLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, standingLegBend * leftWeight + relaxedLegBend * (1 - leftWeight))\n        rig.setRotation('leftUpperLeg', leftLegRot)\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        const rightLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, standingLegBend * rightWeight + relaxedLegBend * (1 - rightWeight))\n        rig.setRotation('rightUpperLeg', rightLegRot)\n      }\n\n      // Knee slight bend for natural stance\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08 * leftWeight))\n      }\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08 * rightWeight))\n      }\n\n      // Feet - flat with slight outward rotation\n      const footOutward = 0.1 * intensity\n      if (rig.hasBone('leftFoot')) {\n        rig.setRotation('leftFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -footOutward))\n      }\n      if (rig.hasBone('rightFoot')) {\n        rig.setRotation('rightFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, footOutward))\n      }\n\n      // ========================================\n      // LAYER 2: BREATHING\n      // ========================================\n\n      const breathPhase = oscBreathing(t, breathRate, intensity)\n\n      // Add breathing to spine\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.015))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.02))\n      }\n\n      // Shoulder rise with breath\n      const shoulderBreath = breathPhase * 0.008\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 3: WEIGHT SHIFT\n      // ========================================\n\n      // Slow, subtle weight shifting\n      const weightShiftTarget = noise.noise2D(t * 0.15, 0) * swayAmount * 0.5\n      state.weightShiftSpring.setTarget(weightShiftTarget)\n      state.weightShiftSpring.update(dt)\n      const currentShift = state.weightShiftSpring.value\n\n      // Apply weight shift to hips\n      if (rig.hasBone('hips')) {\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, currentShift * 0.03))\n      }\n\n      // Counter-rotate spine\n      if (rig.hasBone('spine')) {\n        rig.addRotation('spine', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -currentShift * 0.015))\n      }\n\n      // ========================================\n      // LAYER 4: MICRO-MOVEMENTS\n      // ========================================\n\n      // Subtle noise on all joints for organic feel\n      const microIntensity = 0.003 * intensity\n\n      // Head micro-movements\n      if (rig.hasBone('head')) {\n        const headNoiseX = noise.noise2D(t * 0.3, 100) * microIntensity\n        const headNoiseY = noise.noise2D(t * 0.25, 200) * microIntensity\n        const headNoiseZ = noise.noise2D(t * 0.2, 300) * microIntensity * 0.5\n        rig.addRotation('head', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headNoiseX))\n        rig.addRotation('head', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headNoiseY))\n        rig.addRotation('head', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headNoiseZ))\n      }\n\n      // Upper arm micro-movements\n      if (rig.hasBone('leftUpperArm')) {\n        const armNoiseL = noise.noise2D(t * 0.2, 400) * microIntensity\n        rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armNoiseL))\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const armNoiseR = noise.noise2D(t * 0.2, 500) * microIntensity\n        rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armNoiseR))\n      }\n\n      // ========================================\n      // LAYER 5: FINGERS\n      // ========================================\n\n      if (fingerMovement) {\n        // Relaxed, slightly curled fingers\n        const baseCurl = 0.25 + noise.noise2D(t * 0.1, 600) * 0.1\n\n        // Left hand\n        applyFingerCurl(rig, 'left', {\n          thumb: baseCurl * 0.6,\n          index: baseCurl,\n          middle: baseCurl * 1.1,\n          ring: baseCurl * 1.15,\n          little: baseCurl * 1.2,\n        })\n\n        // Right hand\n        applyFingerCurl(rig, 'right', {\n          thumb: baseCurl * 0.6,\n          index: baseCurl,\n          middle: baseCurl * 1.1,\n          ring: baseCurl * 1.15,\n          little: baseCurl * 1.2,\n        })\n\n        // Slight finger spread\n        applyFingerSpread(rig, 'left', 0.2)\n        applyFingerSpread(rig, 'right', 0.2)\n      }\n\n      // ========================================\n      // LAYER 6: EYES\n      // ========================================\n\n      if (eyeMovement) {\n        // Slow, deliberate eye movement\n        const eyeSpeed = 0.15\n        const eyeX = noise.noise2D(t * eyeSpeed, 700) * 0.04 * intensity\n        const eyeY = noise.noise2D(t * eyeSpeed, 800) * 0.03 * intensity\n\n        // Blinking - deterministic intervals based on seed\n        const timeSinceLastBlink = t - state.lastBlinkTime\n        // Use floor of time to get stable interval calculation\n        const blinkSeed = Math.floor(state.lastBlinkTime * 10)\n        const blinkInterval = 3 + noise.noise2D(blinkSeed, 900) * 2\n        if (timeSinceLastBlink > blinkInterval && !state.isBlinking) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          const blinkProgress = state.blinkTimer / state.blinkDuration\n          if (blinkProgress >= 1) {\n            state.isBlinking = false\n            state.lastBlinkTime = t\n          } else {\n            // Quick close, slower open\n            blinkAmount = blinkProgress < 0.3\n              ? Easing.easeOutQuad(blinkProgress / 0.3)\n              : Easing.easeInQuad(1 - (blinkProgress - 0.3) / 0.7)\n          }\n        }\n\n        // Apply eye rotations\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 7: TOES\n      // ========================================\n\n      // Toes slightly gripping for balance\n      if (rig.hasBone('leftToes')) {\n        rig.setRotation('leftToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1 * leftWeight))\n      }\n      if (rig.hasBone('rightToes')) {\n        rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1 * rightWeight))\n      }\n    },\n  }\n}\n\nexport const confidentStance: MotionProgram<ConfidentStanceParams> = createConfidentStance({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Breath timing does not match domain requirement. The block requires a 4–5 second breathing cycle, but confidentStanceParamsSchema.breathRate defaults to 0.15 Hz (≈6.67 s period). This violates the 'BREATH: Deep, slow breathing (4-5 second cycle)' constraint.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Top-of-file documentation is present but not detailed enough. The domain requires every motion file to begin with detailed documentation that describes HOW the motion should FEEL and specifies timing relationships between body parts (breath cycle, weight shift cadence, phase relationships, citations). The header lacks explicit timing relationships (e.g. breath 4–5 s, weight micro-shifts every 2–3 s) and not enough biomechanical citation detail.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"The implementation does not add forearm (lower arm) rotation coupled to breath. The 'ARMS: Subtle forearm rotation with breath' constraint expects lower-arm rotations modulated by breathing, but the breathing layer only modifies chest and shoulders (lower arms are set statically).\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Jaw bone is not driven. The constraint 'JAW: Relaxed but closed' is not implemented — there is no manipulation of a jaw bone (e.g. 'jaw' or 'lowerJaw') to ensure a relaxed/closed jaw posture.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Micro-noise is not applied to 'all bones'. The domain requires a noise layer across all bones (and consideration of all 69 VRM bones). The code applies micro-noise to head and upper arms only; many bones (spine segments beyond 'spine'/'chest'/'upperChest', pelvis details, clavicles, forearms, hands, finger joints, leg bones beyond setRotation entries, etc.) are not covered systematically.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Finger operations are not wrapped in explicit hasBone() checks. applyFingerCurl() and applyFingerSpread() are called without hasBone() guards in this file. The domain rule 'hasBone() checks wrap all optional bone access' is not demonstrably satisfied here (it may be satisfied internally by the helpers, but the source does not show explicit guards).\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"No explicit guarantee that all 30 finger bones receive subtle curl. The output constraint requires subtle curl on all 30 finger bones; the code calls applyFingerCurl() with per-finger factors but does not explicitly enumerate or assert that all finger bones are present/modified.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Weight bias logic appears incorrect and likely produces unintended weighting. The leftWeight/rightWeight computation is: leftWeight = weightBias < 0 ? 1 : 1 - weightBias; rightWeight = weightBias > 0 ? 1 : 1 + weightBias. For positive weightBias the rightWeight is forced to 1 regardless of magnitude, which does not correctly reflect a continuous bias range (-1..1). This is likely a bug that affects 'LEGS: Weight distributed evenly or slightly forward' and micro-weighting behaviors.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Some core bones are operated without explicit clamping to anatomical limits. The domain requires joint rotations to respect anatomical limits; the code uses small values (good practice) but does not enforce/clamp limits explicitly or document the ranges used.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Unused imports present: BoneChains and getAvailableBones are imported but not used. Remove or use them to avoid introducing unexplained concepts in the file header.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"Blink interval calculation is unconventional and may produce unstable intervals. It uses floor(state.lastBlinkTime * 10) as a seed index into noise.noise2D, then multiplies noise by 2 and offsets 3 seconds. This is a behavioral note (not runtime testable here) but could be made clearer/documented in the file-level timing notes.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    }\n  ],\n  \"summary\": \"The block is well-structured and expresses many aspects of a confident stance (spine/chest/head posture, shoulder/back, breathing influence on chest/shoulders, eye motion/blink logic, finger curl helpers). However it fails multiple domain constraints: breath rate default does not match the required 4–5s cycle; forearm/breath coupling is missing; jaw control is absent; micro-noise is not applied across all bones and the code does not demonstrate handling of all 69 VRM bones; finger bone coverage and hasBone guards are not explicitly enforced; weight-bias math appears incorrect. Additionally, the required detailed documentation with explicit timing relationships is inadequate. Because of these domain-rule violations, the block does not pass validation.\"\n}",
            "tokensUsed": {
              "input": 5604,
              "output": 3281
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "nervous_fidget",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/nervous-fidget.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "nervous_fidget",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe anxiety body language research basis",
                    "CORE: Hunched protective posture, shoulders elevated",
                    "CORE: Forward head posture (hypervigilance)",
                    "BREATH: Shallow, fast, irregular (2-3 second cycles)",
                    "BREATH: Shoulders rise with anxious breath",
                    "WEIGHT: IRREGULAR shifts using turbulence, not sine waves",
                    "WEIGHT: Ready-to-flee stance",
                    "ARMS: Held close to body (self-protection)",
                    "ARMS: Frequent self-touch gestures (neck, arms, face)",
                    "HANDS: Tense, curled fingers",
                    "HANDS: Rubbing, picking, clasping gestures",
                    "HANDS: All finger joints animated during fidgets",
                    "LEGS: Weight shifting, foot tapping bursts",
                    "FEET: Toes tapping or curled with tension",
                    "HEAD: Quick darting looks, then freeze, then dart",
                    "EYES: Rapid saccades, wide, frequent blinks",
                    "JAW: Clenched with occasional release",
                    "TIMING: Fidget BURSTS not continuous",
                    "TIMING: Quiet moments between fidget clusters"
                  ]
                }
              ],
              "description": "Anxiety-driven fidgeting with involuntary self-soothing behaviors.\nWeight shifts irregularly. Hands seek comfort (touching face, arms, clothing).\nShallow rapid breathing. Hypervigilant eye movement.\n\nFEEL: Uncomfortable, restless, seeking escape. Like waiting for bad news.\n\nTIMING: Breath is faster (2-3 second shallow cycles). Weight shifts are IRREGULAR\n(not sine waves - use turbulence). Fidget gestures happen in bursts with quiet\nperiods. Eyes dart quickly, then lock, then dart again.\n",
              "path": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Anxiety-driven fidgeting with involuntary self-soo...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 70073,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Top-of-file documentation is present but does not specify timing relationships between body parts (e.g., relative phase/delay between breath, chest/shoulder rise, head/eye darts, weight transfer propagation). Blocks philosophy requires each motion file to begin with detailed documentation that describes HOW the motion should FEEL and explicit timing relationships.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The file uses many numeric constants (rotation magnitudes, spring stiffness/damping, timing multipliers, fidget durations, etc.) without inline justification. Blocks philosophy requires every numerical choice to be justified (biomechanical reasoning or observational basis).",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Breathing is implemented as a pure sine wave (breathPhase = Math.sin(...)). The output constraints and philosophy require shallow, fast, IRREGULAR 2–3s cycles (turbulence/noise-driven variation). Consider adding controlled turbulence/noise to breath phase so cycles are not perfectly periodic.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Hand/finger handling: applyFingerCurl(rig, ...) is called without an explicit rig.hasBone(...) guard. The file claims 'Every bone rotation is guarded by rig.hasBone() checks' and motions must gracefully handle missing optional bones. If applyFingerCurl does not itself check hasBone() for each finger bone, this could break on rigs without finger bones. Either wrap applyFingerCurl with availability checks or document that applyFingerCurl handles missing bones.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Fidget selection is implemented as a discrete state machine (state.fidgetType toggling between 'none'/'hand_rub'/'arm_touch'/'neck_touch'). The Blocks philosophy advises using overlapping phase envelopes rather than discrete state machines to achieve natural, layered motion. Consider representing fidgets as overlapping envelopes so bursts overlap and blend more organically.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Research citations are minimal and not fully referenced. The header lists 'Navarro' and some behavior labels but does not provide explicit citations or brief notes connecting specific animation choices to research findings. The output constraint demanded documentation that describes an 'anxiety body language research basis' — expand the documentation with concrete references or short citations for the biomechanical/observational basis of chosen timings and magnitudes.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Eyes: the implementation includes rapid saccades and blinking, but does not explicitly implement 'wide' eye expressions or eyelid/eyebrow responses that convey widened eyes (surprise/hypervigilance). The constraint 'EYES: Rapid saccades, wide, frequent blinks' calls out width explicitly.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Toes/feet: code performs right-side foot tapping and sets rightToes, but left toes/left foot toe responses and toe gripping for weight shifts are not considered. The philosophy expects toes to 'grip and respond to weight shifts'. If toes are optional, ensure graceful handling and consider symmetric handling or explanatory comment.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The broader requirement that 'every motion must consider ALL 69 VRM bones' is not demonstrably satisfied. While the program uses core bones (hips, spine, chest, neck, head) and many limb bones, it does not explicitly cover all VRM bones or document fallback behavior for each group. Ensure the documentation/implementation explains degradation strategy and lists which optional bones are expected and how they are handled.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "nervous-fidget.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The nervous-fidget motion program expresses the domain intent clearly and implements many required behaviors: hunched protective posture, raised shoulders, forward head, shallow faster breathing, turbulence-driven weight shifts, fidget bursts, head/eye darts, blinking, jaw tension, and foot tapping. It uses seeded noise generators, springs, and hasBone checks in most places. However, it fails the Blocks Philosophy requirement for complete, detailed top-of-file documentation specifying timing relationships and full numeric justification (error). Additional warnings identify areas for improvement: breath irregularity (pure sine), discrete fidget state machine rather than overlapping envelopes, unguarded applyFingerCurl call (may rely on its internal guards), incomplete toes/eye-width handling, minimal research citations, and no explicit handling/documentation covering all VRM bones. Addressing these items will bring the block into full compliance."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: nervous_fidget\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Anxiety-driven fidgeting with involuntary self-soothing behaviors.\\nWeight shifts irregularly. Hands seek comfort (touching face, arms, clothing).\\nShallow rapid breathing. Hypervigilant eye movement.\\n\\nFEEL: Uncomfortable, restless, seeking escape. Like waiting for bad news.\\n\\nTIMING: Breath is faster (2-3 second shallow cycles). Weight shifts are IRREGULAR\\n(not sine waves - use turbulence). Fidget gestures happen in bursts with quiet\\nperiods. Eyes dart quickly, then lock, then dart again.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe anxiety body language research basis\",\n        \"CORE: Hunched protective posture, shoulders elevated\",\n        \"CORE: Forward head posture (hypervigilance)\",\n        \"BREATH: Shallow, fast, irregular (2-3 second cycles)\",\n        \"BREATH: Shoulders rise with anxious breath\",\n        \"WEIGHT: IRREGULAR shifts using turbulence, not sine waves\",\n        \"WEIGHT: Ready-to-flee stance\",\n        \"ARMS: Held close to body (self-protection)\",\n        \"ARMS: Frequent self-touch gestures (neck, arms, face)\",\n        \"HANDS: Tense, curled fingers\",\n        \"HANDS: Rubbing, picking, clasping gestures\",\n        \"HANDS: All finger joints animated during fidgets\",\n        \"LEGS: Weight shifting, foot tapping bursts\",\n        \"FEET: Toes tapping or curled with tension\",\n        \"HEAD: Quick darting looks, then freeze, then dart\",\n        \"EYES: Rapid saccades, wide, frequent blinks\",\n        \"JAW: Clenched with occasional release\",\n        \"TIMING: Fidget BURSTS not continuous\",\n        \"TIMING: Quiet moments between fidget clusters\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- nervous-fidget.ts ---\n```\n/**\n * Nervous Fidget Motion\n *\n * Anxiety-driven motion with rapid weight shifts, self-soothing gestures,\n * and tense posture. Captures the biomechanics of nervousness and\n * displacement behavior.\n *\n * Research basis:\n * - Anxiety body language (Navarro)\n * - Self-soothing behaviors (pacifying gestures)\n * - Displacement activities in stress\n * - Hypervigilance postural patterns\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  getAvailableBones,\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const nervousFidgetParamsSchema = z.object({\n  /** Overall anxiety level (0-1). Default: 0.6 */\n  anxietyLevel: z.number().min(0).max(1).default(0.6),\n  /** Intensity of fidgeting movements (0-1). Default: 0.5 */\n  fidgetIntensity: z.number().min(0).max(1).default(0.5),\n  /** Speed of looking around (0-1). Default: 0.4 */\n  lookAroundSpeed: z.number().min(0).max(1).default(0.4),\n  /** Breath rate multiplier (nervous = faster). Default: 1.5 */\n  breathRateMultiplier: z.number().min(1).max(3).default(1.5),\n  /** Enable hand fidgeting. Default: true */\n  handFidget: z.boolean().default(true),\n  /** Enable foot tapping. Default: true */\n  footTap: z.boolean().default(true),\n  /** Enable rapid eye movement. Default: true */\n  rapidEyeMovement: z.boolean().default(true),\n  /** Shoulder tension (0-1). Default: 0.7 */\n  shoulderTension: z.number().min(0).max(1).default(0.7),\n})\n\nexport type NervousFidgetParams = z.infer<typeof nervousFidgetParamsSchema>\nexport type NervousFidgetInput = z.input<typeof nervousFidgetParamsSchema>\n\nexport const nervousFidgetMeta: MotionMeta = {\n  id: 'nervous-fidget',\n  name: 'Nervous Fidget',\n  description: 'Anxiety-driven fidgeting with weight shifts, tense posture, and self-soothing gestures',\n  tags: ['nervous', 'anxiety', 'fidget', 'tension'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface NervousFidgetState {\n  noise: NoiseGenerator\n  weightSpring: Spring\n  headSpring: Spring\n  armSpring: Spring\n  fidgetTimer: number\n  fidgetType: 'none' | 'hand_rub' | 'arm_touch' | 'neck_touch'\n  fidgetHand: 'left' | 'right'\n  lastFidgetTime: number\n  blinkTimer: number\n  isBlinking: boolean\n  footTapPhase: number\n  footTapActive: boolean\n  lookTarget: { x: number; y: number }\n  lookChangeTimer: number\n}\n\nfunction initState(seed: number): NervousFidgetState {\n  return {\n    noise: createNoiseGenerator(seed),\n    weightSpring: createSpring({ stiffness: 150, damping: 12 }),\n    headSpring: createSpring({ stiffness: 200, damping: 18 }),\n    armSpring: createSpring({ stiffness: 100, damping: 15 }),\n    fidgetTimer: 0,\n    fidgetType: 'none',\n    fidgetHand: 'right',\n    lastFidgetTime: 0,\n    blinkTimer: 0,\n    isBlinking: false,\n    footTapPhase: 0,\n    footTapActive: false,\n    lookTarget: { x: 0, y: 0 },\n    lookChangeTimer: 0,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a nervous fidget motion with proper handling of optional bones.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (jaw, eyes, fingers, toes) are gracefully skipped\n * - The animation degrades gracefully when bones are unavailable\n * - Core bones (hips, spine, chest) are checked before use\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - No use of Math.random() or Date.now()\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - No allocations in the update loop except for quaternion creation\n * - Pre-computed values used where possible\n * - Springs provide efficient physics simulation\n */\nexport function createNervousFidget(params: NervousFidgetInput = {}): MotionProgram<NervousFidgetParams> {\n  const validatedParams = nervousFidgetParamsSchema.parse(params)\n  let state: NervousFidgetState | null = null\n\n  return {\n    meta: nervousFidgetMeta,\n    paramsSchema: nervousFidgetParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        anxietyLevel,\n        fidgetIntensity,\n        lookAroundSpeed,\n        breathRateMultiplier,\n        handFidget,\n        footTap,\n        rapidEyeMovement,\n        shoulderTension,\n      } = validatedParams\n\n      const noise = state.noise\n      const anxiety = anxietyLevel\n\n      // ========================================\n      // LAYER 1: TENSE BASE POSTURE\n      // ========================================\n\n      // Hunched, protective stance\n      const hunch = 0.05 * anxiety\n      const forwardLean = 0.03 * anxiety\n\n      // Hips - slightly tucked\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, forwardLean)\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // Spine - forward hunch\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch))\n      }\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch * 0.8))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch * 0.5))\n      }\n\n      // Shoulders - raised and tense\n      const shoulderRaise = shoulderTension * 0.08\n      const shoulderForward = shoulderTension * 0.06\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderRaise)\n        leftShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -shoulderForward))\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderRaise)\n        rightShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, shoulderForward))\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // Neck - forward head posture (hypervigilance)\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.08 * anxiety))\n      }\n\n      // Arms - held closer to body\n      const armProtect = 0.05 * anxiety\n      if (rig.hasBone('leftUpperArm')) {\n        const leftArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.15 - armProtect)\n        leftArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1))\n        rig.setRotation('leftUpperArm', leftArmRot)\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const rightArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.15 + armProtect)\n        rightArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1))\n        rig.setRotation('rightUpperArm', rightArmRot)\n      }\n\n      // Bent elbows\n      if (rig.hasBone('leftLowerArm')) {\n        rig.setRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.4 * anxiety))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        rig.setRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.4 * anxiety))\n      }\n\n      // Legs - weight on one leg, ready to move\n      if (rig.hasBone('leftUpperLeg')) {\n        rig.setRotation('leftUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        rig.setRotation('rightUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.05))\n      }\n\n      // ========================================\n      // LAYER 2: SHALLOW BREATHING\n      // ========================================\n\n      const breathRate = 0.25 * breathRateMultiplier\n      const breathPhase = Math.sin(t * breathRate * Math.PI * 2)\n      const shallowBreath = breathPhase * 0.015 * anxiety\n\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, shallowBreath))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, shallowBreath * 1.2))\n      }\n\n      // Shoulder rise with anxious breath\n      const anxiousShoulderBreath = breathPhase * 0.01 * anxiety\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -anxiousShoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, anxiousShoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 3: IRREGULAR WEIGHT SHIFTING\n      // ========================================\n\n      // Quick, irregular weight shifts\n      const weightShiftNoise = noise.turbulence(t * 0.8, 0) * 2 - 1\n      const weightJitter = noise.noise2D(t * 3, 100) * fidgetIntensity * 0.3\n      state.weightSpring.setTarget(weightShiftNoise * 0.5 + weightJitter)\n      state.weightSpring.update(dt)\n      const weightShift = state.weightSpring.value * fidgetIntensity\n\n      if (rig.hasBone('hips')) {\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, weightShift * 0.04))\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightShift * 0.02))\n      }\n\n      // Counter in spine\n      if (rig.hasBone('spine')) {\n        rig.addRotation('spine', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -weightShift * 0.02))\n      }\n\n      // ========================================\n      // LAYER 4: HEAD MOVEMENT (LOOKING AROUND)\n      // ========================================\n\n      // Quick, darting looks\n      state.lookChangeTimer += dt\n      const lookChangeInterval = 1.5 - lookAroundSpeed * 1.2\n      if (state.lookChangeTimer > lookChangeInterval) {\n        state.lookChangeTimer = 0\n        state.lookTarget = {\n          x: (noise.noise2D(t, 200) * 2 - 1) * 0.15 * lookAroundSpeed,\n          y: (noise.noise2D(t, 300) * 2 - 1) * 0.1 * lookAroundSpeed,\n        }\n      }\n\n      state.headSpring.setTarget(state.lookTarget.x)\n      state.headSpring.update(dt)\n      const headYaw = state.headSpring.value\n\n      // Add nervous micro-movements\n      const headJitterX = noise.noise2D(t * 4, 400) * 0.02 * fidgetIntensity\n      const headJitterY = noise.noise2D(t * 4, 500) * 0.015 * fidgetIntensity\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headYaw + headJitterY)\n        headRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, state.lookTarget.y + headJitterX))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        rig.addRotation('neck', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headYaw * 0.3))\n      }\n\n      // ========================================\n      // LAYER 5: HAND FIDGETING\n      // ========================================\n\n      if (handFidget) {\n        // Decide on fidget type\n        const timeSinceFidget = t - state.lastFidgetTime\n        const fidgetInterval = 2 + noise.noise2D(t * 0.1, 600) * 2\n\n        if (state.fidgetType === 'none' && timeSinceFidget > fidgetInterval) {\n          const fidgetRoll = noise.noise2D(t, 700)\n          if (fidgetRoll > 0.6) {\n            state.fidgetType = 'hand_rub'\n          } else if (fidgetRoll > 0.3) {\n            state.fidgetType = 'arm_touch'\n          } else {\n            state.fidgetType = 'neck_touch'\n          }\n          state.fidgetHand = noise.noise2D(t, 800) > 0.5 ? 'left' : 'right'\n          state.fidgetTimer = 0\n        }\n\n        if (state.fidgetType !== 'none') {\n          state.fidgetTimer += dt\n          const fidgetDuration = 1.5 + noise.noise2D(t * 0.2, 900) * 1\n          const fidgetProgress = state.fidgetTimer / fidgetDuration\n\n          if (fidgetProgress >= 1) {\n            state.fidgetType = 'none'\n            state.lastFidgetTime = t\n          } else {\n            // Ease in and out of fidget\n            const fidgetWeight = Math.sin(fidgetProgress * Math.PI)\n\n            // Apply fidget based on type\n            switch (state.fidgetType) {\n              case 'hand_rub':\n                // Bring hands together in front\n                if (rig.hasBone('leftUpperArm')) {\n                  rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.3 * fidgetWeight))\n                  rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.2 * fidgetWeight))\n                }\n                if (rig.hasBone('rightUpperArm')) {\n                  rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.3 * fidgetWeight))\n                  rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.2 * fidgetWeight))\n                }\n                if (rig.hasBone('leftLowerArm')) {\n                  rig.addRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.5 * fidgetWeight))\n                }\n                if (rig.hasBone('rightLowerArm')) {\n                  rig.addRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.5 * fidgetWeight))\n                }\n                // Rubbing motion\n                const rubPhase = Math.sin(state.fidgetTimer * 8) * fidgetWeight\n                if (rig.hasBone('leftHand')) {\n                  rig.addRotation('leftHand', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rubPhase * 0.2))\n                }\n                break\n\n              case 'arm_touch':\n                // Touch opposite arm\n                const touchArm = state.fidgetHand === 'left' ? 'left' : 'right'\n                if (rig.hasBone(`${touchArm}UpperArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${touchArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.4 * fidgetWeight))\n                }\n                if (rig.hasBone(`${touchArm}LowerArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${touchArm}LowerArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 1, z: 0 }, (touchArm === 'left' ? -1 : 1) * 0.8 * fidgetWeight))\n                }\n                break\n\n              case 'neck_touch':\n                // Touch back of neck\n                const neckArm = state.fidgetHand\n                if (rig.hasBone(`${neckArm}UpperArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${neckArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.5 * fidgetWeight))\n                  rig.addRotation(`${neckArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 0, z: 1 }, (neckArm === 'left' ? -1 : 1) * 0.3 * fidgetWeight))\n                }\n                if (rig.hasBone(`${neckArm}LowerArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${neckArm}LowerArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 1, z: 0 }, (neckArm === 'left' ? -1 : 1) * 1.2 * fidgetWeight))\n                }\n                break\n            }\n          }\n        }\n\n        // Fingers - tense, curled\n        const fingerTension = 0.4 + noise.noise2D(t * 2, 1000) * 0.2\n        applyFingerCurl(rig, 'left', {\n          thumb: fingerTension * 0.5,\n          index: fingerTension,\n          middle: fingerTension * 1.1,\n          ring: fingerTension * 1.15,\n          little: fingerTension * 1.2,\n        })\n        applyFingerCurl(rig, 'right', {\n          thumb: fingerTension * 0.5,\n          index: fingerTension,\n          middle: fingerTension * 1.1,\n          ring: fingerTension * 1.15,\n          little: fingerTension * 1.2,\n        })\n      }\n\n      // ========================================\n      // LAYER 6: FOOT TAPPING\n      // ========================================\n\n      if (footTap) {\n        // Decide when to tap\n        const tapTrigger = noise.noise2D(t * 0.5, 1100)\n        if (!state.footTapActive && tapTrigger > 0.7) {\n          state.footTapActive = true\n          state.footTapPhase = 0\n        }\n\n        if (state.footTapActive) {\n          state.footTapPhase += dt * 6 // Fast tapping\n          const tapCycle = Math.sin(state.footTapPhase * Math.PI * 2)\n          const tapUp = Math.max(0, tapCycle) * fidgetIntensity\n\n          // Right foot tap\n          if (rig.hasBone('rightFoot')) {\n            rig.addRotation('rightFoot', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, tapUp * 0.15))\n          }\n          if (rig.hasBone('rightToes')) {\n            rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -tapUp * 0.2))\n          }\n\n          // Stop after a few taps\n          if (state.footTapPhase > 8) {\n            state.footTapActive = false\n          }\n        }\n      }\n\n      // ========================================\n      // LAYER 7: RAPID EYE MOVEMENT\n      // ========================================\n\n      if (rapidEyeMovement) {\n        // Quick, darting eye movements\n        const eyeSpeed = 3 + lookAroundSpeed * 4\n        const eyeX = noise.noise2D(t * eyeSpeed, 1200) * 0.08 * anxiety\n        const eyeY = noise.noise2D(t * eyeSpeed, 1300) * 0.06 * anxiety\n\n        // Frequent blinking\n        const blinkInterval = 1.5 - anxiety * 0.8\n        if (!state.isBlinking && noise.noise2D(t * 0.5, 1400) > 0.8) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          if (state.blinkTimer > 0.12) {\n            state.isBlinking = false\n          } else {\n            blinkAmount = Math.sin(state.blinkTimer / 0.12 * Math.PI)\n          }\n        }\n\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 8: JAW TENSION\n      // ========================================\n\n      if (rig.hasBone('jaw')) {\n        // Slight jaw clench with occasional release\n        const jawTension = anxiety * 0.02\n        const jawRelease = noise.noise2D(t * 0.3, 1500) > 0.8 ? 0.01 : 0\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawRelease - jawTension))\n      }\n    },\n  }\n}\n\nexport const nervousFidget: MotionProgram<NervousFidgetParams> = createNervousFidget({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Top-of-file documentation is present but does not specify timing relationships between body parts (e.g., relative phase/delay between breath, chest/shoulder rise, head/eye darts, weight transfer propagation). Blocks philosophy requires each motion file to begin with detailed documentation that describes HOW the motion should FEEL and explicit timing relationships.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"The file uses many numeric constants (rotation magnitudes, spring stiffness/damping, timing multipliers, fidget durations, etc.) without inline justification. Blocks philosophy requires every numerical choice to be justified (biomechanical reasoning or observational basis).\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Breathing is implemented as a pure sine wave (breathPhase = Math.sin(...)). The output constraints and philosophy require shallow, fast, IRREGULAR 2–3s cycles (turbulence/noise-driven variation). Consider adding controlled turbulence/noise to breath phase so cycles are not perfectly periodic.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Hand/finger handling: applyFingerCurl(rig, ...) is called without an explicit rig.hasBone(...) guard. The file claims 'Every bone rotation is guarded by rig.hasBone() checks' and motions must gracefully handle missing optional bones. If applyFingerCurl does not itself check hasBone() for each finger bone, this could break on rigs without finger bones. Either wrap applyFingerCurl with availability checks or document that applyFingerCurl handles missing bones.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Fidget selection is implemented as a discrete state machine (state.fidgetType toggling between 'none'/'hand_rub'/'arm_touch'/'neck_touch'). The Blocks philosophy advises using overlapping phase envelopes rather than discrete state machines to achieve natural, layered motion. Consider representing fidgets as overlapping envelopes so bursts overlap and blend more organically.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Research citations are minimal and not fully referenced. The header lists 'Navarro' and some behavior labels but does not provide explicit citations or brief notes connecting specific animation choices to research findings. The output constraint demanded documentation that describes an 'anxiety body language research basis' — expand the documentation with concrete references or short citations for the biomechanical/observational basis of chosen timings and magnitudes.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Eyes: the implementation includes rapid saccades and blinking, but does not explicitly implement 'wide' eye expressions or eyelid/eyebrow responses that convey widened eyes (surprise/hypervigilance). The constraint 'EYES: Rapid saccades, wide, frequent blinks' calls out width explicitly.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Toes/feet: code performs right-side foot tapping and sets rightToes, but left toes/left foot toe responses and toe gripping for weight shifts are not considered. The philosophy expects toes to 'grip and respond to weight shifts'. If toes are optional, ensure graceful handling and consider symmetric handling or explanatory comment.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"The broader requirement that 'every motion must consider ALL 69 VRM bones' is not demonstrably satisfied. While the program uses core bones (hips, spine, chest, neck, head) and many limb bones, it does not explicitly cover all VRM bones or document fallback behavior for each group. Ensure the documentation/implementation explains degradation strategy and lists which optional bones are expected and how they are handled.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    }\n  ],\n  \"summary\": \"The nervous-fidget motion program expresses the domain intent clearly and implements many required behaviors: hunched protective posture, raised shoulders, forward head, shallow faster breathing, turbulence-driven weight shifts, fidget bursts, head/eye darts, blinking, jaw tension, and foot tapping. It uses seeded noise generators, springs, and hasBone checks in most places. However, it fails the Blocks Philosophy requirement for complete, detailed top-of-file documentation specifying timing relationships and full numeric justification (error). Additional warnings identify areas for improvement: breath irregularity (pure sine), discrete fidget state machine rather than overlapping envelopes, unguarded applyFingerCurl call (may rely on its internal guards), incomplete toes/eye-width handling, minimal research citations, and no explicit handling/documentation covering all VRM bones. Addressing these items will bring the block into full compliance.\"\n}",
            "tokensUsed": {
              "input": 6706,
              "output": 3839
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "smoking_cigarette",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/smoking-cigarette.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "smoking_cigarette",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe smoking biomechanics and timing offsets",
                    "PHASE ENVELOPES: NO discrete state machine",
                    "PHASE ENVELOPES: Shoulder, elbow, wrist have OVERLAPPING phases",
                    "PHASE ENVELOPES: Shoulder LEADS, wrist TRAILS",
                    "ANTICIPATION: Body telegraphs before arm rises",
                    "ANTICIPATION: Weight shifts before arm movement",
                    "ARM TRAJECTORY: Curved natural path, not linear",
                    "ARM TRAJECTORY: Spring-smoothed with overshoot/settle",
                    "HAND POSITION: Must accurately reach mouth (needs IK or precise targeting)",
                    "FINGERS: Precise cigarette grip (index + middle pinch)",
                    "FINGERS: Subtle adjustments throughout",
                    "INHALE: Chest expands, shoulders rise",
                    "INHALE: Eyes squint slightly",
                    "INHALE: Cheeks may hollow (jaw bone)",
                    "HOLD: Brief stillness, micro-tension",
                    "EXHALE: Jaw opens, lips purse",
                    "EXHALE: Head may tilt up or to side",
                    "EXHALE: Slow controlled breath",
                    "LOWER: Wrist leads, shoulder follows (opposite of raise)",
                    "LOWER: Follow-through past rest position, then settle",
                    "SUPPORT ARM: Crossed under, or relaxed at side",
                    "BREATH: Couples to entire motion cycle",
                    "STYLE: Casual/Stressed/Seductive affect ALL timing"
                  ]
                }
              ],
              "description": "Complete smoking animation with naturalistic hand-to-mouth coordination.\nNOT a state machine - overlapping phase envelopes for each body part.\n\nFEEL: Depends on style param. Casual = relaxed habit. Stressed = need for relief.\nSeductive = deliberate, slow, eye contact maintained.\n\nTIMING RELATIONSHIPS:\n- Shoulder rotation BEGINS first (leads by ~80ms)\n- Elbow extension follows (~50ms after shoulder)\n- Wrist rotation follows (~30ms after elbow)\n- Fingers adjust grip throughout\n- Inhale: chest expands, shoulders rise, eyes squint\n- Hold: brief stillness, smoke held\n- Exhale: jaw opens, head may tilt, slow breath out\n- Lower: reverse order (wrist leads, shoulder follows)\n\nCRITICAL: Hand MUST reach mouth accurately. This requires end-effector\ntargeting, not blind rotation values.\n",
              "path": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Complete smoking animation with naturalistic hand-...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 50367,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Implements a discrete state machine (SmokingPhase + transitions) which directly contradicts the block constraint requiring PHASE ENVELOPES and NO discrete state machine. The domain requires overlapping phase envelopes (continuous, overlapping envelopes per body part) rather than hard phase switches.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Hand-to-mouth positioning uses direct bone rotation targets (upperArm / lowerArm / hand rotations) rather than an end-effector IK or precise targeting system. The block constraints require accurate end-effector targeting (IK or equivalent) to ensure the hand reliably reaches the mouth.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Top-of-file documentation is insufficient relative to Blocks philosophy: it does not specify the detailed timing relationships (e.g., 'shoulder leads elbow by ~80ms, elbow follows by ~50ms, wrist ~30ms'), how the motion should FEEL across styles, nor explicitly cite biomechanical/observational research. The block requires detailed documentation that describes HOW the motion should FEEL and specific timing offsets.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Anticipation and lead/lag relationships between shoulder → elbow → wrist → fingers are not implemented as explicit overlapping phase envelopes or timed offsets. The code sets per-phase targets largely simultaneously and relies on springs, but does not encode the required lead timings (e.g., shoulder leads elbow by ~80ms, elbow leads wrist by ~50ms, wrist leads fingers by ~30ms).",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The block philosophy mandates that every motion 'consider ALL 69 VRM bones' and that core bones are always active. This implementation touches many core bones but does not address the full VRM bone set (toes, many finger subtleties, etc.). The motion therefore does not meet the 'all 69 bones' consideration requirement.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The code introduces an explicit 'state-machine' concept and meta tag ('state-machine') which is contradictory to the Block Definition constraints (explicitly demanded NO discrete state machine). This is a conceptual mismatch between block definition and implementation.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Some important domain constraints are only partially addressed or left implicit: anatomical limits / joint clamping are not enforced (rotations are created without explicit clamping to safe ranges). The Blocks philosophy requires joint rotations to respect anatomical limits.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "applyCigaretteGrip and applyFingerCurl are invoked without explicit hasBone() guards in this file. While these helper functions may internally guard bone access, the block philosophy requires hasBone() checks wrap optional bone access; it is unclear from this file whether the helpers perform those checks.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Phase-duration noise computation uses noise.noise2D(t, phase.length * 100). Using the string length of a phase identifier (phase.length) as a constant for noise frequency is fragile and likely unintended; it couples noise behavior to name length instead of an explicit numeric seed/frequency. This is a stylistic/robustness issue that can lead to confusing variability.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The implementation relies on a discrete phaseProgress per state; it does not express overlapping envelopes per body part (separate envelopes for shoulder, elbow, wrist, chest, head, fingers) which would allow graceful blending between phases as required by the domain's overlapping-phase philosophy.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "smoking-cigarette.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "Failed validation. The implementation uses a discrete state machine and direct bone rotations instead of overlapping phase envelopes and end-effector targeting required by the domain. Documentation is insufficient (missing explicit timing offsets and detailed biomechanical explanation). Several explicit domain rules are unmet (lead/lag timing between shoulder→elbow→wrist→fingers, consideration of all VRM bones, explicit anatomical limits). The code does include useful elements (breathing coupling, chest/shoulder/head links, hasBone checks in many places, finger-grip helpers), but the core architectural mismatch (state machine vs. overlapping phase envelopes and missing IK targeting) causes the block to fail domain validation."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: smoking_cigarette\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Complete smoking animation with naturalistic hand-to-mouth coordination.\\nNOT a state machine - overlapping phase envelopes for each body part.\\n\\nFEEL: Depends on style param. Casual = relaxed habit. Stressed = need for relief.\\nSeductive = deliberate, slow, eye contact maintained.\\n\\nTIMING RELATIONSHIPS:\\n- Shoulder rotation BEGINS first (leads by ~80ms)\\n- Elbow extension follows (~50ms after shoulder)\\n- Wrist rotation follows (~30ms after elbow)\\n- Fingers adjust grip throughout\\n- Inhale: chest expands, shoulders rise, eyes squint\\n- Hold: brief stillness, smoke held\\n- Exhale: jaw opens, head may tilt, slow breath out\\n- Lower: reverse order (wrist leads, shoulder follows)\\n\\nCRITICAL: Hand MUST reach mouth accurately. This requires end-effector\\ntargeting, not blind rotation values.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe smoking biomechanics and timing offsets\",\n        \"PHASE ENVELOPES: NO discrete state machine\",\n        \"PHASE ENVELOPES: Shoulder, elbow, wrist have OVERLAPPING phases\",\n        \"PHASE ENVELOPES: Shoulder LEADS, wrist TRAILS\",\n        \"ANTICIPATION: Body telegraphs before arm rises\",\n        \"ANTICIPATION: Weight shifts before arm movement\",\n        \"ARM TRAJECTORY: Curved natural path, not linear\",\n        \"ARM TRAJECTORY: Spring-smoothed with overshoot/settle\",\n        \"HAND POSITION: Must accurately reach mouth (needs IK or precise targeting)\",\n        \"FINGERS: Precise cigarette grip (index + middle pinch)\",\n        \"FINGERS: Subtle adjustments throughout\",\n        \"INHALE: Chest expands, shoulders rise\",\n        \"INHALE: Eyes squint slightly\",\n        \"INHALE: Cheeks may hollow (jaw bone)\",\n        \"HOLD: Brief stillness, micro-tension\",\n        \"EXHALE: Jaw opens, lips purse\",\n        \"EXHALE: Head may tilt up or to side\",\n        \"EXHALE: Slow controlled breath\",\n        \"LOWER: Wrist leads, shoulder follows (opposite of raise)\",\n        \"LOWER: Follow-through past rest position, then settle\",\n        \"SUPPORT ARM: Crossed under, or relaxed at side\",\n        \"BREATH: Couples to entire motion cycle\",\n        \"STYLE: Casual/Stressed/Seductive affect ALL timing\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- smoking-cigarette.ts ---\n```\n/**\n * Smoking Cigarette Motion\n *\n * Complete smoking animation with state machine for different phases:\n * idle holding, bring to mouth, inhale, hold, exhale, lower, ash tap.\n *\n * Research basis:\n * - Smoking biomechanics studies\n * - Hand-to-mouth coordination patterns\n * - Respiratory mechanics during smoking\n * - Habitual gesture timing\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  SpringPresets,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  applyCigaretteGrip,\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const smokingCigaretteParamsSchema = z.object({\n  /** Smoking style: casual, stressed, or seductive. Default: casual */\n  smokingStyle: z.enum(['casual', 'stressed', 'seductive']).default('casual'),\n  /** Which hand holds the cigarette. Default: right */\n  handedness: z.enum(['left', 'right']).default('right'),\n  /** Depth of inhale (0-1). Default: 0.7 */\n  inhaleDepth: z.number().min(0).max(1).default(0.7),\n  /** Variation in pacing (0-1). Default: 0.3 */\n  paceVariation: z.number().min(0).max(1).default(0.3),\n  /** Overall animation intensity (0-1). Default: 0.8 */\n  intensity: z.number().min(0).max(1).default(0.8),\n  /** Enable eye squint during inhale. Default: true */\n  eyeSquint: z.boolean().default(true),\n  /** Enable jaw animation during exhale. Default: true */\n  jawAnimation: z.boolean().default(true),\n  /** Time between puffs in seconds. Default: 8 */\n  puffInterval: z.number().min(3).max(20).default(8),\n})\n\nexport type SmokingCigaretteParams = z.infer<typeof smokingCigaretteParamsSchema>\nexport type SmokingCigaretteInput = z.input<typeof smokingCigaretteParamsSchema>\n\nexport const smokingCigaretteMeta: MotionMeta = {\n  id: 'smoking-cigarette',\n  name: 'Smoking Cigarette',\n  description: 'Complete smoking animation with hand-to-mouth, inhale, exhale phases',\n  tags: ['smoking', 'gesture', 'complex', 'state-machine'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MACHINE\n// ============================================================================\n\ntype SmokingPhase =\n  | 'idle'           // Holding cigarette at side\n  | 'bring_to_mouth' // Raising arm to mouth\n  | 'inhale'         // Taking a drag\n  | 'hold'           // Holding smoke\n  | 'exhale'         // Breathing out\n  | 'lower'          // Lowering arm\n  | 'ash_tap'        // Tapping ash off\n\ninterface SmokingState {\n  noise: NoiseGenerator\n  phase: SmokingPhase\n  phaseTime: number\n  phaseDuration: number\n  armSpringX: Spring\n  armSpringY: Spring\n  armSpringZ: Spring\n  wristSpring: Spring\n  chestSpring: Spring\n  lastPuffTime: number\n  ashTapPending: boolean\n  blinkTimer: number\n  isBlinking: boolean\n}\n\nconst PHASE_DURATIONS = {\n  idle: { base: 6, variance: 2 },\n  bring_to_mouth: { base: 0.8, variance: 0.2 },\n  inhale: { base: 1.5, variance: 0.3 },\n  hold: { base: 0.8, variance: 0.3 },\n  exhale: { base: 2.0, variance: 0.4 },\n  lower: { base: 0.6, variance: 0.15 },\n  ash_tap: { base: 0.4, variance: 0.1 },\n}\n\nfunction initState(seed: number): SmokingState {\n  return {\n    noise: createNoiseGenerator(seed),\n    phase: 'idle',\n    phaseTime: 0,\n    phaseDuration: PHASE_DURATIONS.idle.base,\n    armSpringX: createSpring(SpringPresets.smooth),\n    armSpringY: createSpring(SpringPresets.smooth),\n    armSpringZ: createSpring(SpringPresets.smooth),\n    wristSpring: createSpring({ stiffness: 250, damping: 22 }),\n    chestSpring: createSpring({ stiffness: 100, damping: 18 }),\n    lastPuffTime: -10,\n    ashTapPending: false,\n    blinkTimer: 0,\n    isBlinking: false,\n  }\n}\n\nfunction getNextPhase(current: SmokingPhase, ashTapPending: boolean): SmokingPhase {\n  switch (current) {\n    case 'idle': return 'bring_to_mouth'\n    case 'bring_to_mouth': return 'inhale'\n    case 'inhale': return 'hold'\n    case 'hold': return 'exhale'\n    case 'exhale': return ashTapPending ? 'ash_tap' : 'lower'\n    case 'lower': return 'idle'\n    case 'ash_tap': return 'lower'\n  }\n}\n\nfunction getPhaseDuration(phase: SmokingPhase, variation: number, noise: NoiseGenerator, t: number): number {\n  const { base, variance } = PHASE_DURATIONS[phase]\n  return base + noise.noise2D(t, phase.length * 100) * variance * variation\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a smoking cigarette motion with complete state machine.\n *\n * STATE MACHINE PHASES:\n * - idle: Holding cigarette at side, relaxed posture\n * - bring_to_mouth: Spring-animated arm raise to face\n * - inhale: Taking a drag, chest expansion, eye squint\n * - hold: Brief pause holding smoke, slight arm lower\n * - exhale: Slow breath out, jaw animation, arm stays high\n * - lower: Return arm to idle position\n * - ash_tap: Optional wrist flick to tap ash (random trigger)\n *\n * TRANSITIONS:\n * - All transitions are time-based with phase duration + variance\n * - Spring physics ensure smooth arm movement between states\n * - Phase progress is used for easing within each state\n * - idle → bring_to_mouth → inhale → hold → exhale → (ash_tap?) → lower → idle\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Dynamic bone names (smokingArm, supportArm) are cast and checked\n * - Missing optional bones (jaw, eyes, fingers) are gracefully skipped\n * - The animation degrades gracefully when bones are unavailable\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Ash tap triggering uses seeded noise\n * - Phase duration variance uses seeded noise\n *\n * SECONDARY MOTION:\n * - Finger positions for cigarette grip via applyCigaretteGrip\n * - Wrist rotation during state transitions\n * - Chest expansion/contraction with breathing phases\n */\nexport function createSmokingCigarette(params: SmokingCigaretteInput = {}): MotionProgram<SmokingCigaretteParams> {\n  const validatedParams = smokingCigaretteParamsSchema.parse(params)\n  let state: SmokingState | null = null\n\n  return {\n    meta: smokingCigaretteMeta,\n    paramsSchema: smokingCigaretteParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        smokingStyle,\n        handedness,\n        inhaleDepth,\n        paceVariation,\n        intensity,\n        eyeSquint,\n        jawAnimation,\n        puffInterval,\n      } = validatedParams\n\n      const noise = state.noise\n      const isRightHanded = handedness === 'right'\n      const handSide = isRightHanded ? 1 : -1\n\n      // Style modifiers\n      const styleModifiers = {\n        casual: { speed: 1, tension: 0.3, lean: 0 },\n        stressed: { speed: 1.3, tension: 0.7, lean: 0.05 },\n        seductive: { speed: 0.7, tension: 0.2, lean: -0.03 },\n      }[smokingStyle]\n\n      // ========================================\n      // STATE MACHINE UPDATE\n      // ========================================\n\n      state.phaseTime += dt\n\n      // Check for phase transition\n      if (state.phaseTime >= state.phaseDuration) {\n        const nextPhase = getNextPhase(state.phase, state.ashTapPending)\n        state.phase = nextPhase\n        state.phaseTime = 0\n        state.phaseDuration = getPhaseDuration(nextPhase, paceVariation, noise, t) / styleModifiers.speed\n\n        if (nextPhase === 'idle') {\n          state.lastPuffTime = t\n          // Randomly decide if next cycle should include ash tap\n          state.ashTapPending = noise.noise2D(t, 500) > 0.7\n        }\n      }\n\n      // Force transition if puff interval exceeded during idle\n      if (state.phase === 'idle' && (t - state.lastPuffTime) > puffInterval) {\n        state.phase = 'bring_to_mouth'\n        state.phaseTime = 0\n        state.phaseDuration = getPhaseDuration('bring_to_mouth', paceVariation, noise, t) / styleModifiers.speed\n      }\n\n      const phaseProgress = Math.min(1, state.phaseTime / state.phaseDuration)\n\n      // ========================================\n      // LAYER 1: BASE POSTURE\n      // ========================================\n\n      // Style-based lean\n      if (rig.hasBone('hips')) {\n        const leanAmount = styleModifiers.lean * intensity\n        rig.setRotation('hips', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leanAmount))\n      }\n\n      // Relaxed spine\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n      }\n\n      // ========================================\n      // LAYER 2: ARM MOVEMENT (SPRING-BASED)\n      // ========================================\n\n      // Define arm targets for each phase\n      let armTargetX = 0 // Forward/back rotation\n      let armTargetY = 0 // Abduction\n      let armTargetZ = 0 // Twist\n      let elbowBend = 0.2 * intensity\n      let wristRotation = 0\n\n      const smokingArm = isRightHanded ? 'right' : 'left'\n      const supportArm = isRightHanded ? 'left' : 'right'\n\n      switch (state.phase) {\n        case 'idle':\n          armTargetX = 0.1 * intensity\n          armTargetY = 0.15 * intensity\n          armTargetZ = 0\n          elbowBend = 0.3 * intensity\n          wristRotation = 0.1 * handSide\n          break\n\n        case 'bring_to_mouth':\n          const raiseEase = Easing.armRaise(phaseProgress)\n          armTargetX = -0.5 * intensity * raiseEase\n          armTargetY = 0.4 * intensity * raiseEase\n          armTargetZ = handSide * 0.2 * intensity * raiseEase\n          elbowBend = 1.4 * intensity * raiseEase\n          wristRotation = handSide * 0.3 * raiseEase\n          break\n\n        case 'inhale':\n          armTargetX = -0.5 * intensity\n          armTargetY = 0.4 * intensity\n          armTargetZ = handSide * 0.2 * intensity\n          elbowBend = 1.4 * intensity\n          wristRotation = handSide * 0.3\n          break\n\n        case 'hold':\n          // Slight arm lower while holding + SECONDARY MOTION\n          // During hold, add subtle micro-movements for organic feel\n          const holdProgress = Easing.easeInOutCubic(phaseProgress)\n\n          // Secondary motion: subtle arm drift and finger adjustments\n          const holdMicroX = noise.noise2D(t * 0.5, 250) * 0.015 * intensity\n          const holdMicroY = noise.noise2D(t * 0.4, 260) * 0.01 * intensity\n          const holdMicroZ = noise.noise2D(t * 0.3, 270) * 0.008 * intensity\n          const holdWristMicro = noise.noise2D(t * 0.6, 280) * 0.02 * intensity\n\n          armTargetX = -0.4 * intensity + holdMicroX\n          armTargetY = 0.35 * intensity + holdMicroY\n          armTargetZ = handSide * 0.18 * intensity + holdMicroZ\n          elbowBend = 1.3 * intensity\n          wristRotation = handSide * 0.25 + holdWristMicro\n          break\n\n        case 'exhale':\n          // Arm stays relatively high during exhale\n          const exhaleProgress = Easing.easeInOutCubic(phaseProgress)\n          armTargetX = -0.35 * intensity * (1 - exhaleProgress * 0.5)\n          armTargetY = 0.3 * intensity * (1 - exhaleProgress * 0.3)\n          armTargetZ = handSide * 0.15 * intensity\n          elbowBend = 1.1 * intensity * (1 - exhaleProgress * 0.3)\n          wristRotation = handSide * 0.2\n          break\n\n        case 'lower':\n          const lowerEase = Easing.easeInCubic(phaseProgress)\n          armTargetX = 0.1 * intensity * lowerEase\n          armTargetY = 0.15 * intensity * lowerEase\n          armTargetZ = 0\n          elbowBend = 0.3 * intensity * lowerEase + 1.1 * intensity * (1 - lowerEase)\n          wristRotation = handSide * 0.1 * lowerEase\n          break\n\n        case 'ash_tap':\n          // Quick wrist flick\n          const tapEase = Math.sin(phaseProgress * Math.PI * 2)\n          armTargetX = 0.1 * intensity\n          armTargetY = 0.15 * intensity\n          armTargetZ = 0\n          elbowBend = 0.4 * intensity\n          wristRotation = handSide * 0.1 + tapEase * 0.3 * handSide\n          break\n      }\n\n      // Apply spring smoothing to arm movements\n      state.armSpringX.setTarget(armTargetX)\n      state.armSpringY.setTarget(armTargetY)\n      state.armSpringZ.setTarget(armTargetZ)\n      state.wristSpring.setTarget(wristRotation)\n\n      state.armSpringX.update(dt)\n      state.armSpringY.update(dt)\n      state.armSpringZ.update(dt)\n      state.wristSpring.update(dt)\n\n      const smoothArmX = state.armSpringX.value\n      const smoothArmY = state.armSpringY.value\n      const smoothArmZ = state.armSpringZ.value\n      const smoothWrist = state.wristSpring.value\n\n      // Apply smoking arm\n      const upperArmBone = `${smokingArm}UpperArm` as VRMHumanBoneName\n      const lowerArmBone = `${smokingArm}LowerArm` as VRMHumanBoneName\n      const handBone = `${smokingArm}Hand` as VRMHumanBoneName\n      const shoulderBone = `${smokingArm}Shoulder` as VRMHumanBoneName\n\n      if (rig.hasBone(shoulderBone)) {\n        rig.setRotation(shoulderBone, quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handSide * smoothArmY * 0.15))\n      }\n\n      if (rig.hasBone(upperArmBone)) {\n        const upperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, smoothArmX)\n        upperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handSide * smoothArmY))\n        upperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, smoothArmZ))\n        rig.setRotation(upperArmBone, upperArmRot)\n      }\n\n      if (rig.hasBone(lowerArmBone)) {\n        rig.setRotation(lowerArmBone, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, handSide * elbowBend))\n      }\n\n      if (rig.hasBone(handBone)) {\n        const handRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, smoothWrist)\n        // Slight wrist extension when holding cigarette up\n        const wristExtension = (state.phase === 'inhale' || state.phase === 'hold') ? 0.15 : 0\n        handRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, wristExtension))\n        rig.setRotation(handBone, handRot)\n      }\n\n      // Apply cigarette grip to smoking hand\n      applyCigaretteGrip(rig, smokingArm as 'left' | 'right', 'between')\n\n      // Support arm - relaxed or crossed\n      const supportUpperArm = `${supportArm}UpperArm` as VRMHumanBoneName\n      const supportLowerArm = `${supportArm}LowerArm` as VRMHumanBoneName\n\n      if (smokingStyle === 'seductive') {\n        // Arm crossed under\n        if (rig.hasBone(supportUpperArm)) {\n          const supportRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.4 * intensity)\n          supportRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handSide * 0.3 * intensity))\n          rig.setRotation(supportUpperArm, supportRot)\n        }\n        if (rig.hasBone(supportLowerArm)) {\n          rig.setRotation(supportLowerArm, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -handSide * 1.2 * intensity))\n        }\n      } else {\n        // Relaxed at side\n        if (rig.hasBone(supportUpperArm)) {\n          rig.setRotation(supportUpperArm, quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handSide * 0.08))\n        }\n        if (rig.hasBone(supportLowerArm)) {\n          rig.setRotation(supportLowerArm, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -handSide * 0.15))\n        }\n        // Relaxed fingers on support hand\n        applyFingerCurl(rig, supportArm as 'left' | 'right', {\n          thumb: 0.3,\n          index: 0.35,\n          middle: 0.4,\n          ring: 0.45,\n          little: 0.5,\n        })\n      }\n\n      // ========================================\n      // LAYER 3: BREATHING & CHEST\n      // ========================================\n\n      let chestExpansion = 0\n      const baseBreath = oscBreathing(t, 0.2, 0.3) * intensity\n\n      switch (state.phase) {\n        case 'inhale':\n          // Deep inhale - chest expands\n          chestExpansion = Easing.easeInCubic(phaseProgress) * inhaleDepth * 0.06\n          break\n        case 'hold':\n          // Held breath\n          chestExpansion = inhaleDepth * 0.06\n          break\n        case 'exhale':\n          // Slow exhale\n          chestExpansion = inhaleDepth * 0.06 * (1 - Easing.easeOutCubic(phaseProgress))\n          break\n        default:\n          chestExpansion = baseBreath * 0.02\n      }\n\n      state.chestSpring.setTarget(chestExpansion)\n      state.chestSpring.update(dt)\n      const smoothChest = state.chestSpring.value\n\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -smoothChest))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -smoothChest * 1.5))\n      }\n\n      // Shoulders rise with inhale\n      const shoulderRise = state.phase === 'inhale' ? phaseProgress * 0.02 * inhaleDepth : 0\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderRise))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderRise))\n      }\n\n      // ========================================\n      // LAYER 4: HEAD MOVEMENT\n      // ========================================\n\n      let headTiltX = 0\n      let headTiltY = 0\n\n      switch (state.phase) {\n        case 'inhale':\n          // Slight head tilt back during inhale\n          headTiltX = -0.05 * phaseProgress * intensity\n          break\n        case 'exhale':\n          // Head forward/up for exhale\n          headTiltX = 0.03 * (1 - phaseProgress) * intensity\n          headTiltY = handSide * 0.02 * phaseProgress * intensity\n          break\n        default:\n          // Subtle idle movement\n          headTiltX = noise.noise2D(t * 0.2, 600) * 0.02 * intensity\n          headTiltY = noise.noise2D(t * 0.15, 700) * 0.025 * intensity\n      }\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headTiltY))\n        rig.setRotation('head', headRot)\n      }\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX * 0.4))\n      }\n\n      // ========================================\n      // LAYER 5: EYES\n      // ========================================\n\n      if (eyeSquint) {\n        let eyeSquintAmount = 0\n\n        if (state.phase === 'inhale') {\n          eyeSquintAmount = phaseProgress * 0.15 * intensity\n        } else if (state.phase === 'hold') {\n          eyeSquintAmount = 0.15 * intensity\n        } else if (state.phase === 'exhale') {\n          eyeSquintAmount = 0.15 * intensity * (1 - phaseProgress)\n        }\n\n        // Eye rotation for squint effect\n        if (rig.hasBone('leftEye')) {\n          rig.setRotation('leftEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeSquintAmount))\n        }\n        if (rig.hasBone('rightEye')) {\n          rig.setRotation('rightEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeSquintAmount))\n        }\n      }\n\n      // Blinking\n      const blinkChance = noise.noise2D(t * 0.3, 800)\n      if (!state.isBlinking && blinkChance > 0.95) {\n        state.isBlinking = true\n        state.blinkTimer = 0\n      }\n\n      if (state.isBlinking) {\n        state.blinkTimer += dt\n        if (state.blinkTimer > 0.15) {\n          state.isBlinking = false\n        } else {\n          const blinkProgress = Math.sin(state.blinkTimer / 0.15 * Math.PI)\n          if (rig.hasBone('leftEye')) {\n            rig.addRotation('leftEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -blinkProgress * 0.25))\n          }\n          if (rig.hasBone('rightEye')) {\n            rig.addRotation('rightEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -blinkProgress * 0.25))\n          }\n        }\n      }\n\n      // ========================================\n      // LAYER 6: JAW (EXHALE)\n      // ========================================\n\n      if (jawAnimation && rig.hasBone('jaw')) {\n        let jawOpen = 0\n\n        if (state.phase === 'exhale') {\n          // Open slightly for exhale\n          const exhaleJaw = Math.sin(phaseProgress * Math.PI) * 0.08 * intensity\n          jawOpen = exhaleJaw\n        }\n\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawOpen))\n      }\n\n      // ========================================\n      // LAYER 7: LEGS & WEIGHT\n      // ========================================\n\n      // Relaxed stance with slight weight shift\n      const weightShift = noise.noise2D(t * 0.1, 900) * 0.03 * intensity\n\n      if (rig.hasBone('leftUpperLeg')) {\n        rig.setRotation('leftUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 + weightShift))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        rig.setRotation('rightUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03 - weightShift))\n      }\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.05))\n      }\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08))\n      }\n\n      // Feet slightly turned out\n      if (rig.hasBone('leftFoot')) {\n        rig.setRotation('leftFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.1))\n      }\n      if (rig.hasBone('rightFoot')) {\n        rig.setRotation('rightFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.15))\n      }\n    },\n  }\n}\n\nexport const smokingCigarette: MotionProgram<SmokingCigaretteParams> = createSmokingCigarette({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Implements a discrete state machine (SmokingPhase + transitions) which directly contradicts the block constraint requiring PHASE ENVELOPES and NO discrete state machine. The domain requires overlapping phase envelopes (continuous, overlapping envelopes per body part) rather than hard phase switches.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Hand-to-mouth positioning uses direct bone rotation targets (upperArm / lowerArm / hand rotations) rather than an end-effector IK or precise targeting system. The block constraints require accurate end-effector targeting (IK or equivalent) to ensure the hand reliably reaches the mouth.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Top-of-file documentation is insufficient relative to Blocks philosophy: it does not specify the detailed timing relationships (e.g., 'shoulder leads elbow by ~80ms, elbow follows by ~50ms, wrist ~30ms'), how the motion should FEEL across styles, nor explicitly cite biomechanical/observational research. The block requires detailed documentation that describes HOW the motion should FEEL and specific timing offsets.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Anticipation and lead/lag relationships between shoulder → elbow → wrist → fingers are not implemented as explicit overlapping phase envelopes or timed offsets. The code sets per-phase targets largely simultaneously and relies on springs, but does not encode the required lead timings (e.g., shoulder leads elbow by ~80ms, elbow leads wrist by ~50ms, wrist leads fingers by ~30ms).\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"The block philosophy mandates that every motion 'consider ALL 69 VRM bones' and that core bones are always active. This implementation touches many core bones but does not address the full VRM bone set (toes, many finger subtleties, etc.). The motion therefore does not meet the 'all 69 bones' consideration requirement.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"The code introduces an explicit 'state-machine' concept and meta tag ('state-machine') which is contradictory to the Block Definition constraints (explicitly demanded NO discrete state machine). This is a conceptual mismatch between block definition and implementation.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Some important domain constraints are only partially addressed or left implicit: anatomical limits / joint clamping are not enforced (rotations are created without explicit clamping to safe ranges). The Blocks philosophy requires joint rotations to respect anatomical limits.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"applyCigaretteGrip and applyFingerCurl are invoked without explicit hasBone() guards in this file. While these helper functions may internally guard bone access, the block philosophy requires hasBone() checks wrap optional bone access; it is unclear from this file whether the helpers perform those checks.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Phase-duration noise computation uses noise.noise2D(t, phase.length * 100). Using the string length of a phase identifier (phase.length) as a constant for noise frequency is fragile and likely unintended; it couples noise behavior to name length instead of an explicit numeric seed/frequency. This is a stylistic/robustness issue that can lead to confusing variability.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"The implementation relies on a discrete phaseProgress per state; it does not express overlapping envelopes per body part (separate envelopes for shoulder, elbow, wrist, chest, head, fingers) which would allow graceful blending between phases as required by the domain's overlapping-phase philosophy.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    }\n  ],\n  \"summary\": \"Failed validation. The implementation uses a discrete state machine and direct bone rotations instead of overlapping phase envelopes and end-effector targeting required by the domain. Documentation is insufficient (missing explicit timing offsets and detailed biomechanical explanation). Several explicit domain rules are unmet (lead/lag timing between shoulder→elbow→wrist→fingers, consideration of all VRM bones, explicit anatomical limits). The code does include useful elements (breathing coupling, chest/shoulder/head links, hasBone checks in many places, finger-grip helpers), but the core architectural mismatch (state machine vs. overlapping phase envelopes and missing IK targeting) causes the block to fail domain validation.\"\n}",
            "tokensUsed": {
              "input": 7382,
              "output": 2669
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "seductive_walk",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/seductive-walk.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 1,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "seductive_walk",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe runway walk biomechanics",
                    "DOCUMENTATION: Must specify timing delays between body parts",
                    "GAIT: Full cycle with stance/swing phases per leg",
                    "GAIT: Crossover step pattern (feet cross midline)",
                    "HIPS: Exaggerated lateral sway, hip drop on swing side",
                    "HIPS: Forward pelvic tilt for posture",
                    "HIPS: Twist opposite to shoulders",
                    "SPINE: Counter-rotation balances hip sway",
                    "SPINE: S-curve through spine chain",
                    "SHOULDERS: Counter-rotate opposite to hips",
                    "SHOULDERS: Drop with arm swing",
                    "ARMS: Fluid swing with DELAY (secondary motion)",
                    "ARMS: Wrist trails elbow",
                    "HANDS: Graceful, slightly spread fingers",
                    "HANDS: Wrist rotation adds elegance",
                    "LEGS: Full hip/knee/ankle coordination",
                    "LEGS: Toe point during swing phase",
                    "FEET: Heel-toe roll, slight turn out",
                    "TOES: Point during swing, grip during stance",
                    "HEAD: Stable (vestibular reflex) with slight tilt",
                    "HEAD: Can include flirtatious glances",
                    "EYES: Confident forward gaze, slow blinks",
                    "BREATH: Couples to gait cycle",
                    "WEIGHT: Visible transfer, hip bounce"
                  ]
                }
              ],
              "description": "Runway-style walk with exaggerated hip sway and fluid full-body coordination.\nEvery step is a performance. Weight transfer is deliberate and visible.\n\nFEEL: Confident, alluring, aware of being watched. Like a model on runway.\n\nGAIT CYCLE TIMING:\n- Hips lead the motion (lateral sway peaks at mid-stance)\n- Spine counter-rotates to balance hip sway\n- Shoulders counter-rotate opposite to hips\n- Arms swing opposite to legs with DELAY (secondary motion)\n- Wrists trail arms, fingers trail wrists\n- Head stays relatively stable (vestibular reflex)\n- Eyes can flirt with \"audience\"\n\nCROSSOVER: Feet cross midline for exaggerated hip motion.\nEach step places foot slightly past center.\n",
              "path": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Runway-style walk with exaggerated hip sway and fl...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 44461,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Documentation is insufficient: the file header does not specify explicit timing relationships (numeric delays / phase offsets) between body parts (hips, spine, shoulders, arms, wrists, fingers, breath). The block-level constraints require documentation that describes how the motion should FEEL, timing relationships between body parts, and citations to biomechanical or observational research; the file lists high-level research topics but includes no citations or explicit timing specs.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Hip drop is computed (hipDrop variable) but never applied to any hip/spine translation or rotation. The output constraint 'HIPS: Hip drop on swing side' is therefore not implemented.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Shoulder counter-rotation (shoulderTwist) is computed and stored in a spring but never applied to shoulder or chest bones. The required behavior 'SHOULDERS: Counter-rotate opposite to hips' is not implemented (only a shoulder drop is applied).",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Breath coupling is missing: there is no explicit breath (chest/abdomen) oscillation tied to the gait cycle. The required 'BREATH: Couples to gait cycle' constraint is not implemented.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Toes do not implement a 'grip during stance' behavior. The toes are set to 0 during stance and only point during swing; the 'TOES: Point during swing, grip during stance' constraint is not satisfied.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Several numeric constants (spring stiffness/damping, multipliers like 0.05, 0.08, phase offsets such as -0.3, etc.) are used throughout without in-code justification or citations. Per the Blocks philosophy ('LLMs lack embodied understanding - every numerical choice must be justified'), these choices should be explained in the documentation.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The code claims to 'consider ALL 69 VRM bones' in the philosophy, but the implementation only addresses a large subset of common bones (hips, spine chain, shoulders, arms, hands, fingers via helpers, legs, feet, toes, eyes). There is no explicit enumeration or handling for the full VRM 69-bone set (e.g. clavicle-specific handling, detailed finger bone joints, facial bones beyond eyes). If the block-level rule requires explicit consideration or graceful handling for every VRM bone, this should be documented or explicitly iterated; currently this is only implicit and not verifiably complete.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "seductive-walk.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "Failed validation. The implementation expresses core domain intent (gait cycle, hip sway, arm swing, optional-toe pointing, fingers, eyes/blinking, and uses rig/ctx correctly with hasBone checks). However, it fails several explicit output constraints: the documentation is not sufficiently detailed (missing timing relationships and citations), hip drop is computed but unused, shoulder counter-rotation is computed but not applied, breath coupling is not implemented, and toes do not implement a stance 'grip'. Additionally, numerical constants lack in-code justification and the file does not explicitly demonstrate handling of all VRM bones. Addressing these errors and documenting numeric/timing choices will bring the block into compliance."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: seductive_walk\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Runway-style walk with exaggerated hip sway and fluid full-body coordination.\\nEvery step is a performance. Weight transfer is deliberate and visible.\\n\\nFEEL: Confident, alluring, aware of being watched. Like a model on runway.\\n\\nGAIT CYCLE TIMING:\\n- Hips lead the motion (lateral sway peaks at mid-stance)\\n- Spine counter-rotates to balance hip sway\\n- Shoulders counter-rotate opposite to hips\\n- Arms swing opposite to legs with DELAY (secondary motion)\\n- Wrists trail arms, fingers trail wrists\\n- Head stays relatively stable (vestibular reflex)\\n- Eyes can flirt with \\\"audience\\\"\\n\\nCROSSOVER: Feet cross midline for exaggerated hip motion.\\nEach step places foot slightly past center.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/seductive-walk.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe runway walk biomechanics\",\n        \"DOCUMENTATION: Must specify timing delays between body parts\",\n        \"GAIT: Full cycle with stance/swing phases per leg\",\n        \"GAIT: Crossover step pattern (feet cross midline)\",\n        \"HIPS: Exaggerated lateral sway, hip drop on swing side\",\n        \"HIPS: Forward pelvic tilt for posture\",\n        \"HIPS: Twist opposite to shoulders\",\n        \"SPINE: Counter-rotation balances hip sway\",\n        \"SPINE: S-curve through spine chain\",\n        \"SHOULDERS: Counter-rotate opposite to hips\",\n        \"SHOULDERS: Drop with arm swing\",\n        \"ARMS: Fluid swing with DELAY (secondary motion)\",\n        \"ARMS: Wrist trails elbow\",\n        \"HANDS: Graceful, slightly spread fingers\",\n        \"HANDS: Wrist rotation adds elegance\",\n        \"LEGS: Full hip/knee/ankle coordination\",\n        \"LEGS: Toe point during swing phase\",\n        \"FEET: Heel-toe roll, slight turn out\",\n        \"TOES: Point during swing, grip during stance\",\n        \"HEAD: Stable (vestibular reflex) with slight tilt\",\n        \"HEAD: Can include flirtatious glances\",\n        \"EYES: Confident forward gaze, slow blinks\",\n        \"BREATH: Couples to gait cycle\",\n        \"WEIGHT: Visible transfer, hip bounce\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- seductive-walk.ts ---\n```\n/**\n * Seductive Walk Motion\n *\n * Runway-style walking with exaggerated hip sway, fluid arm movement,\n * and confident head carriage. Full gait cycle implementation with\n * crossover step pattern.\n *\n * Research basis:\n * - Runway/catwalk biomechanics\n * - Hip kinematics during walking\n * - Arm swing dynamics and coordination\n * - Weight transfer patterns\n * - Secondary motion physics\n */\n\nimport { z } from 'zod'\nimport { Vector3 } from 'three'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  walkPhase,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  getAvailableBones,\n  applyFingerCurl,\n  applyFingerSpread,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const seductiveWalkParamsSchema = z.object({\n  /** Overall intensity of exaggeration (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Walking speed (steps per second). Default: 0.8 */\n  speed: z.number().min(0.3).max(2).default(0.8),\n  /** Amount of hip sway (0-1). Default: 0.8 */\n  hipSwayAmount: z.number().min(0).max(1).default(0.8),\n  /** Fluidity of arm movement (0-1). Default: 0.7 */\n  armFlowiness: z.number().min(0).max(1).default(0.7),\n  /** Head tilt angle (0-1). Default: 0.3 */\n  headTilt: z.number().min(0).max(1).default(0.3),\n  /** Enable crossover step pattern. Default: true */\n  crossoverStep: z.boolean().default(true),\n  /** Enable toe point during swing. Default: true */\n  toePoint: z.boolean().default(true),\n  /** Enable secondary motion (follow-through). Default: true */\n  secondaryMotion: z.boolean().default(true),\n  /** Enable finger movement. Default: true */\n  fingerAnimation: z.boolean().default(true),\n})\n\nexport type SeductiveWalkParams = z.infer<typeof seductiveWalkParamsSchema>\nexport type SeductiveWalkInput = z.input<typeof seductiveWalkParamsSchema>\n\nexport const seductiveWalkMeta: MotionMeta = {\n  id: 'seductive-walk',\n  name: 'Seductive Walk',\n  description: 'Runway-style walk with exaggerated hip sway and fluid arm movement',\n  tags: ['walk', 'locomotion', 'seductive', 'runway'],\n  author: 'posers',\n}\n\n// ============================================================================\n// GAIT CYCLE HELPERS\n// ============================================================================\n\n/**\n * Gait cycle phases (0-1):\n * 0.0 - 0.1: Right heel strike / Left toe off\n * 0.1 - 0.3: Right loading response\n * 0.3 - 0.5: Right midstance / Left swing\n * 0.5 - 0.6: Left heel strike / Right toe off\n * 0.6 - 0.8: Left loading response\n * 0.8 - 1.0: Left midstance / Right swing\n */\n\nfunction getGaitPhase(t: number, speed: number): number {\n  const cycleTime = 1 / speed\n  return (t / cycleTime) % 1\n}\n\nfunction legSwingCurve(phase: number): number {\n  // Smooth leg swing with acceleration\n  return Easing.easeInOutSine(phase)\n}\n\nfunction hipDropCurve(phase: number): number {\n  // Hip drops during swing phase, rises during stance\n  return Math.sin(phase * Math.PI * 2)\n}\n\nfunction armSwingCurve(phase: number, flowiness: number): number {\n  // Fluid arm swing with follow-through\n  const base = Math.sin(phase * Math.PI * 2)\n  const secondary = Math.sin((phase * Math.PI * 2) - 0.3) * 0.2 * flowiness\n  return base + secondary\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface SeductiveWalkState {\n  noise: NoiseGenerator\n  hipSpring: Spring\n  shoulderSpring: Spring\n  headSpring: Spring\n  leftArmSpring: Spring\n  rightArmSpring: Spring\n  blinkTimer: number\n  isBlinking: boolean\n}\n\n/**\n * Initialize deterministic state with seeded random generators.\n * The noise generator uses the seed to ensure identical output\n * for the same seed value across all calls.\n */\nfunction initState(seed: number): SeductiveWalkState {\n  // Noise generator is seeded for deterministic, reproducible motion\n  return {\n    noise: createNoiseGenerator(seed),\n    hipSpring: createSpring({ stiffness: 150, damping: 15 }),\n    shoulderSpring: createSpring({ stiffness: 120, damping: 12 }),\n    headSpring: createSpring({ stiffness: 180, damping: 20 }),\n    leftArmSpring: createSpring({ stiffness: 80, damping: 10 }),\n    rightArmSpring: createSpring({ stiffness: 80, damping: 10 }),\n    blinkTimer: 0,\n    isBlinking: false,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a seductive walk motion with full gait cycle implementation.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (toes, eyes, fingers) are gracefully skipped\n * - The animation degrades gracefully when bones are unavailable\n * - Core bones (hips, spine, legs) are all checked before use\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - Gait phase is purely time-based with no random elements\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - Spring physics use efficient semi-implicit Euler integration\n * - Gait phase calculations are simple trigonometry\n * - No allocations in hot path except quaternion creation\n */\nexport function createSeductiveWalk(params: SeductiveWalkInput = {}): MotionProgram<SeductiveWalkParams> {\n  const validatedParams = seductiveWalkParamsSchema.parse(params)\n  let state: SeductiveWalkState | null = null\n\n  return {\n    meta: seductiveWalkMeta,\n    paramsSchema: seductiveWalkParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        intensity,\n        speed,\n        hipSwayAmount,\n        armFlowiness,\n        headTilt,\n        crossoverStep,\n        toePoint,\n        secondaryMotion,\n        fingerAnimation,\n      } = validatedParams\n\n      const noise = state.noise\n      const gaitPhase = getGaitPhase(t, speed)\n\n      // Which leg is in stance phase\n      const rightStance = gaitPhase < 0.5\n      const leftStance = !rightStance\n\n      // Swing phase for each leg (0-1 during their swing)\n      const rightSwingPhase = rightStance ? 0 : (gaitPhase - 0.5) * 2\n      const leftSwingPhase = leftStance ? 0 : gaitPhase * 2\n\n      // ========================================\n      // LAYER 1: HIP MOVEMENT (CORE OF THE WALK)\n      // ========================================\n\n      // Hip sway - lateral movement\n      const hipSwayTarget = Math.sin(gaitPhase * Math.PI * 2) * hipSwayAmount * 0.12 * intensity\n      state.hipSpring.setTarget(hipSwayTarget)\n      state.hipSpring.update(dt)\n      const hipSway = state.hipSpring.value\n\n      // Hip rotation (twist) - counter to shoulders\n      const hipTwist = Math.sin(gaitPhase * Math.PI * 2) * 0.08 * intensity\n\n      // Hip drop on swing side\n      const hipDrop = hipDropCurve(gaitPhase) * 0.05 * intensity\n\n      // Forward hip oscillation (pelvic tilt during gait)\n      const hipForward = Math.sin(gaitPhase * Math.PI * 4) * 0.02 * intensity\n\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, hipSway)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, hipTwist))\n        hipsRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hipForward))\n        rig.setRotation('hips', hipsRot)\n\n        // Hip translation for bounce\n        const hipBounce = Math.abs(Math.sin(gaitPhase * Math.PI * 2)) * 0.01 * intensity\n        rig.setHipsPositionOffset(new Vector3(hipSway * 0.1, hipBounce, 0))\n      }\n\n      // ========================================\n      // LAYER 2: SPINE COUNTER-ROTATION\n      // ========================================\n\n      // Spine opposes hip movement for balance\n      const spineCounter = -hipTwist * 0.4\n      const spineSway = -hipSway * 0.3\n\n      if (rig.hasBone('spine')) {\n        const spineRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter * 0.5)\n        spineRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.5))\n        rig.setRotation('spine', spineRot)\n      }\n\n      if (rig.hasBone('chest')) {\n        const chestRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter * 0.8)\n        chestRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.3))\n        // Slight chest forward for confidence\n        chestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n        rig.setRotation('chest', chestRot)\n      }\n\n      if (rig.hasBone('upperChest')) {\n        const upperChestRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter)\n        upperChestRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.2))\n        upperChestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.04 * intensity))\n        rig.setRotation('upperChest', upperChestRot)\n      }\n\n      // ========================================\n      // LAYER 3: SHOULDER MOVEMENT\n      // ========================================\n\n      // Shoulders counter-rotate to hips\n      const shoulderTarget = -hipTwist * 0.6\n      state.shoulderSpring.setTarget(shoulderTarget)\n      state.shoulderSpring.update(dt)\n      const shoulderTwist = state.shoulderSpring.value\n\n      // Shoulder drop with arm swing\n      const leftShoulderDrop = armSwingCurve(gaitPhase, armFlowiness) * 0.02 * intensity\n      const rightShoulderDrop = armSwingCurve(gaitPhase + 0.5, armFlowiness) * 0.02 * intensity\n\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -leftShoulderDrop)\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, rightShoulderDrop)\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // ========================================\n      // LAYER 4: ARM SWING\n      // ========================================\n\n      // Fluid arm swing with secondary motion\n      const leftArmSwingBase = armSwingCurve(gaitPhase, armFlowiness) * 0.35 * intensity * armFlowiness\n      const rightArmSwingBase = armSwingCurve(gaitPhase + 0.5, armFlowiness) * 0.35 * intensity * armFlowiness\n\n      // Apply spring for smoothness\n      state.leftArmSpring.setTarget(leftArmSwingBase)\n      state.rightArmSpring.setTarget(rightArmSwingBase)\n      state.leftArmSpring.update(dt)\n      state.rightArmSpring.update(dt)\n\n      const leftArmSwing = state.leftArmSpring.value\n      const rightArmSwing = state.rightArmSpring.value\n\n      // Secondary motion - wrist lag\n      const leftWristLag = secondaryMotion ? leftArmSwing * 0.3 : 0\n      const rightWristLag = secondaryMotion ? rightArmSwing * 0.3 : 0\n\n      if (rig.hasBone('leftUpperArm')) {\n        const leftUpperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftArmSwing)\n        leftUpperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.08 * intensity))\n        rig.setRotation('leftUpperArm', leftUpperArmRot)\n      }\n\n      if (rig.hasBone('rightUpperArm')) {\n        const rightUpperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightArmSwing)\n        rightUpperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.08 * intensity))\n        rig.setRotation('rightUpperArm', rightUpperArmRot)\n      }\n\n      // Lower arms - slight bend with secondary motion\n      if (rig.hasBone('leftLowerArm')) {\n        const leftLowerRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.1 - leftWristLag * 0.2)\n        rig.setRotation('leftLowerArm', leftLowerRot)\n      }\n\n      if (rig.hasBone('rightLowerArm')) {\n        const rightLowerRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.1 + rightWristLag * 0.2)\n        rig.setRotation('rightLowerArm', rightLowerRot)\n      }\n\n      // Wrists - graceful rotation\n      if (rig.hasBone('leftHand')) {\n        const leftHandRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.1 + leftWristLag * 0.15)\n        leftHandRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftWristLag * 0.1))\n        rig.setRotation('leftHand', leftHandRot)\n      }\n\n      if (rig.hasBone('rightHand')) {\n        const rightHandRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.1 - rightWristLag * 0.15)\n        rightHandRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightWristLag * 0.1))\n        rig.setRotation('rightHand', rightHandRot)\n      }\n\n      // ========================================\n      // LAYER 5: LEG MOVEMENT\n      // ========================================\n\n      // Right leg\n      const rightLegForward = rightStance\n        ? -0.1 * (gaitPhase * 2) * intensity // Pushing back during stance\n        : Math.sin(rightSwingPhase * Math.PI) * 0.4 * intensity // Swinging forward\n\n      const rightLegAbduction = crossoverStep\n        ? Math.sin(gaitPhase * Math.PI * 2 + Math.PI) * 0.05 * intensity // Crossover\n        : 0\n\n      if (rig.hasBone('rightUpperLeg')) {\n        const rightUpperLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightLegForward)\n        rightUpperLegRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, rightLegAbduction))\n        rig.setRotation('rightUpperLeg', rightUpperLegRot)\n      }\n\n      // Right knee bend\n      const rightKneeBend = rightStance\n        ? 0.08 * intensity // Slight bend in stance\n        : Math.sin(rightSwingPhase * Math.PI) * 0.6 * intensity // Bend during swing\n\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightKneeBend))\n      }\n\n      // Left leg\n      const leftLegForward = leftStance\n        ? -0.1 * ((gaitPhase - 0.5) * 2) * intensity\n        : Math.sin(leftSwingPhase * Math.PI) * 0.4 * intensity\n\n      const leftLegAbduction = crossoverStep\n        ? Math.sin(gaitPhase * Math.PI * 2) * 0.05 * intensity\n        : 0\n\n      if (rig.hasBone('leftUpperLeg')) {\n        const leftUpperLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftLegForward)\n        leftUpperLegRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, leftLegAbduction))\n        rig.setRotation('leftUpperLeg', leftUpperLegRot)\n      }\n\n      const leftKneeBend = leftStance\n        ? 0.08 * intensity\n        : Math.sin(leftSwingPhase * Math.PI) * 0.6 * intensity\n\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftKneeBend))\n      }\n\n      // ========================================\n      // LAYER 6: FEET & TOES\n      // ========================================\n\n      // Right foot\n      const rightFootAngle = rightStance\n        ? -0.1 * intensity // Flat/slight heel strike\n        : (toePoint ? -0.3 * Math.sin(rightSwingPhase * Math.PI) * intensity : 0) // Toe point during swing\n\n      if (rig.hasBone('rightFoot')) {\n        const rightFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightFootAngle)\n        rightFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.05)) // Slight turn out\n        rig.setRotation('rightFoot', rightFootRot)\n      }\n\n      if (rig.hasBone('rightToes') && toePoint) {\n        const rightToePoint = rightStance ? 0 : Math.sin(rightSwingPhase * Math.PI) * 0.4 * intensity\n        rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightToePoint))\n      }\n\n      // Left foot\n      const leftFootAngle = leftStance\n        ? -0.1 * intensity\n        : (toePoint ? -0.3 * Math.sin(leftSwingPhase * Math.PI) * intensity : 0)\n\n      if (rig.hasBone('leftFoot')) {\n        const leftFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftFootAngle)\n        leftFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.05))\n        rig.setRotation('leftFoot', leftFootRot)\n      }\n\n      if (rig.hasBone('leftToes') && toePoint) {\n        const leftToePoint = leftStance ? 0 : Math.sin(leftSwingPhase * Math.PI) * 0.4 * intensity\n        rig.setRotation('leftToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftToePoint))\n      }\n\n      // ========================================\n      // LAYER 7: HEAD (STABLE WITH TILT)\n      // ========================================\n\n      // Head stays relatively stable (vestibular reflex)\n      const headCompensation = -hipSway * 0.3\n      const headTiltAmount = headTilt * 0.08 * intensity\n\n      // Subtle look direction\n      const lookDirection = noise.noise2D(t * 0.1, 100) * 0.05 * intensity\n\n      state.headSpring.setTarget(headCompensation)\n      state.headSpring.update(dt)\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, state.headSpring.value + headTiltAmount)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, lookDirection))\n        // Slight chin up for confidence\n        headRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        const neckRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, state.headSpring.value * 0.3)\n        neckRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.02 * intensity))\n        rig.setRotation('neck', neckRot)\n      }\n\n      // ========================================\n      // LAYER 8: FINGERS\n      // ========================================\n\n      if (fingerAnimation) {\n        // Graceful, slightly spread fingers\n        const fingerCurl = 0.15 + noise.noise2D(t * 0.2, 200) * 0.05\n\n        applyFingerCurl(rig, 'left', {\n          thumb: fingerCurl * 0.4,\n          index: fingerCurl * 0.8,\n          middle: fingerCurl * 0.9,\n          ring: fingerCurl,\n          little: fingerCurl * 1.1,\n        })\n\n        applyFingerCurl(rig, 'right', {\n          thumb: fingerCurl * 0.4,\n          index: fingerCurl * 0.8,\n          middle: fingerCurl * 0.9,\n          ring: fingerCurl,\n          little: fingerCurl * 1.1,\n        })\n\n        // Elegant spread\n        applyFingerSpread(rig, 'left', 0.4 * intensity)\n        applyFingerSpread(rig, 'right', 0.4 * intensity)\n      }\n\n      // ========================================\n      // LAYER 9: EYES\n      // ========================================\n\n      // Confident forward gaze with subtle movement\n      const eyeX = noise.noise2D(t * 0.15, 300) * 0.03 * intensity\n      const eyeY = noise.noise2D(t * 0.12, 400) * 0.02 * intensity\n\n      // Blinking\n      const blinkChance = noise.noise2D(t * 0.25, 500)\n      if (!state.isBlinking && blinkChance > 0.95) {\n        state.isBlinking = true\n        state.blinkTimer = 0\n      }\n\n      let blinkAmount = 0\n      if (state.isBlinking) {\n        state.blinkTimer += dt\n        if (state.blinkTimer > 0.15) {\n          state.isBlinking = false\n        } else {\n          blinkAmount = Math.sin(state.blinkTimer / 0.15 * Math.PI) * 0.25\n        }\n      }\n\n      if (rig.hasBone('leftEye')) {\n        const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n        leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n        rig.setRotation('leftEye', leftEyeRot)\n      }\n\n      if (rig.hasBone('rightEye')) {\n        const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n        rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n        rig.setRotation('rightEye', rightEyeRot)\n      }\n    },\n  }\n}\n\nexport const seductiveWalk: MotionProgram<SeductiveWalkParams> = createSeductiveWalk({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Documentation is insufficient: the file header does not specify explicit timing relationships (numeric delays / phase offsets) between body parts (hips, spine, shoulders, arms, wrists, fingers, breath). The block-level constraints require documentation that describes how the motion should FEEL, timing relationships between body parts, and citations to biomechanical or observational research; the file lists high-level research topics but includes no citations or explicit timing specs.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Hip drop is computed (hipDrop variable) but never applied to any hip/spine translation or rotation. The output constraint 'HIPS: Hip drop on swing side' is therefore not implemented.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Shoulder counter-rotation (shoulderTwist) is computed and stored in a spring but never applied to shoulder or chest bones. The required behavior 'SHOULDERS: Counter-rotate opposite to hips' is not implemented (only a shoulder drop is applied).\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Breath coupling is missing: there is no explicit breath (chest/abdomen) oscillation tied to the gait cycle. The required 'BREATH: Couples to gait cycle' constraint is not implemented.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Toes do not implement a 'grip during stance' behavior. The toes are set to 0 during stance and only point during swing; the 'TOES: Point during swing, grip during stance' constraint is not satisfied.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Several numeric constants (spring stiffness/damping, multipliers like 0.05, 0.08, phase offsets such as -0.3, etc.) are used throughout without in-code justification or citations. Per the Blocks philosophy ('LLMs lack embodied understanding - every numerical choice must be justified'), these choices should be explained in the documentation.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"The code claims to 'consider ALL 69 VRM bones' in the philosophy, but the implementation only addresses a large subset of common bones (hips, spine chain, shoulders, arms, hands, fingers via helpers, legs, feet, toes, eyes). There is no explicit enumeration or handling for the full VRM 69-bone set (e.g. clavicle-specific handling, detailed finger bone joints, facial bones beyond eyes). If the block-level rule requires explicit consideration or graceful handling for every VRM bone, this should be documented or explicitly iterated; currently this is only implicit and not verifiably complete.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    }\n  ],\n  \"summary\": \"Failed validation. The implementation expresses core domain intent (gait cycle, hip sway, arm swing, optional-toe pointing, fingers, eyes/blinking, and uses rig/ctx correctly with hasBone checks). However, it fails several explicit output constraints: the documentation is not sufficiently detailed (missing timing relationships and citations), hip drop is computed but unused, shoulder counter-rotation is computed but not applied, breath coupling is not implemented, and toes do not implement a stance 'grip'. Additionally, numerical constants lack in-code justification and the file does not explicitly demonstrate handling of all VRM bones. Addressing these errors and documenting numeric/timing choices will bring the block into compliance.\"\n}",
            "tokensUsed": {
              "input": 6879,
              "output": 3314
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "contemplative_lean",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/contemplative-lean.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "contemplative_lean",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe contemplative body language",
                    "DOCUMENTATION: Must specify pose variants and their meanings",
                    "ASYMMETRY: Clear weight distribution (one leg dominant)",
                    "ASYMMETRY: Hip drop on non-weight side",
                    "ASYMMETRY: Spine S-curve to balance",
                    "VARIANTS: chin_rest, crossed_arms, akimbo, hand_on_hip",
                    "VARIANTS: Each has specific arm/hand configuration",
                    "BREATH: Deep, slow (5-6 second cycles)",
                    "BREATH: Visible chest expansion",
                    "GESTURES: Thinking micro-movements (chin tap, lip touch)",
                    "GESTURES: Very slow and deliberate",
                    "GESTURES: Phase envelopes for gesture timing",
                    "HANDS: Detailed finger pose per variant",
                    "HANDS: Subtle finger adjustments",
                    "LEGS: Weight leg straight-ish, relaxed leg bent",
                    "FEET: Weight foot flat, relaxed foot on ball",
                    "HEAD: Tilted slightly, distant gaze angle",
                    "EYES: Slow movement, distant focus",
                    "EYES: Slow, contemplative blinks",
                    "JAW: Relaxed, occasional subtle movement",
                    "MICRO: Very slow noise layer (meditative)",
                    "TRANSITIONS: Smooth between gesture variants"
                  ]
                }
              ],
              "description": "Thoughtful asymmetric pose with weight on one leg and self-reflective gestures.\nMultiple variants: chin rest, crossed arms, akimbo, hand on hip.\n\nFEEL: Lost in thought, processing something, introspective. Like solving a puzzle.\n\nTIMING: Very slow micro-movements. Breath is deep and slow (5-6 seconds).\nThinking gestures are deliberate, not fidgety. Weight shifts are rare and slow.\nEyes have \"distant\" quality - not tracking environment.\n\nASYMMETRY: Weight mostly on one leg. Opposite hip drops. Spine curves.\nArms break symmetry with gesture variant.\n",
              "path": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Thoughtful asymmetric pose with weight on one leg ...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 57748,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Top-of-file documentation is present but not sufficiently detailed to meet the block rules. The Blocks philosophy requires that every motion file begin with detailed documentation describing HOW the motion should FEEL and specifying timing relationships between body parts. The header comment lists a few high-level topics but does not: (a) provide explicit timing relationships (e.g., phase offsets between hips/spine/head/shoulders), (b) document per-variant meanings or how variants differ in intent/timing beyond naming, or (c) state the intended breath period (5–6s) in human-readable form. This violates the 'every motion file MUST begin with detailed documentation' and the output constraints that documentation must describe contemplative body language and specify pose variants/meaning.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The block philosophy requires numerical choices to be justified (every numeric choice must be documented/justified). This source contains many numeric constants (spring stiffness/damping, rotation magnitudes, noise scales, gesture durations, breathing frequency constant 0.12, finger curl amounts, etc.) with little or no inline justification. Several of these constants appear to be tuned values but lack comment justification or citations. This violates the requirement that every numerical choice be justified.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Documentation does not include bibliographic citations or explicit references for the claimed 'Research basis'. The Block Definition and philosophy require citation of biomechanical or observational research basis. The header lists research topics but contains no citations, DOIs, papers, or observational references.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The block output constraints require 'BREATH: Deep, slow (5-6 second cycles)'. The implementation uses oscBreathing(t, 0.12, breathDepth) but the code does not document or justify how '0.12' maps to a 5–6s cycle (or otherwise guarantee the 5–6s target). There is no explicit, human-readable timing parameter or comment that ties the numeric breathing setup to the stated 5–6s requirement. Please document/justify the breathing frequency constant or make the breath period an explicit parameter.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Block philosophy expects motions to consider ALL 69 VRM bones or at least show explicit handling/acknowledgement of every VRM bone by name or via a documented fallback. This implementation touches many core bones (hips, spine, chest, upperChest, neck, head, shoulders, arms, hands, legs, knees, feet, toes, jaw, eyes, and calls applyFingerCurl) but does not enumerate or attempt to handle every VRM bone (e.g., clavicles, clavicle variants, some finger joint names, detailed face bones, etc.). If the project-level applyFingerCurl covers fingers and other helpers cover remaining bones, please state that in the header documentation. As-is, it's unclear whether all 69 bones are considered.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The block outputs require 'TRANSITIONS: Smooth between gesture variants'. The implementation reads poseVariant from validatedParams captured at creation time and applies the variant pose directly in the update loop. There is no explicit transition/blend logic between different poseVariant values if they are changed at runtime (no crossfade/spring/blend of armPose when variant changes). If runtime switching is expected, the code should implement blending so transitions are smooth.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "There is an unused import (BoneChains) at the top of the file. This is a minor cleanliness issue and may indicate a planned feature or omitted logic that would use chain-level coordination.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "contemplative-lean.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The motion program implements a thorough contemplative pose with guarded bone access, multiple variants, breathing, gesture state machine, and finger handling. However it fails the Blocks documentation and philosophy requirements: the top-of-file documentation is not sufficiently detailed (missing explicit timing relationships, per-variant meanings, and research citations), the many numeric parameters lack justifications, and the breathing timing is not explicitly tied to the required 5–6s cycle. Additional warnings: the implementation does not clearly demonstrate handling of all 69 VRM bones, lacks explicit smooth transitions between pose variants at runtime, and has an unused import. Fixing the documentation (add timing relationships, per-constant justifications, citations), exposing or documenting explicit breath period, and adding variant-blending or a note about why runtime switches are unsupported will address the major errors."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: contemplative_lean\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Thoughtful asymmetric pose with weight on one leg and self-reflective gestures.\\nMultiple variants: chin rest, crossed arms, akimbo, hand on hip.\\n\\nFEEL: Lost in thought, processing something, introspective. Like solving a puzzle.\\n\\nTIMING: Very slow micro-movements. Breath is deep and slow (5-6 seconds).\\nThinking gestures are deliberate, not fidgety. Weight shifts are rare and slow.\\nEyes have \\\"distant\\\" quality - not tracking environment.\\n\\nASYMMETRY: Weight mostly on one leg. Opposite hip drops. Spine curves.\\nArms break symmetry with gesture variant.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe contemplative body language\",\n        \"DOCUMENTATION: Must specify pose variants and their meanings\",\n        \"ASYMMETRY: Clear weight distribution (one leg dominant)\",\n        \"ASYMMETRY: Hip drop on non-weight side\",\n        \"ASYMMETRY: Spine S-curve to balance\",\n        \"VARIANTS: chin_rest, crossed_arms, akimbo, hand_on_hip\",\n        \"VARIANTS: Each has specific arm/hand configuration\",\n        \"BREATH: Deep, slow (5-6 second cycles)\",\n        \"BREATH: Visible chest expansion\",\n        \"GESTURES: Thinking micro-movements (chin tap, lip touch)\",\n        \"GESTURES: Very slow and deliberate\",\n        \"GESTURES: Phase envelopes for gesture timing\",\n        \"HANDS: Detailed finger pose per variant\",\n        \"HANDS: Subtle finger adjustments\",\n        \"LEGS: Weight leg straight-ish, relaxed leg bent\",\n        \"FEET: Weight foot flat, relaxed foot on ball\",\n        \"HEAD: Tilted slightly, distant gaze angle\",\n        \"EYES: Slow movement, distant focus\",\n        \"EYES: Slow, contemplative blinks\",\n        \"JAW: Relaxed, occasional subtle movement\",\n        \"MICRO: Very slow noise layer (meditative)\",\n        \"TRANSITIONS: Smooth between gesture variants\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- contemplative-lean.ts ---\n```\n/**\n * Contemplative Lean Motion\n *\n * Thoughtful asymmetric pose with weight on one leg and thinking gestures.\n * Multiple pose variants including chin rest, crossed arms, and akimbo.\n *\n * Research basis:\n * - Thoughtful posture psychology\n * - Asymmetric stance biomechanics\n * - Self-touch gestures in contemplation\n * - Weight-bearing postural patterns\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  BoneChains,\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const contemplativeLeanParamsSchema = z.object({\n  /** Pose variant. Default: chin_rest */\n  poseVariant: z.enum(['chin_rest', 'crossed_arms', 'akimbo', 'hand_on_hip']).default('chin_rest'),\n  /** Intensity of thinking gestures (0-1). Default: 0.5 */\n  thoughtIntensity: z.number().min(0).max(1).default(0.5),\n  /** Depth of breathing (0-1). Default: 0.6 */\n  breathDepth: z.number().min(0).max(1).default(0.6),\n  /** Amount of subtle fidgeting (0-1). Default: 0.3 */\n  fidgetAmount: z.number().min(0).max(1).default(0.3),\n  /** Which leg bears weight. Default: right */\n  weightLeg: z.enum(['left', 'right']).default('right'),\n  /** Overall animation intensity (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Enable eye movement. Default: true */\n  eyeMovement: z.boolean().default(true),\n  /** Enable head tilts. Default: true */\n  headTilts: z.boolean().default(true),\n})\n\nexport type ContemplativeLeanParams = z.infer<typeof contemplativeLeanParamsSchema>\nexport type ContemplativeLeanInput = z.input<typeof contemplativeLeanParamsSchema>\n\nexport const contemplativeLeanMeta: MotionMeta = {\n  id: 'contemplative-lean',\n  name: 'Contemplative Lean',\n  description: 'Thoughtful asymmetric pose with weight on one leg and thinking gestures',\n  tags: ['contemplative', 'thinking', 'pose', 'idle'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface ContemplativeLeanState {\n  noise: NoiseGenerator\n  weightSpring: Spring\n  headSpring: Spring\n  armSpring: Spring\n  blinkTimer: number\n  isBlinking: boolean\n  thinkingGestureTimer: number\n  currentGesture: 'idle' | 'head_tilt' | 'chin_tap' | 'look_away'\n  gestureDuration: number\n}\n\nfunction initState(seed: number): ContemplativeLeanState {\n  return {\n    noise: createNoiseGenerator(seed),\n    weightSpring: createSpring({ stiffness: 30, damping: 12 }),\n    headSpring: createSpring({ stiffness: 50, damping: 15 }),\n    armSpring: createSpring({ stiffness: 40, damping: 10 }),\n    blinkTimer: 0,\n    isBlinking: false,\n    thinkingGestureTimer: 0,\n    currentGesture: 'idle',\n    gestureDuration: 3,\n  }\n}\n\n// ============================================================================\n// POSE VARIANTS\n// ============================================================================\n\ninterface ArmPose {\n  leftUpperArm: { x: number; y: number; z: number }\n  leftLowerArm: { x: number; y: number; z: number }\n  leftHand: { x: number; y: number; z: number }\n  rightUpperArm: { x: number; y: number; z: number }\n  rightLowerArm: { x: number; y: number; z: number }\n  rightHand: { x: number; y: number; z: number }\n  leftFingers: { thumb: number; index: number; middle: number; ring: number; little: number }\n  rightFingers: { thumb: number; index: number; middle: number; ring: number; little: number }\n}\n\nfunction getArmPose(variant: string, intensity: number): ArmPose {\n  switch (variant) {\n    case 'chin_rest':\n      return {\n        // Right arm: hand to chin\n        rightUpperArm: { x: -0.8 * intensity, y: 0, z: -0.4 * intensity },\n        rightLowerArm: { x: 0, y: 1.3 * intensity, z: 0 },\n        rightHand: { x: 0.2 * intensity, y: 0, z: 0.1 * intensity },\n        // Left arm: support under right elbow\n        leftUpperArm: { x: 0.3 * intensity, y: 0, z: 0.25 * intensity },\n        leftLowerArm: { x: 0, y: -1.0 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.15 * intensity },\n        // Fingers\n        rightFingers: { thumb: 0.2, index: 0.15, middle: 0.35, ring: 0.5, little: 0.55 },\n        leftFingers: { thumb: 0.3, index: 0.35, middle: 0.4, ring: 0.45, little: 0.5 },\n      }\n\n    case 'crossed_arms':\n      return {\n        rightUpperArm: { x: 0.5 * intensity, y: 0, z: -0.35 * intensity },\n        rightLowerArm: { x: 0, y: 1.4 * intensity, z: 0 },\n        rightHand: { x: 0, y: 0, z: -0.2 * intensity },\n        leftUpperArm: { x: 0.4 * intensity, y: 0, z: 0.25 * intensity },\n        leftLowerArm: { x: 0, y: -1.3 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.15 * intensity },\n        rightFingers: { thumb: 0.2, index: 0.25, middle: 0.3, ring: 0.35, little: 0.4 },\n        leftFingers: { thumb: 0.25, index: 0.3, middle: 0.35, ring: 0.4, little: 0.45 },\n      }\n\n    case 'akimbo':\n      return {\n        rightUpperArm: { x: 0.15 * intensity, y: 0, z: -0.5 * intensity },\n        rightLowerArm: { x: 0, y: 1.4 * intensity, z: 0 },\n        rightHand: { x: -0.3 * intensity, y: 0, z: 0 },\n        leftUpperArm: { x: 0.15 * intensity, y: 0, z: 0.5 * intensity },\n        leftLowerArm: { x: 0, y: -1.4 * intensity, z: 0 },\n        leftHand: { x: -0.3 * intensity, y: 0, z: 0 },\n        rightFingers: { thumb: 0.1, index: 0.15, middle: 0.2, ring: 0.25, little: 0.3 },\n        leftFingers: { thumb: 0.1, index: 0.15, middle: 0.2, ring: 0.25, little: 0.3 },\n      }\n\n    case 'hand_on_hip':\n      return {\n        // Right hand on hip\n        rightUpperArm: { x: 0.2 * intensity, y: 0, z: -0.45 * intensity },\n        rightLowerArm: { x: 0, y: 1.3 * intensity, z: 0 },\n        rightHand: { x: -0.25 * intensity, y: 0, z: -0.1 * intensity },\n        // Left arm relaxed\n        leftUpperArm: { x: 0.05 * intensity, y: 0, z: 0.12 * intensity },\n        leftLowerArm: { x: 0, y: -0.15 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.1 * intensity },\n        rightFingers: { thumb: 0.15, index: 0.2, middle: 0.25, ring: 0.3, little: 0.35 },\n        leftFingers: { thumb: 0.25, index: 0.3, middle: 0.35, ring: 0.4, little: 0.45 },\n      }\n\n    default:\n      return getArmPose('chin_rest', intensity)\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a contemplative lean motion with multiple pose variants.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (jaw, eyes, toes, fingers) are gracefully skipped\n * - Dynamic bone names (weightLeg, relaxLeg) are cast and checked\n * - The animation degrades gracefully when bones are unavailable\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - Gesture selection is deterministic via seeded noise\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - Spring physics use efficient semi-implicit Euler integration\n * - Pose variant lookup is constant time\n * - No allocations in hot path except quaternion creation\n *\n * TRANSITIONS:\n * - Thinking gesture state machine provides fluid transitions\n * - Gesture weights use sine easing for smooth in/out\n * - Springs smooth all micro-movements\n */\nexport function createContemplativeLean(params: ContemplativeLeanInput = {}): MotionProgram<ContemplativeLeanParams> {\n  const validatedParams = contemplativeLeanParamsSchema.parse(params)\n  let state: ContemplativeLeanState | null = null\n\n  return {\n    meta: contemplativeLeanMeta,\n    paramsSchema: contemplativeLeanParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        poseVariant,\n        thoughtIntensity,\n        breathDepth,\n        fidgetAmount,\n        weightLeg,\n        intensity,\n        eyeMovement,\n        headTilts,\n      } = validatedParams\n\n      const noise = state.noise\n      const isRightWeight = weightLeg === 'right'\n      const weightSide = isRightWeight ? 1 : -1\n\n      // ========================================\n      // THINKING GESTURE STATE MACHINE\n      // ========================================\n\n      state.thinkingGestureTimer += dt\n      if (state.thinkingGestureTimer > state.gestureDuration) {\n        state.thinkingGestureTimer = 0\n        state.gestureDuration = 2 + noise.noise2D(t, 50) * 3\n\n        // Pick next gesture\n        const gestureRoll = noise.noise2D(t, 100)\n        if (gestureRoll > 0.7 && headTilts) {\n          state.currentGesture = 'head_tilt'\n        } else if (gestureRoll > 0.4 && poseVariant === 'chin_rest') {\n          state.currentGesture = 'chin_tap'\n        } else if (gestureRoll > 0.2) {\n          state.currentGesture = 'look_away'\n        } else {\n          state.currentGesture = 'idle'\n        }\n      }\n\n      const gestureProgress = state.thinkingGestureTimer / state.gestureDuration\n      const gestureWeight = Math.sin(gestureProgress * Math.PI) * thoughtIntensity\n\n      // ========================================\n      // LAYER 1: ASYMMETRIC WEIGHT DISTRIBUTION\n      // ========================================\n\n      // Hip drop on non-weight-bearing side\n      const hipDrop = 0.06 * intensity * weightSide\n      const hipShift = 0.04 * intensity * weightSide\n\n      // Subtle weight shift noise\n      const weightNoise = noise.noise2D(t * 0.08, 200) * fidgetAmount * 0.02\n      state.weightSpring.setTarget(hipShift + weightNoise)\n      state.weightSpring.update(dt)\n\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, hipDrop + state.weightSpring.value)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightSide * 0.03 * intensity))\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // ========================================\n      // LAYER 2: SPINE (RELAXED, SLIGHT CURVE)\n      // ========================================\n\n      if (rig.hasBone('spine')) {\n        const spineRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.3)\n        spineRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03 * intensity))\n        rig.setRotation('spine', spineRot)\n      }\n\n      if (rig.hasBone('chest')) {\n        const chestRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.2)\n        chestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n        rig.setRotation('chest', chestRot)\n      }\n\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.1))\n      }\n\n      // ========================================\n      // LAYER 3: DEEP CONTEMPLATIVE BREATHING\n      // ========================================\n\n      const breathPhase = oscBreathing(t, 0.12, breathDepth)\n\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.025))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.035))\n      }\n\n      // Shoulder rise with breath\n      const shoulderBreath = breathPhase * 0.012\n      if (rig.hasBone('leftShoulder')) {\n        rig.setRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.setRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 4: LEGS (WEIGHT DISTRIBUTION)\n      // ========================================\n\n      // Weight-bearing leg: straight\n      // Non-weight leg: bent, relaxed\n\n      const weightLegBone = isRightWeight ? 'rightUpperLeg' : 'leftUpperLeg'\n      const relaxLegBone = isRightWeight ? 'leftUpperLeg' : 'rightUpperLeg'\n      const weightKnee = isRightWeight ? 'rightLowerLeg' : 'leftLowerLeg'\n      const relaxKnee = isRightWeight ? 'leftLowerLeg' : 'rightLowerLeg'\n      const weightFoot = isRightWeight ? 'rightFoot' : 'leftFoot'\n      const relaxFoot = isRightWeight ? 'leftFoot' : 'rightFoot'\n\n      if (rig.hasBone(weightLegBone as VRMHumanBoneName)) {\n        // Slight hip flexion on weight-bearing side\n        rig.setRotation(weightLegBone as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n      }\n\n      if (rig.hasBone(relaxLegBone as VRMHumanBoneName)) {\n        // Relaxed leg forward and slightly bent\n        const relaxRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.15 * intensity)\n        relaxRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -weightSide * 0.08 * intensity))\n        rig.setRotation(relaxLegBone as VRMHumanBoneName, relaxRot)\n      }\n\n      if (rig.hasBone(weightKnee as VRMHumanBoneName)) {\n        // Straight-ish weight-bearing knee\n        rig.setRotation(weightKnee as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n      }\n\n      if (rig.hasBone(relaxKnee as VRMHumanBoneName)) {\n        // Bent relaxed knee\n        rig.setRotation(relaxKnee as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.2 * intensity))\n      }\n\n      // Feet\n      if (rig.hasBone(weightFoot as VRMHumanBoneName)) {\n        rig.setRotation(weightFoot as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightSide * 0.08))\n      }\n\n      if (rig.hasBone(relaxFoot as VRMHumanBoneName)) {\n        // Relaxed foot on ball/toe\n        const relaxFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.15 * intensity)\n        relaxFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -weightSide * 0.1))\n        rig.setRotation(relaxFoot as VRMHumanBoneName, relaxFootRot)\n      }\n\n      // Toes\n      const weightToes = isRightWeight ? 'rightToes' : 'leftToes'\n      const relaxToes = isRightWeight ? 'leftToes' : 'rightToes'\n\n      if (rig.hasBone(weightToes as VRMHumanBoneName)) {\n        rig.setRotation(weightToes as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.05))\n      }\n      if (rig.hasBone(relaxToes as VRMHumanBoneName)) {\n        rig.setRotation(relaxToes as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.1 * intensity))\n      }\n\n      // ========================================\n      // LAYER 5: ARMS (POSE VARIANT)\n      // ========================================\n\n      const armPose = getArmPose(poseVariant, intensity)\n\n      // Add subtle arm movement/fidget\n      const armFidget = noise.noise2D(t * 0.15, 300) * fidgetAmount * 0.02\n      state.armSpring.setTarget(armFidget)\n      state.armSpring.update(dt)\n      const armNoise = state.armSpring.value\n\n      if (rig.hasBone('leftUpperArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftUpperArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftUpperArm.y + armNoise))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftUpperArm.z))\n        rig.setRotation('leftUpperArm', rot)\n      }\n\n      if (rig.hasBone('leftLowerArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftLowerArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftLowerArm.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftLowerArm.z))\n        rig.setRotation('leftLowerArm', rot)\n      }\n\n      if (rig.hasBone('leftHand')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftHand.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftHand.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftHand.z))\n        rig.setRotation('leftHand', rot)\n      }\n\n      if (rig.hasBone('rightUpperArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightUpperArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightUpperArm.y - armNoise))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightUpperArm.z))\n        rig.setRotation('rightUpperArm', rot)\n      }\n\n      if (rig.hasBone('rightLowerArm')) {\n        // Add chin tap gesture for chin_rest variant\n        let chinTapAdd = 0\n        if (poseVariant === 'chin_rest' && state.currentGesture === 'chin_tap') {\n          chinTapAdd = Math.sin(state.thinkingGestureTimer * 6) * 0.05 * gestureWeight\n        }\n\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightLowerArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightLowerArm.y + chinTapAdd))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightLowerArm.z))\n        rig.setRotation('rightLowerArm', rot)\n      }\n\n      if (rig.hasBone('rightHand')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightHand.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightHand.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightHand.z))\n        rig.setRotation('rightHand', rot)\n      }\n\n      // ========================================\n      // LAYER 5B: DETAILED FINGER ANIMATION\n      // ========================================\n      // Animate all 30 finger bones (15 per hand) with pose-specific positioning\n      // Each finger has 3 joints: Proximal, Intermediate, Distal\n      // Bones: thumb/index/middle/ring/little + Proximal/Intermediate/Distal\n      //\n      // Apply base finger curl from pose, plus subtle micro-movements\n\n      // Add subtle finger fidget for organic feel\n      const fingerFidget = noise.noise2D(t * 0.25, 1100) * fidgetAmount * 0.1\n\n      // Left hand finger bones - explicit animation of all joints\n      const leftFingerCurl = {\n        thumb: armPose.leftFingers.thumb + fingerFidget * 0.3,\n        index: armPose.leftFingers.index + fingerFidget * 0.5,\n        middle: armPose.leftFingers.middle + fingerFidget * 0.6,\n        ring: armPose.leftFingers.ring + fingerFidget * 0.7,\n        little: armPose.leftFingers.little + fingerFidget * 0.8,\n      }\n\n      // Right hand finger bones - with thinking gesture adjustments\n      const thinkingFingerMod = state.currentGesture === 'chin_tap' ? gestureWeight * 0.1 : 0\n      const rightFingerCurl = {\n        thumb: armPose.rightFingers.thumb + thinkingFingerMod,\n        index: armPose.rightFingers.index + thinkingFingerMod * 1.5, // Index finger more active in chin tap\n        middle: armPose.rightFingers.middle + thinkingFingerMod * 0.8,\n        ring: armPose.rightFingers.ring + fingerFidget * 0.5,\n        little: armPose.rightFingers.little + fingerFidget * 0.6,\n      }\n\n      // Apply finger curls - this internally handles all 15 bones per hand\n      // with hasBone() checks for each: Proximal, Intermediate, Distal joints\n      applyFingerCurl(rig, 'left', leftFingerCurl)\n      applyFingerCurl(rig, 'right', rightFingerCurl)\n\n      // ========================================\n      // LAYER 6: HEAD & NECK (THINKING)\n      // ========================================\n\n      let headTiltX = 0.02 * intensity // Slight down\n      let headTiltY = 0\n      let headTiltZ = 0.03 * intensity * weightSide // Tilt toward weight side\n\n      // Add thinking gestures\n      if (state.currentGesture === 'head_tilt') {\n        headTiltZ += gestureWeight * 0.05 * (noise.noise2D(t, 400) > 0 ? 1 : -1)\n        headTiltX += gestureWeight * 0.03\n      } else if (state.currentGesture === 'look_away') {\n        headTiltY = gestureWeight * 0.1 * (noise.noise2D(t, 500) > 0 ? 1 : -1)\n      }\n\n      // Micro-movement\n      const headMicroX = noise.noise2D(t * 0.2, 600) * 0.01 * intensity\n      const headMicroY = noise.noise2D(t * 0.15, 700) * 0.015 * intensity\n\n      state.headSpring.setTarget(headTiltY + headMicroY)\n      state.headSpring.update(dt)\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX + headMicroX)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, state.headSpring.value))\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headTiltZ))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        const neckRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX * 0.5)\n        neckRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, state.headSpring.value * 0.4))\n        neckRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headTiltZ * 0.3))\n        rig.setRotation('neck', neckRot)\n      }\n\n      // ========================================\n      // LAYER 7: EYES (CONTEMPLATIVE GAZE)\n      // ========================================\n\n      if (eyeMovement) {\n        // Slow, distant gaze with occasional focus shifts\n        let eyeX = 0\n        let eyeY = 0.02 * intensity // Slight downward gaze\n\n        if (state.currentGesture === 'look_away') {\n          eyeX = gestureWeight * 0.1 * (noise.noise2D(t, 800) > 0 ? 1 : -1)\n        } else {\n          eyeX = noise.noise2D(t * 0.1, 900) * 0.04 * intensity\n          eyeY += noise.noise2D(t * 0.08, 1000) * 0.02 * intensity\n        }\n\n        // Slow blinking (contemplative)\n        const blinkChance = noise.noise2D(t * 0.2, 1100)\n        if (!state.isBlinking && blinkChance > 0.92) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          const blinkDuration = 0.2 // Slower, contemplative blink\n          if (state.blinkTimer > blinkDuration) {\n            state.isBlinking = false\n          } else {\n            blinkAmount = Math.sin(state.blinkTimer / blinkDuration * Math.PI) * 0.25\n          }\n        }\n\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 8: JAW (OCCASIONAL SUBTLE MOVEMENT)\n      // ========================================\n\n      if (rig.hasBone('jaw')) {\n        // Very subtle jaw movement as if thinking words\n        const jawThink = noise.noise2D(t * 0.3, 1200) > 0.85\n          ? noise.noise2D(t * 2, 1300) * 0.02 * thoughtIntensity\n          : 0\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawThink))\n      }\n    },\n  }\n}\n\nexport const contemplativeLean: MotionProgram<ContemplativeLeanParams> = createContemplativeLean({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Top-of-file documentation is present but not sufficiently detailed to meet the block rules. The Blocks philosophy requires that every motion file begin with detailed documentation describing HOW the motion should FEEL and specifying timing relationships between body parts. The header comment lists a few high-level topics but does not: (a) provide explicit timing relationships (e.g., phase offsets between hips/spine/head/shoulders), (b) document per-variant meanings or how variants differ in intent/timing beyond naming, or (c) state the intended breath period (5–6s) in human-readable form. This violates the 'every motion file MUST begin with detailed documentation' and the output constraints that documentation must describe contemplative body language and specify pose variants/meaning.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"The block philosophy requires numerical choices to be justified (every numeric choice must be documented/justified). This source contains many numeric constants (spring stiffness/damping, rotation magnitudes, noise scales, gesture durations, breathing frequency constant 0.12, finger curl amounts, etc.) with little or no inline justification. Several of these constants appear to be tuned values but lack comment justification or citations. This violates the requirement that every numerical choice be justified.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"Documentation does not include bibliographic citations or explicit references for the claimed 'Research basis'. The Block Definition and philosophy require citation of biomechanical or observational research basis. The header lists research topics but contains no citations, DOIs, papers, or observational references.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"The block output constraints require 'BREATH: Deep, slow (5-6 second cycles)'. The implementation uses oscBreathing(t, 0.12, breathDepth) but the code does not document or justify how '0.12' maps to a 5–6s cycle (or otherwise guarantee the 5–6s target). There is no explicit, human-readable timing parameter or comment that ties the numeric breathing setup to the stated 5–6s requirement. Please document/justify the breathing frequency constant or make the breath period an explicit parameter.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"Block philosophy expects motions to consider ALL 69 VRM bones or at least show explicit handling/acknowledgement of every VRM bone by name or via a documented fallback. This implementation touches many core bones (hips, spine, chest, upperChest, neck, head, shoulders, arms, hands, legs, knees, feet, toes, jaw, eyes, and calls applyFingerCurl) but does not enumerate or attempt to handle every VRM bone (e.g., clavicles, clavicle variants, some finger joint names, detailed face bones, etc.). If the project-level applyFingerCurl covers fingers and other helpers cover remaining bones, please state that in the header documentation. As-is, it's unclear whether all 69 bones are considered.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"The block outputs require 'TRANSITIONS: Smooth between gesture variants'. The implementation reads poseVariant from validatedParams captured at creation time and applies the variant pose directly in the update loop. There is no explicit transition/blend logic between different poseVariant values if they are changed at runtime (no crossfade/spring/blend of armPose when variant changes). If runtime switching is expected, the code should implement blending so transitions are smooth.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"There is an unused import (BoneChains) at the top of the file. This is a minor cleanliness issue and may indicate a planned feature or omitted logic that would use chain-level coordination.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    }\n  ],\n  \"summary\": \"The motion program implements a thorough contemplative pose with guarded bone access, multiple variants, breathing, gesture state machine, and finger handling. However it fails the Blocks documentation and philosophy requirements: the top-of-file documentation is not sufficiently detailed (missing explicit timing relationships, per-variant meanings, and research citations), the many numeric parameters lack justifications, and the breathing timing is not explicitly tied to the required 5–6s cycle. Additional warnings: the implementation does not clearly demonstrate handling of all 69 VRM bones, lacks explicit smooth transitions between pose variants at runtime, and has an unused import. Fixing the documentation (add timing relationships, per-constant justifications, citations), exposing or documenting explicit breath period, and adding variant-blending or a note about why runtime switches are unsupported will address the major errors.\"\n}",
            "tokensUsed": {
              "input": 8130,
              "output": 3495
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    }
  ]
}