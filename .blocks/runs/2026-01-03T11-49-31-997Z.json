{
  "version": "1.0",
  "id": "2026-01-03T11-49-31-997Z",
  "timestamp": "2026-01-03T11:54:31.951Z",
  "configPath": "blocks.yml",
  "projectName": "Posers Motion Engine",
  "duration": 300081,
  "summary": {
    "totalBlocks": 5,
    "passed": 0,
    "failed": 4,
    "warnings": 1,
    "cached": {
      "validatorsSkipped": 0,
      "validatorsRun": 10,
      "timeSavedMs": 0
    }
  },
  "blocks": [
    {
      "blockName": "confident_stance",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/confident-stance.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "confident_stance",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must have detailed JSDoc describing confident body language",
                    "CORE: Spine chain must show upright confident posture",
                    "CORE: Hips stable with micro weight shifts",
                    "BREATH: Deep, slow breathing (4-5 second cycle)",
                    "BREATH: Chest expands on inhale, shoulders rise slightly",
                    "ARMS: Relaxed at sides or hands loosely clasped",
                    "ARMS: Subtle forearm rotation with breath",
                    "HANDS: Fingers relaxed but with presence, not limp",
                    "HANDS: All 30 finger bones should have subtle curl",
                    "LEGS: Weight distributed evenly or slightly forward",
                    "LEGS: Knees not locked, micro-flex",
                    "FEET: Toes engaged, gripping ground subtly",
                    "HEAD: Chin slightly elevated, gaze forward",
                    "EYES: Purposeful movement, slow blinks",
                    "JAW: Relaxed but closed",
                    "MICRO: Noise layer on all bones for life"
                  ]
                }
              ],
              "description": "Power pose with commanding presence. Weight evenly distributed or slightly forward.\nChest open, shoulders back but relaxed. Subtle assertive micro-movements.\n\nFEEL: Grounded, assured, ready. Like a CEO about to address the board.\n\nTIMING: Breath drives subtle chest expansion (4-5 second cycle). Weight micro-shifts\nevery 2-3 seconds. Eyes move with purpose, not darting. Fingers relaxed but not limp.\n",
              "path": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Power pose with commanding presence. Weight evenly...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 74946,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "MICRO requirement not satisfied: the implementation does not explicitly apply a continuous micro-noise layer to all VRM bones. The block philosophy and output constraint require a micro-noise layer on all bones (all 69 VRM bones). The code applies micro-noise to many core bones and limbs but does not explicitly address every finger bone, toe bones beyond top-level toes, eyes/jaw micro-noise, or a full traversal of the 69 VRM bones.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "FINGER/CURL requirement not guaranteed: the output constraint requires that 'All 30 finger bones should have subtle curl'. This file calls applyFingerCurl(...) only when leftHand/rightHand exist, but the file does not explicitly enumerate or guard all 30 finger bone names itself. Without inspecting applyFingerCurl() we cannot confirm all 30 finger bones receive subtle curl or micro-noise. The block should either explicitly apply per-finger rotations (or document and assert that applyFingerCurl covers all 30 finger bones and uses hasBone checks internally).",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "WEIGHT SHIFT TIMING mismatch: the top-level documentation and Block Definition state 'WEIGHT MICRO-SHIFTS: Every 2-3 seconds', but the implementation drives weight shifts with noise.noise2D(t * 0.15, ...) — 0.15 Hz corresponds to ~6.7 second period, not 2-3 seconds. This is a timing discrepancy between the documented intent and the code.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "WEIGHT DISTRIBUTION direction: the Block Definition specifies 'Weight evenly distributed or slightly forward', but the implementation's weightBias and left/right weight logic only implements lateral (left/right) bias and leg loading. There is no forward/backward center-of-mass bias or forward lean implementation to represent 'slightly forward' weight distribution.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "BREATH-to-HEAD coupling: the Blocks philosophy emphasizes that breath phase couples to chest, shoulders, and subtle head motion. The implementation couples breath to chest, shoulders and forearms but does not introduce any explicit head translation/rotation driven by breath (only separate micro-noise). Consider adding a subtle breath-coupled head/neck motion (very small amplitude, phase-related) to match the stated biomechanical coupling.",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "hasBone coverage for per-finger bones is unknown: the code wraps calls to applyFingerCurl/applyFingerSpread with hasBone('leftHand') / hasBone('rightHand'), which is good. However the Block Philosophy requires that hasBone() checks wrap all optional bone access. Because per-finger bones are not referenced explicitly here, ensure that applyFingerCurl and applyFingerSpread themselves perform per-bone hasBone checks or the motion will assume bones that may be missing. This is a potential portability issue (warning).",
              "file": "packages/motion-dsl/src/motions/confident-stance.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "confident-stance.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The motion program is well-documented, uses layered breathing/weight/micro-noise logic, and shows clear intent for a confident stance (spine/chest extension, shoulder retraction, relaxed arms, eyes/blink system). However, it fails several explicit domain constraints: micro-noise is not guaranteed on all 69 VRM bones, the 'all 30 finger bones must have subtle curl' constraint is not verifiably satisfied here, and the implemented weight-shift frequency does not match the documented 2–3s timing. Several warnings call out missing forward weight bias and absent breath-coupled head motion. Addressing these issues (explicit per-finger handling or asserting helper behavior, aligning weight-shift frequency with documentation, and ensuring a micro-noise pass over all bones) will bring the block into compliance."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: confident_stance\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Power pose with commanding presence. Weight evenly distributed or slightly forward.\\nChest open, shoulders back but relaxed. Subtle assertive micro-movements.\\n\\nFEEL: Grounded, assured, ready. Like a CEO about to address the board.\\n\\nTIMING: Breath drives subtle chest expansion (4-5 second cycle). Weight micro-shifts\\nevery 2-3 seconds. Eyes move with purpose, not darting. Fingers relaxed but not limp.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/confident-stance.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must have detailed JSDoc describing confident body language\",\n        \"CORE: Spine chain must show upright confident posture\",\n        \"CORE: Hips stable with micro weight shifts\",\n        \"BREATH: Deep, slow breathing (4-5 second cycle)\",\n        \"BREATH: Chest expands on inhale, shoulders rise slightly\",\n        \"ARMS: Relaxed at sides or hands loosely clasped\",\n        \"ARMS: Subtle forearm rotation with breath\",\n        \"HANDS: Fingers relaxed but with presence, not limp\",\n        \"HANDS: All 30 finger bones should have subtle curl\",\n        \"LEGS: Weight distributed evenly or slightly forward\",\n        \"LEGS: Knees not locked, micro-flex\",\n        \"FEET: Toes engaged, gripping ground subtly\",\n        \"HEAD: Chin slightly elevated, gaze forward\",\n        \"EYES: Purposeful movement, slow blinks\",\n        \"JAW: Relaxed but closed\",\n        \"MICRO: Noise layer on all bones for life\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- confident-stance.ts ---\n```\n/**\n * ============================================================================\n * CONFIDENT STANCE MOTION\n * ============================================================================\n *\n * A power pose with commanding presence, subtle organic movements, and\n * deep, slow breathing. Projects authority and self-assurance.\n *\n * ============================================================================\n * HOW THIS MOTION SHOULD FEEL\n * ============================================================================\n *\n * Imagine a CEO standing before a board meeting, or a military officer at\n * ease. The body is grounded, expansive, and utterly still except for the\n * subtle signs of life: breathing, micro-adjustments, and the occasional\n * slow blink. There's no fidgeting, no uncertainty - every micro-movement\n * is deliberate and controlled.\n *\n * The weight is planted firmly, slightly favoring one leg. The chest is\n * open and lifted. The shoulders are pulled back but not tense. The chin\n * is slightly elevated. The hands hang naturally at the sides with fingers\n * loosely curled - not fists, not splayed, just relaxed confidence.\n *\n * ============================================================================\n * TIMING RELATIONSHIPS\n * ============================================================================\n *\n * BREATH CYCLE: 4-5 seconds (0.20-0.25 Hz)\n *   - Primary driver of all torso movement\n *   - Chest leads (peaks first), shoulders follow by ~0.1s\n *   - Forearms rotate subtly in sync with shoulder elevation\n *   - Inhale: 2-2.5s (chest expands, shoulders rise ~3mm)\n *   - Exhale: 2-2.5s (chest settles, shoulders drop)\n *\n * WEIGHT MICRO-SHIFTS: Every 2-3 seconds\n *   - Spring-driven transitions (stiffness 50, damping 15)\n *   - Hips shift laterally ~5-10mm\n *   - Spine counter-rotates to maintain head position\n *   - Settlement time: ~0.5s to reach new position\n *\n * EYE MOVEMENT: Slow, deliberate\n *   - Saccade frequency: 0.15 Hz (every 6-7 seconds)\n *   - Movement range: ±2.5° horizontal, ±2° vertical\n *   - Blink interval: 3-5 seconds (irregular, seeded)\n *   - Blink duration: 150ms (quick close ~45ms, slow open ~105ms)\n *\n * MICRO-NOISE: Continuous, imperceptible\n *   - All bones receive 0.17° of noise at 0.2-0.3 Hz\n *   - Prevents the \"mannequin stillness\" effect\n *   - Head has slightly more (0.2°) for subtle attention shifts\n *\n * ============================================================================\n * BONE HANDLING\n * ============================================================================\n *\n * REQUIRED BONES (must exist):\n *   hips, spine, chest, neck, head\n *\n * OPTIONAL BONES (gracefully handled if missing):\n *   upperChest, leftShoulder, rightShoulder,\n *   leftUpperArm, rightUpperArm, leftLowerArm, rightLowerArm,\n *   leftHand, rightHand, all 30 finger bones,\n *   leftUpperLeg, rightUpperLeg, leftLowerLeg, rightLowerLeg,\n *   leftFoot, rightFoot, leftToes, rightToes,\n *   leftEye, rightEye, jaw\n *\n * All optional bone access is wrapped in hasBone() checks.\n *\n * ============================================================================\n * RESEARCH BASIS\n * ============================================================================\n *\n * - Carney, Cuddy, & Yap (2010): \"Power posing\" - expansive postures affect\n *   hormone levels and feelings of power. Open chest, raised chin, planted\n *   stance all signal dominance.\n *\n * - Winter (1995): \"Biomechanics and Motor Control of Human Movement\" -\n *   Standing postural sway occurs at 0.1-0.3 Hz with ~2-5mm amplitude.\n *\n * - Collins & De Luca (1993): \"Open-loop and closed-loop control of posture\" -\n *   Even quiet standing involves continuous micro-corrections.\n *\n * - Breathing rate of 12-15 breaths/min is normal; confident/relaxed state\n *   drops to 10-12 breaths/min (4-5 second cycles).\n *\n * ============================================================================\n * NUMERICAL JUSTIFICATIONS\n * ============================================================================\n *\n * breathRate: 0.22 Hz = 4.5s cycle (confident, slow breathing)\n * hipsTilt: -0.03 rad = -1.7° (slight posterior pelvic tilt for lordosis)\n * spineExtension: 0.02 rad = 1.1° (subtle chest lift per segment)\n * shoulderPullBack: 0.08 rad = 4.6° (visible but not exaggerated)\n * armAbduction: 0.12 rad = 6.9° (arms away from body, not stiff)\n * microIntensity: 0.003 rad = 0.17° (imperceptible micro-noise)\n * springStiffness: 50 (moderate response, not snappy)\n * springDamping: 15 (prevents oscillation, smooth settling)\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  applyFingerCurl,\n  applyFingerSpread,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const confidentStanceParamsSchema = z.object({\n  /** Overall intensity of the pose (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Breath rate in Hz. Default: 0.22 (4.5s cycle for confident breathing) */\n  breathRate: z.number().min(0.05).max(0.5).default(0.22),\n  /** Amount of hip sway side-to-side (0-1). Default: 0.3 */\n  swayAmount: z.number().min(0).max(1).default(0.3),\n  /** Enable eye micro-movements. Default: true */\n  eyeMovement: z.boolean().default(true),\n  /** Enable finger movements. Default: true */\n  fingerMovement: z.boolean().default(true),\n  /** Weight distribution bias (-1=left, 0=center, 1=right). Default: 0.15 */\n  weightBias: z.number().min(-1).max(1).default(0.15),\n  /** Shoulder tension level (0=relaxed, 1=tense). Default: 0.2 */\n  shoulderTension: z.number().min(0).max(1).default(0.2),\n  /** Chest out amount (0-1). Default: 0.6 */\n  chestOut: z.number().min(0).max(1).default(0.6),\n  /** Chin up amount (0-1). Default: 0.3 */\n  chinUp: z.number().min(0).max(1).default(0.3),\n})\n\nexport type ConfidentStanceParams = z.infer<typeof confidentStanceParamsSchema>\nexport type ConfidentStanceInput = z.input<typeof confidentStanceParamsSchema>\n\nexport const confidentStanceMeta: MotionMeta = {\n  id: 'confident-stance',\n  name: 'Confident Stance',\n  description: 'Power pose with commanding presence, subtle breathing, and organic micro-movements',\n  tags: ['stance', 'confident', 'power', 'idle'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface ConfidentStanceState {\n  noise: NoiseGenerator\n  weightShiftSpring: Spring\n  blinkTimer: number\n  blinkDuration: number\n  isBlinking: boolean\n  lastBlinkTime: number\n}\n\n/**\n * Initialize deterministic state with seeded random generators.\n * All noise and random elements are seeded to ensure identical\n * output for the same seed value - required for deterministic playback.\n */\nfunction initState(seed: number): ConfidentStanceState {\n  return {\n    noise: createNoiseGenerator(seed),\n    // Spring tuned for slow, deliberate weight shifts\n    // stiffness 50: moderate responsiveness (not snappy)\n    // damping 15: prevents oscillation, smooth 0.5s settling\n    weightShiftSpring: createSpring({ stiffness: 50, damping: 15 }),\n    blinkTimer: 0,\n    blinkDuration: 0.15, // 150ms total blink duration\n    isBlinking: false,\n    lastBlinkTime: 0,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\nexport function createConfidentStance(params: ConfidentStanceInput = {}): MotionProgram<ConfidentStanceParams> {\n  const validatedParams = confidentStanceParamsSchema.parse(params)\n  let state: ConfidentStanceState | null = null\n\n  return {\n    meta: confidentStanceMeta,\n    paramsSchema: confidentStanceParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      // Lazy initialization if init wasn't called\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        intensity,\n        breathRate,\n        swayAmount,\n        eyeMovement,\n        fingerMovement,\n        weightBias,\n        shoulderTension,\n        chestOut,\n        chinUp,\n      } = validatedParams\n\n      const noise = state.noise\n\n      // ========================================\n      // LAYER 1: BASE POSTURE\n      // ========================================\n\n      // Hips - slight posterior tilt for confident lordotic curve\n      // -0.03 rad = -1.7° creates natural lumbar lordosis\n      const hipsTilt = -0.03 * intensity\n      const hipsYaw = weightBias * 0.05 * intensity\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hipsTilt)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, hipsYaw))\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // Spine chain - tall, proud posture\n      // Each segment extends ~1.1° for cumulative chest-out effect\n      const spineExtension = 0.02 * intensity * chestOut\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension * 0.5))\n      }\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -spineExtension * 1.2))\n      }\n\n      // Neck and head - chin slightly up\n      // Creates commanding upward gaze angle\n      const neckExtension = chinUp * 0.03 * intensity\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -neckExtension * 0.5))\n      }\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -neckExtension)\n        rig.setRotation('head', headRot)\n      }\n\n      // Jaw - relaxed but closed (neutral position)\n      // Slight downward rotation keeps mouth naturally closed\n      if (rig.hasBone('jaw')) {\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n      }\n\n      // Shoulders - pulled back for confident posture\n      // 0.08 rad = 4.6° of retraction (visible but not exaggerated)\n      const shoulderPullBack = 0.08 * intensity * (1 - shoulderTension * 0.5)\n      const shoulderDown = shoulderTension * 0.05 * intensity\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderDown)\n        leftShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -shoulderPullBack))\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderDown)\n        rightShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, shoulderPullBack))\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // Arms - relaxed at sides with slight separation\n      // 0.12 rad = 6.9° abduction keeps arms away from body naturally\n      const armAbduction = 0.12 * intensity\n      const armRelax = 0.08 * intensity\n      if (rig.hasBone('leftUpperArm')) {\n        const leftUpperArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armAbduction)\n        leftUpperArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armRelax))\n        rig.setRotation('leftUpperArm', leftUpperArmRot)\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const rightUpperArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -armAbduction)\n        rightUpperArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armRelax))\n        rig.setRotation('rightUpperArm', rightUpperArmRot)\n      }\n\n      // Lower arms - slightly bent for natural relaxation\n      // 0.15 rad = 8.6° elbow flexion (not straight, not obviously bent)\n      const elbowBend = 0.15 * intensity\n      if (rig.hasBone('leftLowerArm')) {\n        rig.setRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -elbowBend))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        rig.setRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, elbowBend))\n      }\n\n      // Hands - natural rotation at wrist\n      const handRotation = 0.1 * intensity\n      if (rig.hasBone('leftHand')) {\n        rig.setRotation('leftHand', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handRotation))\n      }\n      if (rig.hasBone('rightHand')) {\n        rig.setRotation('rightHand', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handRotation))\n      }\n\n      // Legs - weight distribution\n      // FIXED: Proper weight bias calculation for continuous -1 to 1 range\n      // weightBias < 0: more weight on left leg\n      // weightBias > 0: more weight on right leg\n      // weightBias = 0: equal weight\n      const leftWeight = 0.5 - weightBias * 0.5  // 1.0 at bias=-1, 0.5 at bias=0, 0.0 at bias=1\n      const rightWeight = 0.5 + weightBias * 0.5 // 0.0 at bias=-1, 0.5 at bias=0, 1.0 at bias=1\n\n      const standingLegBend = 0.05 * intensity\n      const relaxedLegBend = 0.12 * intensity\n\n      if (rig.hasBone('leftUpperLeg')) {\n        // Weighted leg has less bend, relaxed leg has more\n        const leftBend = standingLegBend * leftWeight + relaxedLegBend * (1 - leftWeight)\n        rig.setRotation('leftUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftBend))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        const rightBend = standingLegBend * rightWeight + relaxedLegBend * (1 - rightWeight)\n        rig.setRotation('rightUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightBend))\n      }\n\n      // Knee slight bend for natural stance\n      // 0.08 rad = 4.6° flexion on weighted leg\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08 * leftWeight))\n      }\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08 * rightWeight))\n      }\n\n      // Feet - flat with slight outward rotation (natural turnout)\n      // 0.1 rad = 5.7° external rotation each foot\n      const footOutward = 0.1 * intensity\n      if (rig.hasBone('leftFoot')) {\n        rig.setRotation('leftFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -footOutward))\n      }\n      if (rig.hasBone('rightFoot')) {\n        rig.setRotation('rightFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, footOutward))\n      }\n\n      // ========================================\n      // LAYER 2: BREATHING (4-5 second cycle)\n      // ========================================\n\n      const breathPhase = oscBreathing(t, breathRate, intensity)\n\n      // Chest rises and falls with breath\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.015))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.02))\n      }\n\n      // Shoulder rise with breath (~3mm at peak)\n      // Phase delayed ~0.1s behind chest\n      const shoulderBreathPhase = oscBreathing(t - 0.1, breathRate, intensity)\n      const shoulderBreath = shoulderBreathPhase * 0.008\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderBreath))\n      }\n\n      // Forearm rotation coupled to breath\n      // Subtle pronation/supination as shoulders rise\n      const forearmBreath = shoulderBreath * 0.5\n      if (rig.hasBone('leftLowerArm')) {\n        rig.addRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, forearmBreath))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        rig.addRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -forearmBreath))\n      }\n\n      // ========================================\n      // LAYER 3: WEIGHT SHIFT (every 2-3 seconds)\n      // ========================================\n\n      // Slow, subtle weight shifting driven by noise\n      const weightShiftTarget = noise.noise2D(t * 0.15, 0) * swayAmount * 0.5\n      state.weightShiftSpring.setTarget(weightShiftTarget)\n      state.weightShiftSpring.update(dt)\n      const currentShift = state.weightShiftSpring.value\n\n      // Apply weight shift to hips\n      if (rig.hasBone('hips')) {\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, currentShift * 0.03))\n      }\n\n      // Counter-rotate spine to keep head stable\n      if (rig.hasBone('spine')) {\n        rig.addRotation('spine', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -currentShift * 0.015))\n      }\n\n      // ========================================\n      // LAYER 4: MICRO-MOVEMENTS (all bones)\n      // ========================================\n\n      // 0.003 rad = 0.17° of noise prevents mannequin stillness\n      const microIntensity = 0.003 * intensity\n\n      // Head - slightly more micro-movement for subtle attention shifts\n      if (rig.hasBone('head')) {\n        const headNoiseX = noise.noise2D(t * 0.3, 100) * microIntensity * 1.2\n        const headNoiseY = noise.noise2D(t * 0.25, 200) * microIntensity * 1.2\n        const headNoiseZ = noise.noise2D(t * 0.2, 300) * microIntensity * 0.6\n        rig.addRotation('head', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headNoiseX))\n        rig.addRotation('head', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headNoiseY))\n        rig.addRotation('head', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headNoiseZ))\n      }\n\n      // Neck micro-noise\n      if (rig.hasBone('neck')) {\n        const neckNoiseX = noise.noise2D(t * 0.25, 110) * microIntensity * 0.5\n        const neckNoiseY = noise.noise2D(t * 0.2, 210) * microIntensity * 0.5\n        rig.addRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, neckNoiseX))\n        rig.addRotation('neck', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, neckNoiseY))\n      }\n\n      // Spine segments micro-noise\n      if (rig.hasBone('spine')) {\n        const spineNoise = noise.noise2D(t * 0.15, 120) * microIntensity * 0.3\n        rig.addRotation('spine', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineNoise))\n      }\n      if (rig.hasBone('chest')) {\n        const chestNoise = noise.noise2D(t * 0.18, 130) * microIntensity * 0.3\n        rig.addRotation('chest', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, chestNoise))\n      }\n      if (rig.hasBone('upperChest')) {\n        const upperChestNoise = noise.noise2D(t * 0.2, 140) * microIntensity * 0.3\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, upperChestNoise))\n      }\n\n      // Shoulder micro-noise\n      if (rig.hasBone('leftShoulder')) {\n        const lShoulderNoise = noise.noise2D(t * 0.2, 150) * microIntensity * 0.4\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, lShoulderNoise))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        const rShoulderNoise = noise.noise2D(t * 0.2, 160) * microIntensity * 0.4\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, rShoulderNoise))\n      }\n\n      // Upper arm micro-noise\n      if (rig.hasBone('leftUpperArm')) {\n        const armNoiseL = noise.noise2D(t * 0.2, 400) * microIntensity\n        rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armNoiseL))\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const armNoiseR = noise.noise2D(t * 0.2, 500) * microIntensity\n        rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armNoiseR))\n      }\n\n      // Lower arm micro-noise\n      if (rig.hasBone('leftLowerArm')) {\n        const lowerArmNoiseL = noise.noise2D(t * 0.22, 410) * microIntensity * 0.5\n        rig.addRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, lowerArmNoiseL))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        const lowerArmNoiseR = noise.noise2D(t * 0.22, 510) * microIntensity * 0.5\n        rig.addRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, lowerArmNoiseR))\n      }\n\n      // Hand micro-noise\n      if (rig.hasBone('leftHand')) {\n        const handNoiseL = noise.noise2D(t * 0.25, 420) * microIntensity * 0.4\n        rig.addRotation('leftHand', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, handNoiseL))\n      }\n      if (rig.hasBone('rightHand')) {\n        const handNoiseR = noise.noise2D(t * 0.25, 520) * microIntensity * 0.4\n        rig.addRotation('rightHand', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, handNoiseR))\n      }\n\n      // Hip micro-noise\n      if (rig.hasBone('hips')) {\n        const hipsNoise = noise.noise2D(t * 0.1, 170) * microIntensity * 0.3\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, hipsNoise))\n      }\n\n      // Leg micro-noise (very subtle)\n      if (rig.hasBone('leftUpperLeg')) {\n        const legNoiseL = noise.noise2D(t * 0.12, 180) * microIntensity * 0.2\n        rig.addRotation('leftUpperLeg', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, legNoiseL))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        const legNoiseR = noise.noise2D(t * 0.12, 190) * microIntensity * 0.2\n        rig.addRotation('rightUpperLeg', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, legNoiseR))\n      }\n\n      // ========================================\n      // LAYER 5: FINGERS (all 30 bones)\n      // ========================================\n\n      if (fingerMovement) {\n        // Relaxed, slightly curled fingers\n        // Base curl ~15° with noise variation\n        const baseCurl = 0.25 + noise.noise2D(t * 0.1, 600) * 0.1\n\n        // Left hand - apply curl with per-finger variation\n        // Pinky curls most, index least - natural hand relaxation\n        if (rig.hasBone('leftHand')) {\n          applyFingerCurl(rig, 'left', {\n            thumb: baseCurl * 0.6,   // Thumb curls less\n            index: baseCurl,         // Index baseline\n            middle: baseCurl * 1.1,  // Middle slightly more\n            ring: baseCurl * 1.15,   // Ring more still\n            little: baseCurl * 1.2,  // Pinky most curled\n          })\n        }\n\n        // Right hand - same pattern\n        if (rig.hasBone('rightHand')) {\n          applyFingerCurl(rig, 'right', {\n            thumb: baseCurl * 0.6,\n            index: baseCurl,\n            middle: baseCurl * 1.1,\n            ring: baseCurl * 1.15,\n            little: baseCurl * 1.2,\n          })\n        }\n\n        // Slight finger spread for natural hand appearance\n        // 0.2 = ~12° between fingers\n        if (rig.hasBone('leftHand')) {\n          applyFingerSpread(rig, 'left', 0.2)\n        }\n        if (rig.hasBone('rightHand')) {\n          applyFingerSpread(rig, 'right', 0.2)\n        }\n      }\n\n      // ========================================\n      // LAYER 6: EYES (deliberate, slow movement)\n      // ========================================\n\n      if (eyeMovement) {\n        // Slow, deliberate eye movement (0.15 Hz = every 6-7 seconds)\n        const eyeSpeed = 0.15\n        const eyeX = noise.noise2D(t * eyeSpeed, 700) * 0.04 * intensity\n        const eyeY = noise.noise2D(t * eyeSpeed, 800) * 0.03 * intensity\n\n        // Blinking - deterministic intervals based on seeded noise\n        // Interval: 3-5 seconds (2 base + 3 noise range / 2 + offset)\n        const timeSinceLastBlink = t - state.lastBlinkTime\n        const blinkIntervalBase = 4 // 4 second base interval\n        const blinkIntervalVariation = noise.noise2D(state.lastBlinkTime, 900) // -1 to 1\n        const blinkInterval = blinkIntervalBase + blinkIntervalVariation // 3-5 second range\n\n        if (timeSinceLastBlink > blinkInterval && !state.isBlinking) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          const blinkProgress = state.blinkTimer / state.blinkDuration\n          if (blinkProgress >= 1) {\n            state.isBlinking = false\n            state.lastBlinkTime = t\n          } else {\n            // Quick close (30% of duration = ~45ms), slower open (70% = ~105ms)\n            blinkAmount = blinkProgress < 0.3\n              ? Easing.easeOutQuad(blinkProgress / 0.3)\n              : Easing.easeInQuad(1 - (blinkProgress - 0.3) / 0.7)\n          }\n        }\n\n        // Apply eye rotations\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 7: TOES (grip for balance)\n      // ========================================\n\n      // Toes curl slightly on weighted leg for balance grip\n      // 0.1 rad = 5.7° flexion\n      if (rig.hasBone('leftToes')) {\n        rig.setRotation('leftToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1 * leftWeight))\n      }\n      if (rig.hasBone('rightToes')) {\n        rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1 * rightWeight))\n      }\n    },\n  }\n}\n\nexport const confidentStance: MotionProgram<ConfidentStanceParams> = createConfidentStance({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"MICRO requirement not satisfied: the implementation does not explicitly apply a continuous micro-noise layer to all VRM bones. The block philosophy and output constraint require a micro-noise layer on all bones (all 69 VRM bones). The code applies micro-noise to many core bones and limbs but does not explicitly address every finger bone, toe bones beyond top-level toes, eyes/jaw micro-noise, or a full traversal of the 69 VRM bones.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"FINGER/CURL requirement not guaranteed: the output constraint requires that 'All 30 finger bones should have subtle curl'. This file calls applyFingerCurl(...) only when leftHand/rightHand exist, but the file does not explicitly enumerate or guard all 30 finger bone names itself. Without inspecting applyFingerCurl() we cannot confirm all 30 finger bones receive subtle curl or micro-noise. The block should either explicitly apply per-finger rotations (or document and assert that applyFingerCurl covers all 30 finger bones and uses hasBone checks internally).\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"WEIGHT SHIFT TIMING mismatch: the top-level documentation and Block Definition state 'WEIGHT MICRO-SHIFTS: Every 2-3 seconds', but the implementation drives weight shifts with noise.noise2D(t * 0.15, ...) — 0.15 Hz corresponds to ~6.7 second period, not 2-3 seconds. This is a timing discrepancy between the documented intent and the code.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"WEIGHT DISTRIBUTION direction: the Block Definition specifies 'Weight evenly distributed or slightly forward', but the implementation's weightBias and left/right weight logic only implements lateral (left/right) bias and leg loading. There is no forward/backward center-of-mass bias or forward lean implementation to represent 'slightly forward' weight distribution.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"BREATH-to-HEAD coupling: the Blocks philosophy emphasizes that breath phase couples to chest, shoulders, and subtle head motion. The implementation couples breath to chest, shoulders and forearms but does not introduce any explicit head translation/rotation driven by breath (only separate micro-noise). Consider adding a subtle breath-coupled head/neck motion (very small amplitude, phase-related) to match the stated biomechanical coupling.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    },\n    {\n      \"message\": \"hasBone coverage for per-finger bones is unknown: the code wraps calls to applyFingerCurl/applyFingerSpread with hasBone('leftHand') / hasBone('rightHand'), which is good. However the Block Philosophy requires that hasBone() checks wrap all optional bone access. Because per-finger bones are not referenced explicitly here, ensure that applyFingerCurl and applyFingerSpread themselves perform per-bone hasBone checks or the motion will assume bones that may be missing. This is a potential portability issue (warning).\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/confident-stance.ts\"\n    }\n  ],\n  \"summary\": \"The motion program is well-documented, uses layered breathing/weight/micro-noise logic, and shows clear intent for a confident stance (spine/chest extension, shoulder retraction, relaxed arms, eyes/blink system). However, it fails several explicit domain constraints: micro-noise is not guaranteed on all 69 VRM bones, the 'all 30 finger bones must have subtle curl' constraint is not verifiably satisfied here, and the implemented weight-shift frequency does not match the documented 2–3s timing. Several warnings call out missing forward weight bias and absent breath-coupled head motion. Addressing these issues (explicit per-finger handling or asserting helper behavior, aligning weight-shift frequency with documentation, and ensuring a micro-noise pass over all bones) will bring the block into compliance.\"\n}",
            "tokensUsed": {
              "input": 8521,
              "output": 4155
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "nervous_fidget",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/nervous-fidget.ts",
      "hasErrors": false,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "nervous_fidget",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe anxiety body language research basis",
                    "CORE: Hunched protective posture, shoulders elevated",
                    "CORE: Forward head posture (hypervigilance)",
                    "BREATH: Shallow, fast, irregular (2-3 second cycles)",
                    "BREATH: Shoulders rise with anxious breath",
                    "WEIGHT: IRREGULAR shifts using turbulence, not sine waves",
                    "WEIGHT: Ready-to-flee stance",
                    "ARMS: Held close to body (self-protection)",
                    "ARMS: Frequent self-touch gestures (neck, arms, face)",
                    "HANDS: Tense, curled fingers",
                    "HANDS: Rubbing, picking, clasping gestures",
                    "HANDS: All finger joints animated during fidgets",
                    "LEGS: Weight shifting, foot tapping bursts",
                    "FEET: Toes tapping or curled with tension",
                    "HEAD: Quick darting looks, then freeze, then dart",
                    "EYES: Rapid saccades, wide, frequent blinks",
                    "JAW: Clenched with occasional release",
                    "TIMING: Fidget BURSTS not continuous",
                    "TIMING: Quiet moments between fidget clusters"
                  ]
                }
              ],
              "description": "Anxiety-driven fidgeting with involuntary self-soothing behaviors.\nWeight shifts irregularly. Hands seek comfort (touching face, arms, clothing).\nShallow rapid breathing. Hypervigilant eye movement.\n\nFEEL: Uncomfortable, restless, seeking escape. Like waiting for bad news.\n\nTIMING: Breath is faster (2-3 second shallow cycles). Weight shifts are IRREGULAR\n(not sine waves - use turbulence). Fidget gestures happen in bursts with quiet\nperiods. Eyes dart quickly, then lock, then dart again.\n",
              "path": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Anxiety-driven fidgeting with involuntary self-soo...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": true,
          "duration": 58077,
          "issues": [
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Hand rubbing (hand_rub) phase applies a rubbing rotation only to leftHand (rubPhase) while symmetric arm rotations are applied to both upper arms. This makes the rubbing gesture asymmetric regardless of selected fidgetHand and likely omits the intended rubbing motion for the other hand.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The fidget logic supports neck_touch and arm_touch but no explicit 'face' touch / clothing touch implementation is present, yet the output constraints and documentation expect self-touching that can include the face. Consider adding an explicit face-touch variant or documenting that hand_rub represents face/contact behavior.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The code assumes applyFingerCurl will animate all finger joints (requirement: 'All finger joints animated during fidgets') but this file does not verify or document that applyFingerCurl animates all 30 finger joints. Either assert/document the helper's behavior or explicitly animate additional finger joints here.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "There are inconsistent uses of the noise API: some noise outputs are remapped to [-1,1] with '*2-1' while others are used raw (e.g. fidgetInterval uses noise.noise2D(...) * 2 without remapping). This inconsistency can change ranges unpredictably depending on the noise implementation and should be standardized or documented.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Minor comment/code mismatch in the foot tapping section: comments around phase/rate math are confusing and contain incorrect intermediate reasoning (mentions 6×π and 'Let's fix' while code settles on state.footTapPhase += dt * 3). Clean up comments to match implemented 3 Hz behavior.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Unused imports detected: 'osc' and 'Easing' are imported from '@posers/core' but not used in this file. Remove or use them to avoid dead code.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The file documents and guards against missing bones (hasBone checks) and touches many VRM bones, but it does not explicitly reference every VRM bone (the project rule states 'Every motion must consider ALL 69 VRM bones'). While the implementation degrades gracefully (good), if the requirement is to explicitly consider all bones you should either enumerate handling for the remaining optional bones (e.g., leftFoot/leftToes and other minor facial bones) or add an explicit statement in the documentation that unmentioned bones are intentionally left passive and will be skipped with hasBone guards.",
              "file": "packages/motion-dsl/src/motions/nervous-fidget.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "nervous-fidget.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The motion file is well-documented, aligns closely with the block's domain intent (anxious posture, irregular breath, turbulent weight shifts, darting head/eyes, finger curling, and foot tapping), uses rig/ctx inputs through the MotionProgram interface correctly, and applies hasBone() guards to avoid errors on missing bones. No blocking errors were found, but several warnings highlight areas for improvement: asymmetric hand rubbing, missing explicit face-touch gesture, reliance on an external helper for animating all finger joints without verification, inconsistent noise-range handling, confusing foot-tap comments, unused imports, and the absence of explicit coverage of every VRM bone if that is strictly required. Addressing these will improve fidelity to the Blocks philosophy and the block's stated constraints."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: nervous_fidget\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Anxiety-driven fidgeting with involuntary self-soothing behaviors.\\nWeight shifts irregularly. Hands seek comfort (touching face, arms, clothing).\\nShallow rapid breathing. Hypervigilant eye movement.\\n\\nFEEL: Uncomfortable, restless, seeking escape. Like waiting for bad news.\\n\\nTIMING: Breath is faster (2-3 second shallow cycles). Weight shifts are IRREGULAR\\n(not sine waves - use turbulence). Fidget gestures happen in bursts with quiet\\nperiods. Eyes dart quickly, then lock, then dart again.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe anxiety body language research basis\",\n        \"CORE: Hunched protective posture, shoulders elevated\",\n        \"CORE: Forward head posture (hypervigilance)\",\n        \"BREATH: Shallow, fast, irregular (2-3 second cycles)\",\n        \"BREATH: Shoulders rise with anxious breath\",\n        \"WEIGHT: IRREGULAR shifts using turbulence, not sine waves\",\n        \"WEIGHT: Ready-to-flee stance\",\n        \"ARMS: Held close to body (self-protection)\",\n        \"ARMS: Frequent self-touch gestures (neck, arms, face)\",\n        \"HANDS: Tense, curled fingers\",\n        \"HANDS: Rubbing, picking, clasping gestures\",\n        \"HANDS: All finger joints animated during fidgets\",\n        \"LEGS: Weight shifting, foot tapping bursts\",\n        \"FEET: Toes tapping or curled with tension\",\n        \"HEAD: Quick darting looks, then freeze, then dart\",\n        \"EYES: Rapid saccades, wide, frequent blinks\",\n        \"JAW: Clenched with occasional release\",\n        \"TIMING: Fidget BURSTS not continuous\",\n        \"TIMING: Quiet moments between fidget clusters\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- nervous-fidget.ts ---\n```\n/**\n * ============================================================================\n * NERVOUS FIDGET MOTION\n * ============================================================================\n *\n * Anxiety-driven motion with rapid weight shifts, self-soothing gestures,\n * and tense posture. Captures the biomechanics of nervousness and\n * displacement behavior.\n *\n * ============================================================================\n * HOW THIS MOTION SHOULD FEEL\n * ============================================================================\n *\n * Imagine someone waiting for bad news, or sitting in a job interview they're\n * not prepared for. The body is tense, protective, ready to flee. Every few\n * seconds there's a displacement behavior - rubbing hands together, touching\n * the neck, adjusting posture. The eyes dart around, never settling. Weight\n * shifts frequently from foot to foot. Breathing is shallow and fast.\n *\n * The key quality is IRREGULARITY. Unlike calm breathing which is rhythmic,\n * anxious breathing has catches and variations. Unlike confident posture\n * which is still, nervous posture constantly adjusts. The hands never rest.\n *\n * ============================================================================\n * TIMING RELATIONSHIPS\n * ============================================================================\n *\n * BREATH CYCLE: 2-3 seconds (irregular, turbulence-driven)\n *   - Base frequency: 0.25-0.4 Hz (faster than calm breathing)\n *   - Amplitude variation: ±30% noise-driven\n *   - Occasional \"catches\" where breath holds briefly\n *   - Shallow chest rise (half of calm breathing amplitude)\n *   - Shoulders rise with each inhale (tension accumulation)\n *\n * WEIGHT SHIFTS: Every 0.5-2 seconds (irregular)\n *   - Turbulence-driven (not sinusoidal)\n *   - Spring stiffness 150 (quick, snappy transitions)\n *   - Additional jitter layer at 3Hz for micro-adjustments\n *   - Never truly settles - constant micro-corrections\n *\n * HEAD MOVEMENT: Darting looks every 0.3-1.5 seconds\n *   - Quick saccades to new targets (spring stiffness 200)\n *   - 4Hz micro-jitter overlay for instability\n *   - Look targets change before head settles\n *   - Hypervigilant scanning pattern\n *\n * FIDGET GESTURES: Every 2-4 seconds\n *   - Phase envelope duration: 1.5-2.5 seconds\n *   - Sine-wave intensity curve (smooth in/out)\n *   - Types cycle: hand_rub → arm_touch → neck_touch\n *   - Overlap allowed (start next before previous ends)\n *\n * EYE MOVEMENT: Rapid, darting (3-7 Hz)\n *   - Much faster than calm eyes\n *   - Wider range: ±4.5° horizontal, ±3.5° vertical\n *   - Frequent blinking: every 0.7-1.5 seconds\n *   - Blinks are quick (120ms)\n *\n * FOOT TAPPING: Bursts of 4-8 taps at 3Hz\n *   - Triggered randomly when tension builds\n *   - Right foot dominant (natural for right-handers)\n *   - Toes lift, heel plants\n *\n * ============================================================================\n * PHASE RELATIONSHIPS (not state machine)\n * ============================================================================\n *\n * Multiple overlapping phase envelopes:\n *   - breathEnvelope: continuous, irregular sine + turbulence\n *   - weightEnvelope: continuous, turbulence-driven spring\n *   - fidgetEnvelope: 0-1-0 sine curve, duration 1.5-2.5s\n *   - lookEnvelope: spring-driven target following\n *   - tapEnvelope: triggered bursts, decays naturally\n *\n * Cross-coupling:\n *   - High anxiety → faster breathing → more shoulder tension\n *   - Fidget gestures → temporary weight shift compensation\n *   - Head darts → eye saccade slightly precedes\n *\n * ============================================================================\n * BONE HANDLING\n * ============================================================================\n *\n * Every bone access is wrapped in hasBone() checks.\n * Missing bones (jaw, eyes, fingers, toes) are gracefully skipped.\n *\n * TOUCHED BONES:\n *   Core: hips, spine, chest, upperChest, neck, head\n *   Arms: leftShoulder, rightShoulder, leftUpperArm, rightUpperArm,\n *         leftLowerArm, rightLowerArm, leftHand, rightHand\n *   Fingers: all 30 via applyFingerCurl helper\n *   Legs: leftUpperLeg, rightUpperLeg\n *   Feet: rightFoot, rightToes (for tapping)\n *   Face: leftEye, rightEye, jaw\n *\n * ============================================================================\n * RESEARCH BASIS\n * ============================================================================\n *\n * - Navarro, J. (2008): \"What Every Body Is Saying\" - Body language of\n *   anxiety includes self-touching (pacifying behaviors), weight shifting,\n *   and protective postures (arms close to body).\n *\n * - Ekman, P. & Friesen, W. (1969): Displacement activities in stress\n *   include grooming behaviors (touching face/neck) and object manipulation.\n *\n * - Harrigan, J. et al. (1991): Anxious individuals show increased postural\n *   sway frequency (0.5-1 Hz vs 0.1-0.3 Hz for calm).\n *\n * - Anxious breathing: 18-25 breaths/min vs 12-15 normal (2-3 second cycles).\n *\n * - Eye movements: Anxious scanning involves 3-5 saccades/second with wider\n *   amplitude than calm focused gaze.\n *\n * ============================================================================\n * NUMERICAL JUSTIFICATIONS\n * ============================================================================\n *\n * anxietyLevel default 0.6: Visible nervousness without panic\n * breathRateMultiplier 1.5: 1.5x faster = ~2.7s cycles (anxious range)\n * shoulderTension 0.7: Shoulders raised ~0.5cm (visible tension)\n * hunch 0.05 rad = 2.9°: Subtle forward curve (protective)\n * weightSpring stiffness 150: Quick shifts (0.2s settle time)\n * headSpring stiffness 200: Fast head turns (0.15s settle time)\n * fidgetInterval 2-4s: Based on Navarro's pacifying behavior frequency\n * eyeSpeed 3-7 Hz: Hypervigilant scanning rate\n * footTapFrequency 3 Hz: Natural tapping rhythm (180 bpm equivalent)\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport { applyFingerCurl } from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const nervousFidgetParamsSchema = z.object({\n  /** Overall anxiety level (0-1). Default: 0.6 - visible nervousness */\n  anxietyLevel: z.number().min(0).max(1).default(0.6),\n  /** Intensity of fidgeting movements (0-1). Default: 0.5 */\n  fidgetIntensity: z.number().min(0).max(1).default(0.5),\n  /** Speed of looking around (0-1). Default: 0.4 */\n  lookAroundSpeed: z.number().min(0).max(1).default(0.4),\n  /** Breath rate multiplier (nervous = faster). Default: 1.5 = ~2.7s cycles */\n  breathRateMultiplier: z.number().min(1).max(3).default(1.5),\n  /** Enable hand fidgeting. Default: true */\n  handFidget: z.boolean().default(true),\n  /** Enable foot tapping. Default: true */\n  footTap: z.boolean().default(true),\n  /** Enable rapid eye movement. Default: true */\n  rapidEyeMovement: z.boolean().default(true),\n  /** Shoulder tension (0-1). Default: 0.7 - raised ~0.5cm */\n  shoulderTension: z.number().min(0).max(1).default(0.7),\n})\n\nexport type NervousFidgetParams = z.infer<typeof nervousFidgetParamsSchema>\nexport type NervousFidgetInput = z.input<typeof nervousFidgetParamsSchema>\n\nexport const nervousFidgetMeta: MotionMeta = {\n  id: 'nervous-fidget',\n  name: 'Nervous Fidget',\n  description: 'Anxiety-driven fidgeting with weight shifts, tense posture, and self-soothing gestures',\n  tags: ['nervous', 'anxiety', 'fidget', 'tension'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface NervousFidgetState {\n  noise: NoiseGenerator\n  weightSpring: Spring      // stiffness 150: quick weight shifts\n  headSpring: Spring        // stiffness 200: fast head turns\n  armSpring: Spring         // stiffness 100: moderate arm motion\n  fidgetTimer: number\n  fidgetType: 'none' | 'hand_rub' | 'arm_touch' | 'neck_touch'\n  fidgetHand: 'left' | 'right'\n  lastFidgetTime: number\n  blinkTimer: number\n  isBlinking: boolean\n  footTapPhase: number\n  footTapActive: boolean\n  lookTarget: { x: number; y: number }\n  lookChangeTimer: number\n}\n\nfunction initState(seed: number): NervousFidgetState {\n  return {\n    noise: createNoiseGenerator(seed),\n    // Quick weight shifts - stiffness 150 gives ~0.2s settle time\n    weightSpring: createSpring({ stiffness: 150, damping: 12 }),\n    // Fast head turns - stiffness 200 gives ~0.15s settle time\n    headSpring: createSpring({ stiffness: 200, damping: 18 }),\n    // Moderate arm motion for fidgets\n    armSpring: createSpring({ stiffness: 100, damping: 15 }),\n    fidgetTimer: 0,\n    fidgetType: 'none',\n    fidgetHand: 'right',\n    lastFidgetTime: 0,\n    blinkTimer: 0,\n    isBlinking: false,\n    footTapPhase: 0,\n    footTapActive: false,\n    lookTarget: { x: 0, y: 0 },\n    lookChangeTimer: 0,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\nexport function createNervousFidget(params: NervousFidgetInput = {}): MotionProgram<NervousFidgetParams> {\n  const validatedParams = nervousFidgetParamsSchema.parse(params)\n  let state: NervousFidgetState | null = null\n\n  return {\n    meta: nervousFidgetMeta,\n    paramsSchema: nervousFidgetParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        anxietyLevel,\n        fidgetIntensity,\n        lookAroundSpeed,\n        breathRateMultiplier,\n        handFidget,\n        footTap,\n        rapidEyeMovement,\n        shoulderTension,\n      } = validatedParams\n\n      const noise = state.noise\n      const anxiety = anxietyLevel\n\n      // ========================================\n      // LAYER 1: TENSE BASE POSTURE\n      // ========================================\n\n      // Hunched, protective stance\n      // 0.05 rad = 2.9° forward hunch (subtle but visible)\n      const hunch = 0.05 * anxiety\n      // 0.03 rad = 1.7° forward lean at hips\n      const forwardLean = 0.03 * anxiety\n\n      // Hips - slightly tucked, ready to flee\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, forwardLean)\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // Spine - forward hunch (protective)\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch))\n      }\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch * 0.8))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hunch * 0.5))\n      }\n\n      // Shoulders - raised and tense (0.08 rad = 4.6° = ~0.5cm rise)\n      const shoulderRaise = shoulderTension * 0.08\n      const shoulderForward = shoulderTension * 0.06\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderRaise)\n        leftShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -shoulderForward))\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderRaise)\n        rightShoulderRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, shoulderForward))\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // Neck - forward head posture (hypervigilance)\n      // 0.08 rad = 4.6° forward (jutting chin)\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.08 * anxiety))\n      }\n\n      // Arms - held closer to body (protective)\n      const armProtect = 0.05 * anxiety\n      if (rig.hasBone('leftUpperArm')) {\n        // 0.15 rad base abduction, reduced by anxiety\n        const leftArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.15 - armProtect)\n        leftArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1))\n        rig.setRotation('leftUpperArm', leftArmRot)\n      }\n      if (rig.hasBone('rightUpperArm')) {\n        const rightArmRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.15 + armProtect)\n        rightArmRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.1))\n        rig.setRotation('rightUpperArm', rightArmRot)\n      }\n\n      // Bent elbows (0.4 rad = 23° flexion)\n      if (rig.hasBone('leftLowerArm')) {\n        rig.setRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.4 * anxiety))\n      }\n      if (rig.hasBone('rightLowerArm')) {\n        rig.setRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.4 * anxiety))\n      }\n\n      // Legs - slight asymmetry, ready to move\n      if (rig.hasBone('leftUpperLeg')) {\n        rig.setRotation('leftUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        rig.setRotation('rightUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.05))\n      }\n\n      // ========================================\n      // LAYER 2: IRREGULAR BREATHING (2-3 second cycles)\n      // ========================================\n\n      // Base breath rate: 0.25 Hz × 1.5 = 0.375 Hz = ~2.7s cycle\n      const breathRate = 0.25 * breathRateMultiplier\n\n      // IRREGULAR breathing using turbulence, not pure sine\n      // Turbulence adds catches and variations\n      const breathBase = Math.sin(t * breathRate * Math.PI * 2)\n      const breathTurbulence = noise.turbulence(t * 0.8, 50) * 0.3\n      const breathAmplitude = 1 + noise.noise2D(t * 0.5, 60) * 0.3 // ±30% variation\n      const breathPhase = (breathBase + breathTurbulence) * breathAmplitude\n\n      // Shallow chest movement (half of calm breathing)\n      const shallowBreath = breathPhase * 0.015 * anxiety\n\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, shallowBreath))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, shallowBreath * 1.2))\n      }\n\n      // Shoulder rise with anxious breath (tension accumulation)\n      const anxiousShoulderBreath = breathPhase * 0.01 * anxiety\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -anxiousShoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, anxiousShoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 3: IRREGULAR WEIGHT SHIFTING (0.5-2s)\n      // ========================================\n\n      // Quick, irregular weight shifts using turbulence\n      // 0.8 Hz base frequency for turbulence sampling\n      const weightShiftNoise = noise.turbulence(t * 0.8, 0) * 2 - 1\n      // 3 Hz micro-jitter for constant instability\n      const weightJitter = noise.noise2D(t * 3, 100) * fidgetIntensity * 0.3\n      state.weightSpring.setTarget(weightShiftNoise * 0.5 + weightJitter)\n      state.weightSpring.update(dt)\n      const weightShift = state.weightSpring.value * fidgetIntensity\n\n      if (rig.hasBone('hips')) {\n        // 0.04 rad = 2.3° lateral tilt\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, weightShift * 0.04))\n        // 0.02 rad = 1.1° rotation\n        rig.addRotation('hips', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightShift * 0.02))\n      }\n\n      // Counter in spine to stabilize head\n      if (rig.hasBone('spine')) {\n        rig.addRotation('spine', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -weightShift * 0.02))\n      }\n\n      // ========================================\n      // LAYER 4: HEAD MOVEMENT (darting looks 0.3-1.5s)\n      // ========================================\n\n      // Quick, darting looks (never settles)\n      state.lookChangeTimer += dt\n      // Interval: 1.5s base, reduced by lookAroundSpeed\n      const lookChangeInterval = 1.5 - lookAroundSpeed * 1.2\n\n      if (state.lookChangeTimer > lookChangeInterval) {\n        state.lookChangeTimer = 0\n        // New target before previous is reached\n        state.lookTarget = {\n          x: (noise.noise2D(t, 200) * 2 - 1) * 0.15 * lookAroundSpeed,\n          y: (noise.noise2D(t, 300) * 2 - 1) * 0.1 * lookAroundSpeed,\n        }\n      }\n\n      // Spring-driven head following (stiffness 200 = 0.15s settle)\n      state.headSpring.setTarget(state.lookTarget.x)\n      state.headSpring.update(dt)\n      const headYaw = state.headSpring.value\n\n      // 4 Hz micro-jitter overlay for instability\n      const headJitterX = noise.noise2D(t * 4, 400) * 0.02 * fidgetIntensity\n      const headJitterY = noise.noise2D(t * 4, 500) * 0.015 * fidgetIntensity\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headYaw + headJitterY)\n        headRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, state.lookTarget.y + headJitterX))\n        rig.setRotation('head', headRot)\n      }\n\n      // Neck follows head partially (30% coupling)\n      if (rig.hasBone('neck')) {\n        rig.addRotation('neck', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headYaw * 0.3))\n      }\n\n      // ========================================\n      // LAYER 5: HAND FIDGETING (phase envelopes)\n      // ========================================\n\n      if (handFidget) {\n        // Fidget interval: 2-4 seconds (Navarro's pacifying behavior frequency)\n        const timeSinceFidget = t - state.lastFidgetTime\n        const fidgetInterval = 2 + noise.noise2D(t * 0.1, 600) * 2\n\n        if (state.fidgetType === 'none' && timeSinceFidget > fidgetInterval) {\n          // Select fidget type based on noise\n          const fidgetRoll = noise.noise2D(t, 700)\n          if (fidgetRoll > 0.6) {\n            state.fidgetType = 'hand_rub'\n          } else if (fidgetRoll > 0.3) {\n            state.fidgetType = 'arm_touch'\n          } else {\n            state.fidgetType = 'neck_touch'\n          }\n          state.fidgetHand = noise.noise2D(t, 800) > 0.5 ? 'left' : 'right'\n          state.fidgetTimer = 0\n        }\n\n        if (state.fidgetType !== 'none') {\n          state.fidgetTimer += dt\n          // Fidget duration: 1.5-2.5 seconds\n          const fidgetDuration = 1.5 + noise.noise2D(t * 0.2, 900) * 1\n          const fidgetProgress = state.fidgetTimer / fidgetDuration\n\n          if (fidgetProgress >= 1) {\n            state.fidgetType = 'none'\n            state.lastFidgetTime = t\n          } else {\n            // Sine-wave phase envelope (smooth in/out)\n            const fidgetWeight = Math.sin(fidgetProgress * Math.PI)\n\n            // Apply fidget based on type (all with hasBone checks)\n            switch (state.fidgetType) {\n              case 'hand_rub':\n                // Bring hands together in front (self-soothing)\n                if (rig.hasBone('leftUpperArm')) {\n                  rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.3 * fidgetWeight))\n                  rig.addRotation('leftUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.2 * fidgetWeight))\n                }\n                if (rig.hasBone('rightUpperArm')) {\n                  rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.3 * fidgetWeight))\n                  rig.addRotation('rightUpperArm', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.2 * fidgetWeight))\n                }\n                if (rig.hasBone('leftLowerArm')) {\n                  rig.addRotation('leftLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.5 * fidgetWeight))\n                }\n                if (rig.hasBone('rightLowerArm')) {\n                  rig.addRotation('rightLowerArm', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.5 * fidgetWeight))\n                }\n                // Rubbing motion at 8 Hz\n                const rubPhase = Math.sin(state.fidgetTimer * 8) * fidgetWeight\n                if (rig.hasBone('leftHand')) {\n                  rig.addRotation('leftHand', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rubPhase * 0.2))\n                }\n                break\n\n              case 'arm_touch':\n                // Touch opposite arm (pacifying)\n                const touchArm = state.fidgetHand === 'left' ? 'left' : 'right'\n                if (rig.hasBone(`${touchArm}UpperArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${touchArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.4 * fidgetWeight))\n                }\n                if (rig.hasBone(`${touchArm}LowerArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${touchArm}LowerArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 1, z: 0 }, (touchArm === 'left' ? -1 : 1) * 0.8 * fidgetWeight))\n                }\n                break\n\n              case 'neck_touch':\n                // Touch back of neck (vulnerability display)\n                const neckArm = state.fidgetHand\n                if (rig.hasBone(`${neckArm}UpperArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${neckArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.5 * fidgetWeight))\n                  rig.addRotation(`${neckArm}UpperArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 0, z: 1 }, (neckArm === 'left' ? -1 : 1) * 0.3 * fidgetWeight))\n                }\n                if (rig.hasBone(`${neckArm}LowerArm` as VRMHumanBoneName)) {\n                  rig.addRotation(`${neckArm}LowerArm` as VRMHumanBoneName,\n                    quatFromAxisAngle({ x: 0, y: 1, z: 0 }, (neckArm === 'left' ? -1 : 1) * 1.2 * fidgetWeight))\n                }\n                break\n            }\n          }\n        }\n\n        // Fingers - tense, curled (anxiety response)\n        // Base curl 0.4 rad = 23° with ±0.2 noise variation\n        const fingerTension = 0.4 + noise.noise2D(t * 2, 1000) * 0.2\n        if (rig.hasBone('leftHand')) {\n          applyFingerCurl(rig, 'left', {\n            thumb: fingerTension * 0.5,\n            index: fingerTension,\n            middle: fingerTension * 1.1,\n            ring: fingerTension * 1.15,\n            little: fingerTension * 1.2,\n          })\n        }\n        if (rig.hasBone('rightHand')) {\n          applyFingerCurl(rig, 'right', {\n            thumb: fingerTension * 0.5,\n            index: fingerTension,\n            middle: fingerTension * 1.1,\n            ring: fingerTension * 1.15,\n            little: fingerTension * 1.2,\n          })\n        }\n      }\n\n      // ========================================\n      // LAYER 6: FOOT TAPPING (burst pattern)\n      // ========================================\n\n      if (footTap) {\n        // Trigger tapping when noise threshold exceeded (tension release)\n        const tapTrigger = noise.noise2D(t * 0.5, 1100)\n        if (!state.footTapActive && tapTrigger > 0.7) {\n          state.footTapActive = true\n          state.footTapPhase = 0\n        }\n\n        if (state.footTapActive) {\n          // 3 Hz tapping (6 × π = 2 taps per second × 3 = 6 taps/sec)\n          // Actually: phase increases by 6*dt, so 6 full cycles per second at π*2\n          // Let's fix: 3 Hz = 3 taps per second\n          state.footTapPhase += dt * 3\n          const tapCycle = Math.sin(state.footTapPhase * Math.PI * 2)\n          const tapUp = Math.max(0, tapCycle) * fidgetIntensity\n\n          // Right foot tap (natural for most people)\n          if (rig.hasBone('rightFoot')) {\n            // 0.15 rad = 8.6° dorsiflexion\n            rig.addRotation('rightFoot', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, tapUp * 0.15))\n          }\n          if (rig.hasBone('rightToes')) {\n            // Toes lift opposite to foot\n            rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -tapUp * 0.2))\n          }\n\n          // Stop after 4-8 taps (footTapPhase 4-8)\n          if (state.footTapPhase > 4 + noise.noise2D(t, 1150) * 4) {\n            state.footTapActive = false\n          }\n        }\n      }\n\n      // ========================================\n      // LAYER 7: RAPID EYE MOVEMENT (3-7 Hz)\n      // ========================================\n\n      if (rapidEyeMovement) {\n        // Quick, darting eye movements\n        // Speed: 3-7 Hz based on lookAroundSpeed\n        const eyeSpeed = 3 + lookAroundSpeed * 4\n        // Range: ±4.5° horizontal, ±3.5° vertical\n        const eyeX = noise.noise2D(t * eyeSpeed, 1200) * 0.08 * anxiety\n        const eyeY = noise.noise2D(t * eyeSpeed, 1300) * 0.06 * anxiety\n\n        // Frequent blinking: 0.7-1.5 seconds based on anxiety\n        // Higher anxiety = more frequent blinks\n        if (!state.isBlinking && noise.noise2D(t * 0.5, 1400) > 0.8) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          // Quick blink: 120ms\n          if (state.blinkTimer > 0.12) {\n            state.isBlinking = false\n          } else {\n            blinkAmount = Math.sin(state.blinkTimer / 0.12 * Math.PI)\n          }\n        }\n\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount * 0.3)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 8: JAW TENSION\n      // ========================================\n\n      if (rig.hasBone('jaw')) {\n        // Jaw clench with occasional stress release\n        // 0.02 rad = 1.1° clench (visible as facial tension)\n        const jawTension = anxiety * 0.02\n        // Occasional release when noise threshold exceeded\n        const jawRelease = noise.noise2D(t * 0.3, 1500) > 0.8 ? 0.01 : 0\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawRelease - jawTension))\n      }\n    },\n  }\n}\n\nexport const nervousFidget: MotionProgram<NervousFidgetParams> = createNervousFidget({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": true,\n  \"issues\": [\n    {\n      \"message\": \"Hand rubbing (hand_rub) phase applies a rubbing rotation only to leftHand (rubPhase) while symmetric arm rotations are applied to both upper arms. This makes the rubbing gesture asymmetric regardless of selected fidgetHand and likely omits the intended rubbing motion for the other hand.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"The fidget logic supports neck_touch and arm_touch but no explicit 'face' touch / clothing touch implementation is present, yet the output constraints and documentation expect self-touching that can include the face. Consider adding an explicit face-touch variant or documenting that hand_rub represents face/contact behavior.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"The code assumes applyFingerCurl will animate all finger joints (requirement: 'All finger joints animated during fidgets') but this file does not verify or document that applyFingerCurl animates all 30 finger joints. Either assert/document the helper's behavior or explicitly animate additional finger joints here.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"There are inconsistent uses of the noise API: some noise outputs are remapped to [-1,1] with '*2-1' while others are used raw (e.g. fidgetInterval uses noise.noise2D(...) * 2 without remapping). This inconsistency can change ranges unpredictably depending on the noise implementation and should be standardized or documented.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Minor comment/code mismatch in the foot tapping section: comments around phase/rate math are confusing and contain incorrect intermediate reasoning (mentions 6×π and 'Let's fix' while code settles on state.footTapPhase += dt * 3). Clean up comments to match implemented 3 Hz behavior.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"Unused imports detected: 'osc' and 'Easing' are imported from '@posers/core' but not used in this file. Remove or use them to avoid dead code.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    },\n    {\n      \"message\": \"The file documents and guards against missing bones (hasBone checks) and touches many VRM bones, but it does not explicitly reference every VRM bone (the project rule states 'Every motion must consider ALL 69 VRM bones'). While the implementation degrades gracefully (good), if the requirement is to explicitly consider all bones you should either enumerate handling for the remaining optional bones (e.g., leftFoot/leftToes and other minor facial bones) or add an explicit statement in the documentation that unmentioned bones are intentionally left passive and will be skipped with hasBone guards.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/nervous-fidget.ts\"\n    }\n  ],\n  \"summary\": \"The motion file is well-documented, aligns closely with the block's domain intent (anxious posture, irregular breath, turbulent weight shifts, darting head/eyes, finger curling, and foot tapping), uses rig/ctx inputs through the MotionProgram interface correctly, and applies hasBone() guards to avoid errors on missing bones. No blocking errors were found, but several warnings highlight areas for improvement: asymmetric hand rubbing, missing explicit face-touch gesture, reliance on an external helper for animating all finger joints without verification, inconsistent noise-range handling, confusing foot-tap comments, unused imports, and the absence of explicit coverage of every VRM bone if that is strictly required. Addressing these will improve fidelity to the Blocks philosophy and the block's stated constraints.\"\n}",
            "tokensUsed": {
              "input": 8604,
              "output": 3645
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "smoking_cigarette",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/smoking-cigarette.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "smoking_cigarette",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe smoking biomechanics and timing offsets",
                    "PHASE ENVELOPES: NO discrete state machine",
                    "PHASE ENVELOPES: Shoulder, elbow, wrist have OVERLAPPING phases",
                    "PHASE ENVELOPES: Shoulder LEADS, wrist TRAILS",
                    "ANTICIPATION: Body telegraphs before arm rises",
                    "ANTICIPATION: Weight shifts before arm movement",
                    "ARM TRAJECTORY: Curved natural path, not linear",
                    "ARM TRAJECTORY: Spring-smoothed with overshoot/settle",
                    "HAND POSITION: Must accurately reach mouth (needs IK or precise targeting)",
                    "FINGERS: Precise cigarette grip (index + middle pinch)",
                    "FINGERS: Subtle adjustments throughout",
                    "INHALE: Chest expands, shoulders rise",
                    "INHALE: Eyes squint slightly",
                    "INHALE: Cheeks may hollow (jaw bone)",
                    "HOLD: Brief stillness, micro-tension",
                    "EXHALE: Jaw opens, lips purse",
                    "EXHALE: Head may tilt up or to side",
                    "EXHALE: Slow controlled breath",
                    "LOWER: Wrist leads, shoulder follows (opposite of raise)",
                    "LOWER: Follow-through past rest position, then settle",
                    "SUPPORT ARM: Crossed under, or relaxed at side",
                    "BREATH: Couples to entire motion cycle",
                    "STYLE: Casual/Stressed/Seductive affect ALL timing"
                  ]
                }
              ],
              "description": "Complete smoking animation with naturalistic hand-to-mouth coordination.\nNOT a state machine - overlapping phase envelopes for each body part.\n\nFEEL: Depends on style param. Casual = relaxed habit. Stressed = need for relief.\nSeductive = deliberate, slow, eye contact maintained.\n\nTIMING RELATIONSHIPS:\n- Shoulder rotation BEGINS first (leads by ~80ms)\n- Elbow extension follows (~50ms after shoulder)\n- Wrist rotation follows (~30ms after elbow)\n- Fingers adjust grip throughout\n- Inhale: chest expands, shoulders rise, eyes squint\n- Hold: brief stillness, smoke held\n- Exhale: jaw opens, head may tilt, slow breath out\n- Lower: reverse order (wrist leads, shoulder follows)\n\nCRITICAL: Hand MUST reach mouth accurately. This requires end-effector\ntargeting, not blind rotation values.\n",
              "path": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Complete smoking animation with naturalistic hand-...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 50975,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Missing end-effector targeting / IK for hand-to-mouth. The implementation only blends joint rotations (shoulder/upperArm/lowerArm/wrist) to approximate a reach. The block's output constraints explicitly require the hand MUST reach the mouth accurately and state that this requires IK or precise targeting. Without IK or an explicit end-effector solver, the hand position can drift and will not reliably meet the 'must reach mouth accurately' requirement across different rigs.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Phase envelopes are present but there are no explicit per-joint staggered phase envelopes or timing offsets for shoulder → elbow → wrist. The code uses shared armUpEnv and spring smoothing to create coordination, but the domain rules require overlapping phase envelopes for each body part with shoulder leading and wrist trailing by specified offsets (e.g., ~80ms/50ms/30ms). This is only implicitly approximated by springs rather than explicit enveloping.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Fingers/grip lack explicit continuous micro-adjustments across inhale/hold/exhale phases. applyCigaretteGrip is applied (good) but the code does not drive subtle phase-dependent finger adjustments beyond a static grip helper call. The output constraints expect 'FINGERS: Subtle adjustments throughout' during the puff cycle.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "No explicit anatomical joint limit/clamping enforcement. The domain rules require joint rotations to respect anatomical limits. While the numeric choices appear reasonable, there are no clamps or checks to guarantee safety across different rigs/scale factors.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Per-joint phase envelopes (shoulder/elbow/wrist) are not separately declared. The PHASE_TIMING table defines high-level phases (armRaise, inhale, etc.) but the implementation does not expose or use distinct timed envelopes for shoulder/elbow/wrist which the domain philosophy and block constraints emphasize.",
              "file": "packages/motion-dsl/src/motions/smoking-cigarette.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "smoking-cigarette.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block is well-documented and largely aligns with the Blocks motion philosophy: it uses phase envelopes (getPhaseEnvelope), springs, noise, hasBone() guards, breath coupling, style parameters, and helpers for grip. However it fails a critical domain requirement: it does not implement end-effector targeting or IK to guarantee the hand reaches the mouth accurately (an explicit output constraint). Additionally, the implementation relies on spring smoothing rather than explicit per-joint staggered phase envelopes and lacks explicit joint limit enforcement and fine continuous finger micro-adjustments. Fixing those issues (add IK/targeting for the hand, add per-joint phase envelopes or explicit timing offsets for shoulder→elbow→wrist, clamp joint rotations, and drive finger micro-motions through the phases) would bring the block into compliance."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: smoking_cigarette\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Complete smoking animation with naturalistic hand-to-mouth coordination.\\nNOT a state machine - overlapping phase envelopes for each body part.\\n\\nFEEL: Depends on style param. Casual = relaxed habit. Stressed = need for relief.\\nSeductive = deliberate, slow, eye contact maintained.\\n\\nTIMING RELATIONSHIPS:\\n- Shoulder rotation BEGINS first (leads by ~80ms)\\n- Elbow extension follows (~50ms after shoulder)\\n- Wrist rotation follows (~30ms after elbow)\\n- Fingers adjust grip throughout\\n- Inhale: chest expands, shoulders rise, eyes squint\\n- Hold: brief stillness, smoke held\\n- Exhale: jaw opens, head may tilt, slow breath out\\n- Lower: reverse order (wrist leads, shoulder follows)\\n\\nCRITICAL: Hand MUST reach mouth accurately. This requires end-effector\\ntargeting, not blind rotation values.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe smoking biomechanics and timing offsets\",\n        \"PHASE ENVELOPES: NO discrete state machine\",\n        \"PHASE ENVELOPES: Shoulder, elbow, wrist have OVERLAPPING phases\",\n        \"PHASE ENVELOPES: Shoulder LEADS, wrist TRAILS\",\n        \"ANTICIPATION: Body telegraphs before arm rises\",\n        \"ANTICIPATION: Weight shifts before arm movement\",\n        \"ARM TRAJECTORY: Curved natural path, not linear\",\n        \"ARM TRAJECTORY: Spring-smoothed with overshoot/settle\",\n        \"HAND POSITION: Must accurately reach mouth (needs IK or precise targeting)\",\n        \"FINGERS: Precise cigarette grip (index + middle pinch)\",\n        \"FINGERS: Subtle adjustments throughout\",\n        \"INHALE: Chest expands, shoulders rise\",\n        \"INHALE: Eyes squint slightly\",\n        \"INHALE: Cheeks may hollow (jaw bone)\",\n        \"HOLD: Brief stillness, micro-tension\",\n        \"EXHALE: Jaw opens, lips purse\",\n        \"EXHALE: Head may tilt up or to side\",\n        \"EXHALE: Slow controlled breath\",\n        \"LOWER: Wrist leads, shoulder follows (opposite of raise)\",\n        \"LOWER: Follow-through past rest position, then settle\",\n        \"SUPPORT ARM: Crossed under, or relaxed at side\",\n        \"BREATH: Couples to entire motion cycle\",\n        \"STYLE: Casual/Stressed/Seductive affect ALL timing\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- smoking-cigarette.ts ---\n```\n/**\n * ============================================================================\n * SMOKING CIGARETTE MOTION\n * ============================================================================\n *\n * Complete smoking animation using overlapping phase envelopes (NOT state\n * machine). Hand raises to mouth, inhale expands chest, hold briefly, exhale\n * slowly, arm lowers with optional ash tap.\n *\n * ============================================================================\n * HOW THIS MOTION SHOULD FEEL\n * ============================================================================\n *\n * Think of a film noir character leaning against a doorframe, cigarette\n * dangling between fingers. The motion is unhurried, sensual even. The arm\n * drifts up to the mouth in a lazy arc. The inhale is deep and deliberate,\n * eyes narrowing slightly against the smoke. A pause - savoring - then the\n * slow exhale, smoke curling upward. The arm descends but never quite\n * returns to rest before the next pull.\n *\n * The key is OVERLAP. The exhale begins before the arm fully lowers. The\n * chest relaxation lags behind the breath. The wrist adjusts before the\n * elbow completes its motion. This creates the fluid, organic feel of a\n * habitual smoker rather than a robot performing discrete steps.\n *\n * ============================================================================\n * PHASE ENVELOPE ARCHITECTURE (NOT STATE MACHINE)\n * ============================================================================\n *\n * PUFF CYCLE: ~8 seconds total, overlapping phases\n *\n *   Time:    0   1   2   3   4   5   6   7   8\n *   armRaise:  ▁▁▄▇██▇▄▁▁▁▁▁▁▁▁▁▁\n *   inhale:       ▁▄▇██▇▄▁▁▁▁▁▁▁\n *   hold:            ▁▄▇██▇▄▁▁▁▁\n *   exhale:              ▁▄▇█▇▄▁▁\n *   armLower:                 ▁▄▇█▇▄▁\n *   ashTap:                    ▁▃▇▃▁ (optional, random)\n *\n * Each envelope is a smooth 0→1→0 curve (typically sine or smootherstep).\n * Overlaps ensure no discrete transitions.\n *\n * ============================================================================\n * TIMING RELATIONSHIPS\n * ============================================================================\n *\n * ARM RAISE: 0.8-1.0 seconds\n *   - Shoulder leads by ~0.1s (anticipation)\n *   - Elbow follows shoulder by ~0.15s\n *   - Wrist follows elbow by ~0.1s\n *   - Uses spring physics for natural deceleration at top\n *\n * INHALE: 1.5-2.0 seconds\n *   - Starts 0.3s before arm reaches peak\n *   - Chest expansion leads (visible breath intake)\n *   - Shoulders rise 0.1s after chest begins\n *   - Eyes squint gradually, peak at 80% inhale\n *\n * HOLD: 0.5-1.0 seconds\n *   - Overlaps end of inhale and start of exhale\n *   - Arm micro-drifts with noise (not frozen)\n *   - Fingers maintain cigarette grip\n *\n * EXHALE: 2.0-2.5 seconds\n *   - Begins while arm still high\n *   - Jaw opens slightly at 30% exhale\n *   - Head tilts up/back for \"blowing smoke up\"\n *   - Chest deflates slower than inhale inflated\n *\n * ARM LOWER: 0.6-0.8 seconds\n *   - Begins at 70% of exhale\n *   - Wrist leads (cigarette tips forward first)\n *   - Elbow follows by ~0.1s\n *   - Shoulder follows by ~0.15s\n *\n * ASH TAP: 0.3-0.4 seconds (random, 30% of cycles)\n *   - Quick wrist flick during lower phase\n *   - Two oscillations (tap-tap)\n *\n * ============================================================================\n * BONE HANDLING\n * ============================================================================\n *\n * All bone access wrapped in hasBone() checks.\n *\n * SMOKING ARM: [side]Shoulder, [side]UpperArm, [side]LowerArm, [side]Hand\n *   + all 15 finger bones via applyCigaretteGrip helper\n *\n * SUPPORT ARM: Opposite side arm, relaxed or crossed (style-dependent)\n *   + all 15 finger bones via applyFingerCurl helper\n *\n * CORE: hips, spine, chest, upperChest, neck, head\n * LEGS: leftUpperLeg, rightUpperLeg, leftLowerLeg, rightLowerLeg\n * FEET: leftFoot, rightFoot\n * FACE: leftEye, rightEye, jaw\n *\n * ============================================================================\n * RESEARCH BASIS\n * ============================================================================\n *\n * - Bernstein, N. (1967): \"The Co-ordination and Regulation of Movements\" -\n *   Proximal joints initiate movement, distal joints follow (kinetic chain).\n *   Shoulder → elbow → wrist → fingers timing.\n *\n * - Smoking behavior studies show average puff duration 1.5-2s, inter-puff\n *   interval 20-60s for casual smoking. We compress to 8s for visual interest.\n *\n * - Hand-to-mouth gestures: Peak velocity at 40% of movement, smooth\n *   deceleration approaching target (Fitts's Law applied to natural motion).\n *\n * - Respiratory mechanics: Inhale 1.5-2s, exhale 2-3s (exhale longer than\n *   inhale for relaxed breathing).\n *\n * ============================================================================\n * NUMERICAL JUSTIFICATIONS\n * ============================================================================\n *\n * puffInterval 8s: Compressed from real 20-60s for visual engagement\n * armRaiseDuration 0.8s: Natural reach-to-mouth speed\n * inhaleDepth 0.06 rad: Visible but not exaggerated chest expansion\n * eyeSquint 0.15 rad: Subtle narrowing, not cartoonish\n * wristFlick 0.3 rad: 17° flick for ash tap (natural wrist range)\n * springStiffness smooth preset: ~180 for natural arm deceleration\n * phase overlap 30%: Ensures continuous blending between actions\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  SpringPresets,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  applyCigaretteGrip,\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const smokingCigaretteParamsSchema = z.object({\n  /** Smoking style: casual, stressed, or seductive. Default: casual */\n  smokingStyle: z.enum(['casual', 'stressed', 'seductive']).default('casual'),\n  /** Which hand holds the cigarette. Default: right */\n  handedness: z.enum(['left', 'right']).default('right'),\n  /** Depth of inhale (0-1). Default: 0.7 */\n  inhaleDepth: z.number().min(0).max(1).default(0.7),\n  /** Variation in pacing (0-1). Default: 0.3 */\n  paceVariation: z.number().min(0).max(1).default(0.3),\n  /** Overall animation intensity (0-1). Default: 0.8 */\n  intensity: z.number().min(0).max(1).default(0.8),\n  /** Enable eye squint during inhale. Default: true */\n  eyeSquint: z.boolean().default(true),\n  /** Enable jaw animation during exhale. Default: true */\n  jawAnimation: z.boolean().default(true),\n  /** Time between puffs in seconds. Default: 8 */\n  puffInterval: z.number().min(3).max(20).default(8),\n})\n\nexport type SmokingCigaretteParams = z.infer<typeof smokingCigaretteParamsSchema>\nexport type SmokingCigaretteInput = z.input<typeof smokingCigaretteParamsSchema>\n\nexport const smokingCigaretteMeta: MotionMeta = {\n  id: 'smoking-cigarette',\n  name: 'Smoking Cigarette',\n  description: 'Complete smoking animation with overlapping phase envelopes for hand-to-mouth, inhale, exhale',\n  tags: ['smoking', 'gesture', 'complex', 'phase-envelope'],\n  author: 'posers',\n}\n\n// ============================================================================\n// PHASE ENVELOPE TIMING\n// ============================================================================\n\n/**\n * Phase timing configuration - defines when each phase starts and ends\n * as a fraction of the total puff cycle.\n *\n * These overlap to create smooth blending between actions.\n */\nconst PHASE_TIMING = {\n  // armRaise: starts at 0%, peaks at 15%, ends at 30%\n  armRaise: { start: 0.0, peak: 0.15, end: 0.30 },\n  // inhale: starts at 10%, peaks at 25%, ends at 40%\n  inhale: { start: 0.10, peak: 0.25, end: 0.40 },\n  // hold: starts at 35%, peaks at 45%, ends at 55%\n  hold: { start: 0.35, peak: 0.45, end: 0.55 },\n  // exhale: starts at 45%, peaks at 65%, ends at 85%\n  exhale: { start: 0.45, peak: 0.65, end: 0.85 },\n  // armLower: starts at 70%, peaks at 85%, ends at 100%\n  armLower: { start: 0.70, peak: 0.85, end: 1.0 },\n}\n\n/**\n * Calculate phase envelope value (0-1) given cycle progress.\n * Uses smootherstep for organic acceleration/deceleration.\n */\nfunction getPhaseEnvelope(cycleProgress: number, timing: { start: number; peak: number; end: number }): number {\n  if (cycleProgress < timing.start || cycleProgress > timing.end) {\n    return 0\n  }\n\n  if (cycleProgress < timing.peak) {\n    // Rising edge: start → peak\n    const t = (cycleProgress - timing.start) / (timing.peak - timing.start)\n    return Easing.smootherstep(t)\n  } else {\n    // Falling edge: peak → end\n    const t = (cycleProgress - timing.peak) / (timing.end - timing.peak)\n    return 1 - Easing.smootherstep(t)\n  }\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface SmokingState {\n  noise: NoiseGenerator\n  armSpringX: Spring      // Forward/back shoulder rotation\n  armSpringY: Spring      // Abduction\n  armSpringZ: Spring      // Twist\n  wristSpring: Spring     // Wrist rotation\n  chestSpring: Spring     // Chest expansion\n  lastPuffStart: number   // Time when current puff cycle started\n  ashTapActive: boolean   // Whether current cycle includes ash tap\n  blinkTimer: number\n  isBlinking: boolean\n}\n\nfunction initState(seed: number): SmokingState {\n  return {\n    noise: createNoiseGenerator(seed),\n    // Smooth spring preset (stiffness ~180, damping ~20) for natural arm motion\n    armSpringX: createSpring(SpringPresets.smooth),\n    armSpringY: createSpring(SpringPresets.smooth),\n    armSpringZ: createSpring(SpringPresets.smooth),\n    wristSpring: createSpring({ stiffness: 250, damping: 22 }),\n    chestSpring: createSpring({ stiffness: 100, damping: 18 }),\n    lastPuffStart: -10, // Start immediately\n    ashTapActive: false,\n    blinkTimer: 0,\n    isBlinking: false,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\nexport function createSmokingCigarette(params: SmokingCigaretteInput = {}): MotionProgram<SmokingCigaretteParams> {\n  const validatedParams = smokingCigaretteParamsSchema.parse(params)\n  let state: SmokingState | null = null\n\n  return {\n    meta: smokingCigaretteMeta,\n    paramsSchema: smokingCigaretteParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        smokingStyle,\n        handedness,\n        inhaleDepth,\n        paceVariation,\n        intensity,\n        eyeSquint,\n        jawAnimation,\n        puffInterval,\n      } = validatedParams\n\n      const noise = state.noise\n      const isRightHanded = handedness === 'right'\n      const handSide = isRightHanded ? 1 : -1\n\n      // Style modifiers affect speed and posture\n      const styleModifiers = {\n        casual: { speed: 1, tension: 0.3, lean: 0 },\n        stressed: { speed: 1.3, tension: 0.7, lean: 0.05 },\n        seductive: { speed: 0.7, tension: 0.2, lean: -0.03 },\n      }[smokingStyle]\n\n      // ========================================\n      // PUFF CYCLE PROGRESS (0-1)\n      // ========================================\n\n      // Calculate where we are in the current puff cycle\n      const timeSincePuffStart = t - state.lastPuffStart\n      const adjustedPuffInterval = puffInterval / styleModifiers.speed\n      // Add noise-based variation to puff interval\n      const puffVariation = 1 + noise.noise2D(state.lastPuffStart, 100) * paceVariation * 0.3\n\n      // Cycle progress from 0 to 1\n      let cycleProgress = timeSincePuffStart / (adjustedPuffInterval * puffVariation)\n\n      // Start new cycle when current one completes\n      if (cycleProgress >= 1) {\n        state.lastPuffStart = t\n        cycleProgress = 0\n        // Decide if this cycle includes ash tap (30% chance)\n        state.ashTapActive = noise.noise2D(t, 500) > 0.7\n      }\n\n      // ========================================\n      // CALCULATE ALL PHASE ENVELOPES\n      // ========================================\n\n      const armRaiseEnv = getPhaseEnvelope(cycleProgress, PHASE_TIMING.armRaise)\n      const inhaleEnv = getPhaseEnvelope(cycleProgress, PHASE_TIMING.inhale)\n      const holdEnv = getPhaseEnvelope(cycleProgress, PHASE_TIMING.hold)\n      const exhaleEnv = getPhaseEnvelope(cycleProgress, PHASE_TIMING.exhale)\n      const armLowerEnv = getPhaseEnvelope(cycleProgress, PHASE_TIMING.armLower)\n\n      // Combined \"arm up\" envelope - arm raised for inhale/hold/start of exhale\n      // This creates the overlap where arm stays up during multiple phases\n      const armUpEnv = Math.max(armRaiseEnv, inhaleEnv, holdEnv, exhaleEnv * 0.7)\n\n      // Ash tap envelope (optional, during arm lower)\n      let ashTapEnv = 0\n      if (state.ashTapActive && cycleProgress > 0.75 && cycleProgress < 0.85) {\n        // Two quick oscillations\n        const tapProgress = (cycleProgress - 0.75) / 0.1\n        ashTapEnv = Math.sin(tapProgress * Math.PI * 4) * (1 - tapProgress)\n      }\n\n      // ========================================\n      // LAYER 1: BASE POSTURE\n      // ========================================\n\n      // Style-based lean\n      if (rig.hasBone('hips')) {\n        const leanAmount = styleModifiers.lean * intensity\n        rig.setRotation('hips', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leanAmount))\n      }\n\n      // Relaxed spine with slight forward curve\n      if (rig.hasBone('spine')) {\n        rig.setRotation('spine', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n      }\n\n      // ========================================\n      // LAYER 2: ARM MOVEMENT (PHASE ENVELOPES)\n      // ========================================\n\n      const smokingArm = isRightHanded ? 'right' : 'left'\n      const supportArm = isRightHanded ? 'left' : 'right'\n\n      // Arm position interpolated between idle and raised based on envelopes\n      // Idle position\n      const idleArmX = 0.1 * intensity    // Slight forward\n      const idleArmY = 0.15 * intensity   // Slight abduction\n      const idleArmZ = 0                  // No twist\n      const idleElbow = 0.3 * intensity   // Slight bend\n      const idleWrist = 0.1 * handSide    // Slight rotation\n\n      // Raised position (at mouth)\n      const raisedArmX = -0.5 * intensity  // Forward flexion to bring to mouth\n      const raisedArmY = 0.4 * intensity   // Abducted\n      const raisedArmZ = handSide * 0.2 * intensity  // Twist toward face\n      const raisedElbow = 1.4 * intensity  // Fully bent\n      const raisedWrist = handSide * 0.3   // Rotated for cigarette\n\n      // Blend between idle and raised based on armUpEnv\n      const armTargetX = idleArmX + (raisedArmX - idleArmX) * armUpEnv\n      const armTargetY = idleArmY + (raisedArmY - idleArmY) * armUpEnv\n      const armTargetZ = idleArmZ + (raisedArmZ - idleArmZ) * armUpEnv\n      const elbowBend = idleElbow + (raisedElbow - idleElbow) * armUpEnv\n      let wristRotation = idleWrist + (raisedWrist - idleWrist) * armUpEnv\n\n      // Add ash tap to wrist if active\n      wristRotation += ashTapEnv * 0.3 * handSide\n\n      // Add micro-movements during hold phase for organic feel\n      if (holdEnv > 0.1) {\n        const holdMicroX = noise.noise2D(t * 0.5, 250) * 0.015 * intensity * holdEnv\n        const holdMicroY = noise.noise2D(t * 0.4, 260) * 0.01 * intensity * holdEnv\n        wristRotation += noise.noise2D(t * 0.6, 280) * 0.02 * intensity * holdEnv\n      }\n\n      // Apply spring smoothing for natural motion\n      state.armSpringX.setTarget(armTargetX)\n      state.armSpringY.setTarget(armTargetY)\n      state.armSpringZ.setTarget(armTargetZ)\n      state.wristSpring.setTarget(wristRotation)\n\n      state.armSpringX.update(dt)\n      state.armSpringY.update(dt)\n      state.armSpringZ.update(dt)\n      state.wristSpring.update(dt)\n\n      const smoothArmX = state.armSpringX.value\n      const smoothArmY = state.armSpringY.value\n      const smoothArmZ = state.armSpringZ.value\n      const smoothWrist = state.wristSpring.value\n\n      // Bone names for smoking arm\n      const upperArmBone = `${smokingArm}UpperArm` as VRMHumanBoneName\n      const lowerArmBone = `${smokingArm}LowerArm` as VRMHumanBoneName\n      const handBone = `${smokingArm}Hand` as VRMHumanBoneName\n      const shoulderBone = `${smokingArm}Shoulder` as VRMHumanBoneName\n\n      // Shoulder leads arm movement (anticipation)\n      if (rig.hasBone(shoulderBone)) {\n        rig.setRotation(shoulderBone, quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handSide * smoothArmY * 0.15))\n      }\n\n      // Upper arm rotation\n      if (rig.hasBone(upperArmBone)) {\n        const upperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, smoothArmX)\n        upperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -handSide * smoothArmY))\n        upperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, smoothArmZ))\n        rig.setRotation(upperArmBone, upperArmRot)\n      }\n\n      // Lower arm (elbow bend)\n      if (rig.hasBone(lowerArmBone)) {\n        rig.setRotation(lowerArmBone, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, handSide * elbowBend))\n      }\n\n      // Hand/wrist\n      if (rig.hasBone(handBone)) {\n        const handRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, smoothWrist)\n        // Wrist extension when holding cigarette up\n        const wristExtension = armUpEnv * 0.15\n        handRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, wristExtension))\n        rig.setRotation(handBone, handRot)\n      }\n\n      // Apply cigarette grip to smoking hand\n      if (rig.hasBone(handBone)) {\n        applyCigaretteGrip(rig, smokingArm as 'left' | 'right', 'between')\n      }\n\n      // Support arm - relaxed or crossed depending on style\n      const supportUpperArm = `${supportArm}UpperArm` as VRMHumanBoneName\n      const supportLowerArm = `${supportArm}LowerArm` as VRMHumanBoneName\n      const supportHand = `${supportArm}Hand` as VRMHumanBoneName\n\n      if (smokingStyle === 'seductive') {\n        // Arm crossed under (one arm supporting the other)\n        if (rig.hasBone(supportUpperArm)) {\n          const supportRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.4 * intensity)\n          supportRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handSide * 0.3 * intensity))\n          rig.setRotation(supportUpperArm, supportRot)\n        }\n        if (rig.hasBone(supportLowerArm)) {\n          rig.setRotation(supportLowerArm, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -handSide * 1.2 * intensity))\n        }\n      } else {\n        // Relaxed at side\n        if (rig.hasBone(supportUpperArm)) {\n          rig.setRotation(supportUpperArm, quatFromAxisAngle({ x: 0, y: 0, z: 1 }, handSide * 0.08))\n        }\n        if (rig.hasBone(supportLowerArm)) {\n          rig.setRotation(supportLowerArm, quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -handSide * 0.15))\n        }\n      }\n\n      // Relaxed fingers on support hand\n      if (rig.hasBone(supportHand)) {\n        applyFingerCurl(rig, supportArm as 'left' | 'right', {\n          thumb: 0.3,\n          index: 0.35,\n          middle: 0.4,\n          ring: 0.45,\n          little: 0.5,\n        })\n      }\n\n      // ========================================\n      // LAYER 3: BREATHING (PHASE ENVELOPES)\n      // ========================================\n\n      // Chest expansion based on inhale/hold/exhale envelopes\n      // Inhale expands, hold maintains, exhale contracts\n      const breathExpansion = (inhaleEnv + holdEnv) * inhaleDepth * 0.06\n        - exhaleEnv * inhaleDepth * 0.04 // Exhale slower than inhale deflates\n\n      // Add baseline breathing when not in puff cycle\n      const idleBreath = (1 - Math.max(inhaleEnv, holdEnv, exhaleEnv)) * oscBreathing(t, 0.2, 0.3) * 0.02 * intensity\n\n      state.chestSpring.setTarget(breathExpansion + idleBreath)\n      state.chestSpring.update(dt)\n      const smoothChest = state.chestSpring.value\n\n      if (rig.hasBone('chest')) {\n        rig.setRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -smoothChest))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -smoothChest * 1.5))\n      }\n\n      // Shoulders rise during inhale (delayed 0.1s / ~1% of cycle)\n      const shoulderRise = inhaleEnv * 0.02 * inhaleDepth\n      if (rig.hasBone('leftShoulder')) {\n        rig.addRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderRise))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.addRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderRise))\n      }\n\n      // ========================================\n      // LAYER 4: HEAD MOVEMENT\n      // ========================================\n\n      // Head tilts based on phase\n      // Inhale: slight tilt back\n      // Exhale: head up/forward to blow smoke\n      const inhaleTilt = inhaleEnv * -0.05 * intensity\n      const exhaleTilt = exhaleEnv * 0.03 * intensity\n      const headTiltX = inhaleTilt + exhaleTilt\n\n      // Head turns slightly during exhale\n      const headTiltY = exhaleEnv * handSide * 0.02 * intensity\n\n      // Idle micro-movements when not in puff\n      const idleHeadX = (1 - armUpEnv) * noise.noise2D(t * 0.2, 600) * 0.02 * intensity\n      const idleHeadY = (1 - armUpEnv) * noise.noise2D(t * 0.15, 700) * 0.025 * intensity\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX + idleHeadX)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, headTiltY + idleHeadY))\n        rig.setRotation('head', headRot)\n      }\n      if (rig.hasBone('neck')) {\n        rig.setRotation('neck', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, (headTiltX + idleHeadX) * 0.4))\n      }\n\n      // ========================================\n      // LAYER 5: EYES\n      // ========================================\n\n      if (eyeSquint) {\n        // Eyes squint during inhale and hold\n        // 0.15 rad = 8.6° (subtle narrowing)\n        const eyeSquintAmount = (inhaleEnv + holdEnv * 0.8) * 0.15 * intensity\n\n        if (rig.hasBone('leftEye')) {\n          rig.setRotation('leftEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeSquintAmount))\n        }\n        if (rig.hasBone('rightEye')) {\n          rig.setRotation('rightEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeSquintAmount))\n        }\n      }\n\n      // Blinking (seeded for determinism)\n      const blinkChance = noise.noise2D(t * 0.3, 800)\n      if (!state.isBlinking && blinkChance > 0.95) {\n        state.isBlinking = true\n        state.blinkTimer = 0\n      }\n\n      if (state.isBlinking) {\n        state.blinkTimer += dt\n        if (state.blinkTimer > 0.15) {\n          state.isBlinking = false\n        } else {\n          const blinkProgress = Math.sin(state.blinkTimer / 0.15 * Math.PI)\n          if (rig.hasBone('leftEye')) {\n            rig.addRotation('leftEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -blinkProgress * 0.25))\n          }\n          if (rig.hasBone('rightEye')) {\n            rig.addRotation('rightEye', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -blinkProgress * 0.25))\n          }\n        }\n      }\n\n      // ========================================\n      // LAYER 6: JAW (EXHALE)\n      // ========================================\n\n      if (jawAnimation && rig.hasBone('jaw')) {\n        // Jaw opens during exhale (peak at 50% of exhale)\n        // 0.08 rad = 4.6° opening\n        const jawOpen = Math.sin(exhaleEnv * Math.PI) * 0.08 * intensity\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawOpen))\n      }\n\n      // ========================================\n      // LAYER 7: LEGS & WEIGHT\n      // ========================================\n\n      // Relaxed stance with subtle weight shifting\n      const weightShift = noise.noise2D(t * 0.1, 900) * 0.03 * intensity\n\n      if (rig.hasBone('leftUpperLeg')) {\n        rig.setRotation('leftUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 + weightShift))\n      }\n      if (rig.hasBone('rightUpperLeg')) {\n        rig.setRotation('rightUpperLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03 - weightShift))\n      }\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.05))\n      }\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.08))\n      }\n\n      // Feet slightly turned out for relaxed stance\n      if (rig.hasBone('leftFoot')) {\n        rig.setRotation('leftFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.1))\n      }\n      if (rig.hasBone('rightFoot')) {\n        rig.setRotation('rightFoot', quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.15))\n      }\n    },\n  }\n}\n\nexport const smokingCigarette: MotionProgram<SmokingCigaretteParams> = createSmokingCigarette({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Missing end-effector targeting / IK for hand-to-mouth. The implementation only blends joint rotations (shoulder/upperArm/lowerArm/wrist) to approximate a reach. The block's output constraints explicitly require the hand MUST reach the mouth accurately and state that this requires IK or precise targeting. Without IK or an explicit end-effector solver, the hand position can drift and will not reliably meet the 'must reach mouth accurately' requirement across different rigs.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Phase envelopes are present but there are no explicit per-joint staggered phase envelopes or timing offsets for shoulder → elbow → wrist. The code uses shared armUpEnv and spring smoothing to create coordination, but the domain rules require overlapping phase envelopes for each body part with shoulder leading and wrist trailing by specified offsets (e.g., ~80ms/50ms/30ms). This is only implicitly approximated by springs rather than explicit enveloping.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Fingers/grip lack explicit continuous micro-adjustments across inhale/hold/exhale phases. applyCigaretteGrip is applied (good) but the code does not drive subtle phase-dependent finger adjustments beyond a static grip helper call. The output constraints expect 'FINGERS: Subtle adjustments throughout' during the puff cycle.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"No explicit anatomical joint limit/clamping enforcement. The domain rules require joint rotations to respect anatomical limits. While the numeric choices appear reasonable, there are no clamps or checks to guarantee safety across different rigs/scale factors.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    },\n    {\n      \"message\": \"Per-joint phase envelopes (shoulder/elbow/wrist) are not separately declared. The PHASE_TIMING table defines high-level phases (armRaise, inhale, etc.) but the implementation does not expose or use distinct timed envelopes for shoulder/elbow/wrist which the domain philosophy and block constraints emphasize.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/smoking-cigarette.ts\"\n    }\n  ],\n  \"summary\": \"The block is well-documented and largely aligns with the Blocks motion philosophy: it uses phase envelopes (getPhaseEnvelope), springs, noise, hasBone() guards, breath coupling, style parameters, and helpers for grip. However it fails a critical domain requirement: it does not implement end-effector targeting or IK to guarantee the hand reaches the mouth accurately (an explicit output constraint). Additionally, the implementation relies on spring smoothing rather than explicit per-joint staggered phase envelopes and lacks explicit joint limit enforcement and fine continuous finger micro-adjustments. Fixing those issues (add IK/targeting for the hand, add per-joint phase envelopes or explicit timing offsets for shoulder→elbow→wrist, clamp joint rotations, and drive finger micro-motions through the phases) would bring the block into compliance.\"\n}",
            "tokensUsed": {
              "input": 8236,
              "output": 2706
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "seductive_walk",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/seductive-walk.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "seductive_walk",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe runway walk biomechanics",
                    "DOCUMENTATION: Must specify timing delays between body parts",
                    "GAIT: Full cycle with stance/swing phases per leg",
                    "GAIT: Crossover step pattern (feet cross midline)",
                    "HIPS: Exaggerated lateral sway, hip drop on swing side",
                    "HIPS: Forward pelvic tilt for posture",
                    "HIPS: Twist opposite to shoulders",
                    "SPINE: Counter-rotation balances hip sway",
                    "SPINE: S-curve through spine chain",
                    "SHOULDERS: Counter-rotate opposite to hips",
                    "SHOULDERS: Drop with arm swing",
                    "ARMS: Fluid swing with DELAY (secondary motion)",
                    "ARMS: Wrist trails elbow",
                    "HANDS: Graceful, slightly spread fingers",
                    "HANDS: Wrist rotation adds elegance",
                    "LEGS: Full hip/knee/ankle coordination",
                    "LEGS: Toe point during swing phase",
                    "FEET: Heel-toe roll, slight turn out",
                    "TOES: Point during swing, grip during stance",
                    "HEAD: Stable (vestibular reflex) with slight tilt",
                    "HEAD: Can include flirtatious glances",
                    "EYES: Confident forward gaze, slow blinks",
                    "BREATH: Couples to gait cycle",
                    "WEIGHT: Visible transfer, hip bounce"
                  ]
                }
              ],
              "description": "Runway-style walk with exaggerated hip sway and fluid full-body coordination.\nEvery step is a performance. Weight transfer is deliberate and visible.\n\nFEEL: Confident, alluring, aware of being watched. Like a model on runway.\n\nGAIT CYCLE TIMING:\n- Hips lead the motion (lateral sway peaks at mid-stance)\n- Spine counter-rotates to balance hip sway\n- Shoulders counter-rotate opposite to hips\n- Arms swing opposite to legs with DELAY (secondary motion)\n- Wrists trail arms, fingers trail wrists\n- Head stays relatively stable (vestibular reflex)\n- Eyes can flirt with \"audience\"\n\nCROSSOVER: Feet cross midline for exaggerated hip motion.\nEach step places foot slightly past center.\n",
              "path": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Runway-style walk with exaggerated hip sway and fl...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 57374,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Computed hipDrop value is never applied to the hips rotation/translation. The code calculates hipDrop but does not use it when composing the hips quaternion or position, so the required 'HIPS: Hip drop on swing side' behavior is not implemented.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Shoulder counter-rotation is not actually applied. The code computes shoulderTwist (via a spring) but never uses it to rotate the shoulder bones around the yaw axis; only a shoulder drop is applied. This violates the 'SHOULDERS: Counter-rotate opposite to hips' constraint.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Breath coupling to the gait cycle is not implemented. The output constraints require 'BREATH: Couples to gait cycle' but there is no chest/upperChest breathing oscillation or explicit breath phase coupling (only a small constant chest forward tilt is present).",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "Documentation contains a contradictory statement about vestibular compensation: the RESEARCH BASIS claims ~70% compensation while TIMING RELATIONSHIPS / implementation use 30% (head compensation = -hipSway * 0.3). This inconsistency should be resolved so the documented target matches implementation and research citations.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The code does not clamp joint rotations to anatomical limits or query rig-specific joint limits. The domain rule 'Joint rotations must respect anatomical limits' is not enforced programmatically — rotations are small by design but there is no explicit guarding against out-of-range values.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The block claims to 'consider ALL 69 VRM bones' in documentation, but not all face bones (jaw, eyebrows, lips), ear bones, or other VRM-specific bones are referenced or handled. While many core and limb bones are covered (with hasBone guards), the file does not explicitly consider a full VRM bone list and may skip expressive face bones that could be important for the motion.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "The documentation states 'wrist lags arm by ~0.3 radians phase' (a phase shift), but the implementation models wrist lag as an amplitude/proportional trailing (leftWristLag = leftArmSwing * 0.3) rather than a phase-shifted oscillator. This is a behavioral mismatch between spec and implementation.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "applyFingerCurl/applyFingerSpread are invoked without surrounding hasBone checks for per-finger bones. The file claims 'All bone access wrapped in hasBone() checks' — that statement is not strictly true at the callsite (though the helper may internally guard). Consider adding explicit guards or verifying the helpers internally perform the necessary hasBone checks.",
              "file": "packages/motion-dsl/src/motions/seductive-walk.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "seductive-walk.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block is well-documented and broadly expresses the intended seductive runway walk (detailed TIMING, research citations, seeded noise, gait-phase math, spring-based smoothing, finger helpers, toe-pointing, eye/blink behavior). However, it fails several required domain rules: the hip drop calculation is computed but not applied, shoulder counter-rotation is computed but not used, and breath coupling to the gait cycle is missing — these are flagged as errors because they violate explicit output constraints. Additional warnings call out a documentation/implementation mismatch for head compensation, lack of explicit joint-limit clamping, incomplete consideration of all VRM bones, a spec vs implementation mismatch for wrist lag (phase vs amplitude), and an unguarded direct call to finger helpers (helper may hide guards). Fixing the three errors and addressing the warnings will bring the block into strong compliance with the domain rules."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: seductive_walk\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Runway-style walk with exaggerated hip sway and fluid full-body coordination.\\nEvery step is a performance. Weight transfer is deliberate and visible.\\n\\nFEEL: Confident, alluring, aware of being watched. Like a model on runway.\\n\\nGAIT CYCLE TIMING:\\n- Hips lead the motion (lateral sway peaks at mid-stance)\\n- Spine counter-rotates to balance hip sway\\n- Shoulders counter-rotate opposite to hips\\n- Arms swing opposite to legs with DELAY (secondary motion)\\n- Wrists trail arms, fingers trail wrists\\n- Head stays relatively stable (vestibular reflex)\\n- Eyes can flirt with \\\"audience\\\"\\n\\nCROSSOVER: Feet cross midline for exaggerated hip motion.\\nEach step places foot slightly past center.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/seductive-walk.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe runway walk biomechanics\",\n        \"DOCUMENTATION: Must specify timing delays between body parts\",\n        \"GAIT: Full cycle with stance/swing phases per leg\",\n        \"GAIT: Crossover step pattern (feet cross midline)\",\n        \"HIPS: Exaggerated lateral sway, hip drop on swing side\",\n        \"HIPS: Forward pelvic tilt for posture\",\n        \"HIPS: Twist opposite to shoulders\",\n        \"SPINE: Counter-rotation balances hip sway\",\n        \"SPINE: S-curve through spine chain\",\n        \"SHOULDERS: Counter-rotate opposite to hips\",\n        \"SHOULDERS: Drop with arm swing\",\n        \"ARMS: Fluid swing with DELAY (secondary motion)\",\n        \"ARMS: Wrist trails elbow\",\n        \"HANDS: Graceful, slightly spread fingers\",\n        \"HANDS: Wrist rotation adds elegance\",\n        \"LEGS: Full hip/knee/ankle coordination\",\n        \"LEGS: Toe point during swing phase\",\n        \"FEET: Heel-toe roll, slight turn out\",\n        \"TOES: Point during swing, grip during stance\",\n        \"HEAD: Stable (vestibular reflex) with slight tilt\",\n        \"HEAD: Can include flirtatious glances\",\n        \"EYES: Confident forward gaze, slow blinks\",\n        \"BREATH: Couples to gait cycle\",\n        \"WEIGHT: Visible transfer, hip bounce\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- seductive-walk.ts ---\n```\n/**\n * ============================================================================\n * SEDUCTIVE WALK MOTION\n * ============================================================================\n *\n * Runway-style walking with exaggerated hip sway, fluid arm movement,\n * and confident head carriage. Full gait cycle with crossover step pattern.\n *\n * ============================================================================\n * HOW THIS MOTION SHOULD FEEL\n * ============================================================================\n *\n * Think of a supermodel on a Milan runway. Each step is deliberate, the hips\n * swinging in an exaggerated figure-8 pattern. The legs cross slightly in\n * front of each other (crossover step) creating that distinctive catwalk\n * line. The arms flow gracefully, slightly behind the beat of the legs.\n * The head stays remarkably stable despite the body movement below - a\n * testament to the vestibular-ocular reflex keeping the gaze steady.\n *\n * The key quality is FLOW. Nothing is jerky or mechanical. The hip leads,\n * the spine follows with counter-rotation, the shoulders oppose the hips,\n * and the arms trail behind with elegant follow-through. Even the fingers\n * are slightly spread in that characteristic model pose.\n *\n * ============================================================================\n * TIMING RELATIONSHIPS\n * ============================================================================\n *\n * GAIT CYCLE: 1.25 seconds at default speed (0.8 Hz)\n *   - Right heel strike: 0% of cycle\n *   - Right midstance: 25% (peak weight on right leg)\n *   - Left heel strike: 50% of cycle\n *   - Left midstance: 75% (peak weight on left leg)\n *\n * HIP MOVEMENT:\n *   - Lateral sway: peaks at midstance (25%, 75%)\n *   - Forward/back tilt: 2x frequency (peaks at 12.5%, 37.5%, 62.5%, 87.5%)\n *   - Rotation (twist): same phase as sway but opposite direction\n *   - Drop: peaks during swing phase (opposite to sway)\n *\n * SPINE COUNTER-ROTATION:\n *   - Spine: 50% counter to hip twist\n *   - Chest: 80% counter to hip twist\n *   - Upper chest: 100% counter (shoulders oppose hips)\n *\n * ARM SWING:\n *   - Phase: opposite to same-side leg (arm forward when leg back)\n *   - Secondary motion: wrist lags arm by ~0.3 radians phase\n *   - Fingers: subtle curl variation at 0.2 Hz\n *\n * LEG MOVEMENT:\n *   - Swing phase: 40% of gait cycle per leg\n *   - Peak knee bend: 60% into swing phase\n *   - Crossover: 5% adduction creates line\n *\n * HEAD STABILIZATION:\n *   - Compensates 30% of hip sway\n *   - Spring-driven (stiffness 180) for natural lag\n *   - Slight constant tilt for attitude\n *\n * ============================================================================\n * BONE HANDLING\n * ============================================================================\n *\n * All bone access wrapped in hasBone() checks.\n *\n * CORE: hips (with translation), spine, chest, upperChest, neck, head\n * ARMS: leftShoulder, rightShoulder, leftUpperArm, rightUpperArm,\n *       leftLowerArm, rightLowerArm, leftHand, rightHand\n * FINGERS: all 30 bones via applyFingerCurl/applyFingerSpread helpers\n * LEGS: leftUpperLeg, rightUpperLeg, leftLowerLeg, rightLowerLeg\n * FEET: leftFoot, rightFoot, leftToes, rightToes\n * FACE: leftEye, rightEye\n *\n * ============================================================================\n * RESEARCH BASIS\n * ============================================================================\n *\n * - Whittle, M. (2007): \"Gait Analysis: An Introduction\" - Normal gait cycle\n *   timing, stance/swing phase ratios, hip/knee kinematics.\n *\n * - Murray, M.P. et al. (1970): \"Walking patterns of normal women\" - Hip\n *   rotation 8-12° total, lateral trunk sway increases with slower walking.\n *\n * - Runway studies show exaggerated hip rotation (15-20°), crossover step\n *   pattern, and deliberate arm placement.\n *\n * - Vestibular-ocular reflex: Head stabilization compensates ~70% of body\n *   motion for gaze stability.\n *\n * ============================================================================\n * NUMERICAL JUSTIFICATIONS\n * ============================================================================\n *\n * speed 0.8 Hz: Slower than normal walk (1.0-1.2 Hz) for runway effect\n * hipSwayAmount 0.8: Exaggerated from normal 0.3-0.4\n * hipSway 0.12 rad: 7° lateral sway (2x normal)\n * hipTwist 0.08 rad: 4.6° rotation per side (within normal range)\n * crossoverStep 0.05 rad: 2.9° adduction for crossover line\n * armFlowiness 0.7: Secondary motion amplitude\n * headTilt 0.08 rad: 4.6° constant attitude tilt\n * springStiffness 150-180: Moderate lag for organic feel\n */\n\nimport { z } from 'zod'\nimport { Vector3 } from 'three'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  walkPhase,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  applyFingerCurl,\n  applyFingerSpread,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const seductiveWalkParamsSchema = z.object({\n  /** Overall intensity of exaggeration (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Walking speed (steps per second). Default: 0.8 */\n  speed: z.number().min(0.3).max(2).default(0.8),\n  /** Amount of hip sway (0-1). Default: 0.8 */\n  hipSwayAmount: z.number().min(0).max(1).default(0.8),\n  /** Fluidity of arm movement (0-1). Default: 0.7 */\n  armFlowiness: z.number().min(0).max(1).default(0.7),\n  /** Head tilt angle (0-1). Default: 0.3 */\n  headTilt: z.number().min(0).max(1).default(0.3),\n  /** Enable crossover step pattern. Default: true */\n  crossoverStep: z.boolean().default(true),\n  /** Enable toe point during swing. Default: true */\n  toePoint: z.boolean().default(true),\n  /** Enable secondary motion (follow-through). Default: true */\n  secondaryMotion: z.boolean().default(true),\n  /** Enable finger movement. Default: true */\n  fingerAnimation: z.boolean().default(true),\n})\n\nexport type SeductiveWalkParams = z.infer<typeof seductiveWalkParamsSchema>\nexport type SeductiveWalkInput = z.input<typeof seductiveWalkParamsSchema>\n\nexport const seductiveWalkMeta: MotionMeta = {\n  id: 'seductive-walk',\n  name: 'Seductive Walk',\n  description: 'Runway-style walk with exaggerated hip sway and fluid arm movement',\n  tags: ['walk', 'locomotion', 'seductive', 'runway'],\n  author: 'posers',\n}\n\n// ============================================================================\n// GAIT CYCLE HELPERS\n// ============================================================================\n\n/**\n * Gait cycle phases (0-1):\n * 0.0 - 0.1: Right heel strike / Left toe off\n * 0.1 - 0.3: Right loading response\n * 0.3 - 0.5: Right midstance / Left swing\n * 0.5 - 0.6: Left heel strike / Right toe off\n * 0.6 - 0.8: Left loading response\n * 0.8 - 1.0: Left midstance / Right swing\n */\n\nfunction getGaitPhase(t: number, speed: number): number {\n  const cycleTime = 1 / speed\n  return (t / cycleTime) % 1\n}\n\nfunction legSwingCurve(phase: number): number {\n  // Smooth leg swing with acceleration\n  return Easing.easeInOutSine(phase)\n}\n\nfunction hipDropCurve(phase: number): number {\n  // Hip drops during swing phase, rises during stance\n  return Math.sin(phase * Math.PI * 2)\n}\n\nfunction armSwingCurve(phase: number, flowiness: number): number {\n  // Fluid arm swing with follow-through\n  const base = Math.sin(phase * Math.PI * 2)\n  const secondary = Math.sin((phase * Math.PI * 2) - 0.3) * 0.2 * flowiness\n  return base + secondary\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface SeductiveWalkState {\n  noise: NoiseGenerator\n  hipSpring: Spring\n  shoulderSpring: Spring\n  headSpring: Spring\n  leftArmSpring: Spring\n  rightArmSpring: Spring\n  blinkTimer: number\n  isBlinking: boolean\n}\n\n/**\n * Initialize deterministic state with seeded random generators.\n * The noise generator uses the seed to ensure identical output\n * for the same seed value across all calls.\n */\nfunction initState(seed: number): SeductiveWalkState {\n  // Noise generator is seeded for deterministic, reproducible motion\n  return {\n    noise: createNoiseGenerator(seed),\n    hipSpring: createSpring({ stiffness: 150, damping: 15 }),\n    shoulderSpring: createSpring({ stiffness: 120, damping: 12 }),\n    headSpring: createSpring({ stiffness: 180, damping: 20 }),\n    leftArmSpring: createSpring({ stiffness: 80, damping: 10 }),\n    rightArmSpring: createSpring({ stiffness: 80, damping: 10 }),\n    blinkTimer: 0,\n    isBlinking: false,\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a seductive walk motion with full gait cycle implementation.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (toes, eyes, fingers) are gracefully skipped\n * - The animation degrades gracefully when bones are unavailable\n * - Core bones (hips, spine, legs) are all checked before use\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - Gait phase is purely time-based with no random elements\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - Spring physics use efficient semi-implicit Euler integration\n * - Gait phase calculations are simple trigonometry\n * - No allocations in hot path except quaternion creation\n */\nexport function createSeductiveWalk(params: SeductiveWalkInput = {}): MotionProgram<SeductiveWalkParams> {\n  const validatedParams = seductiveWalkParamsSchema.parse(params)\n  let state: SeductiveWalkState | null = null\n\n  return {\n    meta: seductiveWalkMeta,\n    paramsSchema: seductiveWalkParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        intensity,\n        speed,\n        hipSwayAmount,\n        armFlowiness,\n        headTilt,\n        crossoverStep,\n        toePoint,\n        secondaryMotion,\n        fingerAnimation,\n      } = validatedParams\n\n      const noise = state.noise\n      const gaitPhase = getGaitPhase(t, speed)\n\n      // Which leg is in stance phase\n      const rightStance = gaitPhase < 0.5\n      const leftStance = !rightStance\n\n      // Swing phase for each leg (0-1 during their swing)\n      const rightSwingPhase = rightStance ? 0 : (gaitPhase - 0.5) * 2\n      const leftSwingPhase = leftStance ? 0 : gaitPhase * 2\n\n      // ========================================\n      // LAYER 1: HIP MOVEMENT (CORE OF THE WALK)\n      // ========================================\n\n      // Hip sway - lateral movement\n      const hipSwayTarget = Math.sin(gaitPhase * Math.PI * 2) * hipSwayAmount * 0.12 * intensity\n      state.hipSpring.setTarget(hipSwayTarget)\n      state.hipSpring.update(dt)\n      const hipSway = state.hipSpring.value\n\n      // Hip rotation (twist) - counter to shoulders\n      const hipTwist = Math.sin(gaitPhase * Math.PI * 2) * 0.08 * intensity\n\n      // Hip drop on swing side\n      const hipDrop = hipDropCurve(gaitPhase) * 0.05 * intensity\n\n      // Forward hip oscillation (pelvic tilt during gait)\n      const hipForward = Math.sin(gaitPhase * Math.PI * 4) * 0.02 * intensity\n\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, hipSway)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, hipTwist))\n        hipsRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, hipForward))\n        rig.setRotation('hips', hipsRot)\n\n        // Hip translation for bounce\n        const hipBounce = Math.abs(Math.sin(gaitPhase * Math.PI * 2)) * 0.01 * intensity\n        rig.setHipsPositionOffset(new Vector3(hipSway * 0.1, hipBounce, 0))\n      }\n\n      // ========================================\n      // LAYER 2: SPINE COUNTER-ROTATION\n      // ========================================\n\n      // Spine opposes hip movement for balance\n      const spineCounter = -hipTwist * 0.4\n      const spineSway = -hipSway * 0.3\n\n      if (rig.hasBone('spine')) {\n        const spineRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter * 0.5)\n        spineRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.5))\n        rig.setRotation('spine', spineRot)\n      }\n\n      if (rig.hasBone('chest')) {\n        const chestRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter * 0.8)\n        chestRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.3))\n        // Slight chest forward for confidence\n        chestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n        rig.setRotation('chest', chestRot)\n      }\n\n      if (rig.hasBone('upperChest')) {\n        const upperChestRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, spineCounter)\n        upperChestRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, spineSway * 0.2))\n        upperChestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.04 * intensity))\n        rig.setRotation('upperChest', upperChestRot)\n      }\n\n      // ========================================\n      // LAYER 3: SHOULDER MOVEMENT\n      // ========================================\n\n      // Shoulders counter-rotate to hips\n      const shoulderTarget = -hipTwist * 0.6\n      state.shoulderSpring.setTarget(shoulderTarget)\n      state.shoulderSpring.update(dt)\n      const shoulderTwist = state.shoulderSpring.value\n\n      // Shoulder drop with arm swing\n      const leftShoulderDrop = armSwingCurve(gaitPhase, armFlowiness) * 0.02 * intensity\n      const rightShoulderDrop = armSwingCurve(gaitPhase + 0.5, armFlowiness) * 0.02 * intensity\n\n      if (rig.hasBone('leftShoulder')) {\n        const leftShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -leftShoulderDrop)\n        rig.setRotation('leftShoulder', leftShoulderRot)\n      }\n\n      if (rig.hasBone('rightShoulder')) {\n        const rightShoulderRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, rightShoulderDrop)\n        rig.setRotation('rightShoulder', rightShoulderRot)\n      }\n\n      // ========================================\n      // LAYER 4: ARM SWING\n      // ========================================\n\n      // Fluid arm swing with secondary motion\n      const leftArmSwingBase = armSwingCurve(gaitPhase, armFlowiness) * 0.35 * intensity * armFlowiness\n      const rightArmSwingBase = armSwingCurve(gaitPhase + 0.5, armFlowiness) * 0.35 * intensity * armFlowiness\n\n      // Apply spring for smoothness\n      state.leftArmSpring.setTarget(leftArmSwingBase)\n      state.rightArmSpring.setTarget(rightArmSwingBase)\n      state.leftArmSpring.update(dt)\n      state.rightArmSpring.update(dt)\n\n      const leftArmSwing = state.leftArmSpring.value\n      const rightArmSwing = state.rightArmSpring.value\n\n      // Secondary motion - wrist lag\n      const leftWristLag = secondaryMotion ? leftArmSwing * 0.3 : 0\n      const rightWristLag = secondaryMotion ? rightArmSwing * 0.3 : 0\n\n      if (rig.hasBone('leftUpperArm')) {\n        const leftUpperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftArmSwing)\n        leftUpperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.08 * intensity))\n        rig.setRotation('leftUpperArm', leftUpperArmRot)\n      }\n\n      if (rig.hasBone('rightUpperArm')) {\n        const rightUpperArmRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightArmSwing)\n        rightUpperArmRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.08 * intensity))\n        rig.setRotation('rightUpperArm', rightUpperArmRot)\n      }\n\n      // Lower arms - slight bend with secondary motion\n      if (rig.hasBone('leftLowerArm')) {\n        const leftLowerRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.1 - leftWristLag * 0.2)\n        rig.setRotation('leftLowerArm', leftLowerRot)\n      }\n\n      if (rig.hasBone('rightLowerArm')) {\n        const rightLowerRot = quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.1 + rightWristLag * 0.2)\n        rig.setRotation('rightLowerArm', rightLowerRot)\n      }\n\n      // Wrists - graceful rotation\n      if (rig.hasBone('leftHand')) {\n        const leftHandRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, 0.1 + leftWristLag * 0.15)\n        leftHandRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftWristLag * 0.1))\n        rig.setRotation('leftHand', leftHandRot)\n      }\n\n      if (rig.hasBone('rightHand')) {\n        const rightHandRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -0.1 - rightWristLag * 0.15)\n        rightHandRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightWristLag * 0.1))\n        rig.setRotation('rightHand', rightHandRot)\n      }\n\n      // ========================================\n      // LAYER 5: LEG MOVEMENT\n      // ========================================\n\n      // Right leg\n      const rightLegForward = rightStance\n        ? -0.1 * (gaitPhase * 2) * intensity // Pushing back during stance\n        : Math.sin(rightSwingPhase * Math.PI) * 0.4 * intensity // Swinging forward\n\n      const rightLegAbduction = crossoverStep\n        ? Math.sin(gaitPhase * Math.PI * 2 + Math.PI) * 0.05 * intensity // Crossover\n        : 0\n\n      if (rig.hasBone('rightUpperLeg')) {\n        const rightUpperLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightLegForward)\n        rightUpperLegRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, rightLegAbduction))\n        rig.setRotation('rightUpperLeg', rightUpperLegRot)\n      }\n\n      // Right knee bend\n      const rightKneeBend = rightStance\n        ? 0.08 * intensity // Slight bend in stance\n        : Math.sin(rightSwingPhase * Math.PI) * 0.6 * intensity // Bend during swing\n\n      if (rig.hasBone('rightLowerLeg')) {\n        rig.setRotation('rightLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightKneeBend))\n      }\n\n      // Left leg\n      const leftLegForward = leftStance\n        ? -0.1 * ((gaitPhase - 0.5) * 2) * intensity\n        : Math.sin(leftSwingPhase * Math.PI) * 0.4 * intensity\n\n      const leftLegAbduction = crossoverStep\n        ? Math.sin(gaitPhase * Math.PI * 2) * 0.05 * intensity\n        : 0\n\n      if (rig.hasBone('leftUpperLeg')) {\n        const leftUpperLegRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftLegForward)\n        leftUpperLegRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, leftLegAbduction))\n        rig.setRotation('leftUpperLeg', leftUpperLegRot)\n      }\n\n      const leftKneeBend = leftStance\n        ? 0.08 * intensity\n        : Math.sin(leftSwingPhase * Math.PI) * 0.6 * intensity\n\n      if (rig.hasBone('leftLowerLeg')) {\n        rig.setRotation('leftLowerLeg', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftKneeBend))\n      }\n\n      // ========================================\n      // LAYER 6: FEET & TOES\n      // ========================================\n\n      // Right foot\n      const rightFootAngle = rightStance\n        ? -0.1 * intensity // Flat/slight heel strike\n        : (toePoint ? -0.3 * Math.sin(rightSwingPhase * Math.PI) * intensity : 0) // Toe point during swing\n\n      if (rig.hasBone('rightFoot')) {\n        const rightFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, rightFootAngle)\n        rightFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, 0.05)) // Slight turn out\n        rig.setRotation('rightFoot', rightFootRot)\n      }\n\n      if (rig.hasBone('rightToes')) {\n        if (rightStance) {\n          // Toes grip during stance for balance and push-off\n          // 0.15 rad = 8.6° flexion for grip\n          rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.15 * intensity))\n        } else if (toePoint) {\n          // Toes point during swing for elegance\n          const rightToePoint = Math.sin(rightSwingPhase * Math.PI) * 0.4 * intensity\n          rig.setRotation('rightToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -rightToePoint))\n        }\n      }\n\n      // Left foot\n      const leftFootAngle = leftStance\n        ? -0.1 * intensity\n        : (toePoint ? -0.3 * Math.sin(leftSwingPhase * Math.PI) * intensity : 0)\n\n      if (rig.hasBone('leftFoot')) {\n        const leftFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, leftFootAngle)\n        leftFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -0.05))\n        rig.setRotation('leftFoot', leftFootRot)\n      }\n\n      if (rig.hasBone('leftToes')) {\n        if (leftStance) {\n          // Toes grip during stance for balance and push-off\n          // 0.15 rad = 8.6° flexion for grip\n          rig.setRotation('leftToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.15 * intensity))\n        } else if (toePoint) {\n          // Toes point during swing for elegance\n          const leftToePoint = Math.sin(leftSwingPhase * Math.PI) * 0.4 * intensity\n          rig.setRotation('leftToes', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -leftToePoint))\n        }\n      }\n\n      // ========================================\n      // LAYER 7: HEAD (STABLE WITH TILT)\n      // ========================================\n\n      // Head stays relatively stable (vestibular reflex)\n      const headCompensation = -hipSway * 0.3\n      const headTiltAmount = headTilt * 0.08 * intensity\n\n      // Subtle look direction\n      const lookDirection = noise.noise2D(t * 0.1, 100) * 0.05 * intensity\n\n      state.headSpring.setTarget(headCompensation)\n      state.headSpring.update(dt)\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, state.headSpring.value + headTiltAmount)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, lookDirection))\n        // Slight chin up for confidence\n        headRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        const neckRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, state.headSpring.value * 0.3)\n        neckRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.02 * intensity))\n        rig.setRotation('neck', neckRot)\n      }\n\n      // ========================================\n      // LAYER 8: FINGERS\n      // ========================================\n\n      if (fingerAnimation) {\n        // Graceful, slightly spread fingers\n        const fingerCurl = 0.15 + noise.noise2D(t * 0.2, 200) * 0.05\n\n        applyFingerCurl(rig, 'left', {\n          thumb: fingerCurl * 0.4,\n          index: fingerCurl * 0.8,\n          middle: fingerCurl * 0.9,\n          ring: fingerCurl,\n          little: fingerCurl * 1.1,\n        })\n\n        applyFingerCurl(rig, 'right', {\n          thumb: fingerCurl * 0.4,\n          index: fingerCurl * 0.8,\n          middle: fingerCurl * 0.9,\n          ring: fingerCurl,\n          little: fingerCurl * 1.1,\n        })\n\n        // Elegant spread\n        applyFingerSpread(rig, 'left', 0.4 * intensity)\n        applyFingerSpread(rig, 'right', 0.4 * intensity)\n      }\n\n      // ========================================\n      // LAYER 9: EYES\n      // ========================================\n\n      // Confident forward gaze with subtle movement\n      const eyeX = noise.noise2D(t * 0.15, 300) * 0.03 * intensity\n      const eyeY = noise.noise2D(t * 0.12, 400) * 0.02 * intensity\n\n      // Blinking\n      const blinkChance = noise.noise2D(t * 0.25, 500)\n      if (!state.isBlinking && blinkChance > 0.95) {\n        state.isBlinking = true\n        state.blinkTimer = 0\n      }\n\n      let blinkAmount = 0\n      if (state.isBlinking) {\n        state.blinkTimer += dt\n        if (state.blinkTimer > 0.15) {\n          state.isBlinking = false\n        } else {\n          blinkAmount = Math.sin(state.blinkTimer / 0.15 * Math.PI) * 0.25\n        }\n      }\n\n      if (rig.hasBone('leftEye')) {\n        const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n        leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n        rig.setRotation('leftEye', leftEyeRot)\n      }\n\n      if (rig.hasBone('rightEye')) {\n        const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n        rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n        rig.setRotation('rightEye', rightEyeRot)\n      }\n    },\n  }\n}\n\nexport const seductiveWalk: MotionProgram<SeductiveWalkParams> = createSeductiveWalk({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"Computed hipDrop value is never applied to the hips rotation/translation. The code calculates hipDrop but does not use it when composing the hips quaternion or position, so the required 'HIPS: Hip drop on swing side' behavior is not implemented.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Shoulder counter-rotation is not actually applied. The code computes shoulderTwist (via a spring) but never uses it to rotate the shoulder bones around the yaw axis; only a shoulder drop is applied. This violates the 'SHOULDERS: Counter-rotate opposite to hips' constraint.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Breath coupling to the gait cycle is not implemented. The output constraints require 'BREATH: Couples to gait cycle' but there is no chest/upperChest breathing oscillation or explicit breath phase coupling (only a small constant chest forward tilt is present).\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"Documentation contains a contradictory statement about vestibular compensation: the RESEARCH BASIS claims ~70% compensation while TIMING RELATIONSHIPS / implementation use 30% (head compensation = -hipSway * 0.3). This inconsistency should be resolved so the documented target matches implementation and research citations.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"The code does not clamp joint rotations to anatomical limits or query rig-specific joint limits. The domain rule 'Joint rotations must respect anatomical limits' is not enforced programmatically — rotations are small by design but there is no explicit guarding against out-of-range values.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"The block claims to 'consider ALL 69 VRM bones' in documentation, but not all face bones (jaw, eyebrows, lips), ear bones, or other VRM-specific bones are referenced or handled. While many core and limb bones are covered (with hasBone guards), the file does not explicitly consider a full VRM bone list and may skip expressive face bones that could be important for the motion.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"The documentation states 'wrist lags arm by ~0.3 radians phase' (a phase shift), but the implementation models wrist lag as an amplitude/proportional trailing (leftWristLag = leftArmSwing * 0.3) rather than a phase-shifted oscillator. This is a behavioral mismatch between spec and implementation.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    },\n    {\n      \"message\": \"applyFingerCurl/applyFingerSpread are invoked without surrounding hasBone checks for per-finger bones. The file claims 'All bone access wrapped in hasBone() checks' — that statement is not strictly true at the callsite (though the helper may internally guard). Consider adding explicit guards or verifying the helpers internally perform the necessary hasBone checks.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/seductive-walk.ts\"\n    }\n  ],\n  \"summary\": \"The block is well-documented and broadly expresses the intended seductive runway walk (detailed TIMING, research citations, seeded noise, gait-phase math, spring-based smoothing, finger helpers, toe-pointing, eye/blink behavior). However, it fails several required domain rules: the hip drop calculation is computed but not applied, shoulder counter-rotation is computed but not used, and breath coupling to the gait cycle is missing — these are flagged as errors because they violate explicit output constraints. Additional warnings call out a documentation/implementation mismatch for head compensation, lack of explicit joint-limit clamping, incomplete consideration of all VRM bones, a spec vs implementation mismatch for wrist lag (phase vs amplitude), and an unguarded direct call to finger helpers (helper may hide guards). Fixing the three errors and addressing the warnings will bring the block into strong compliance with the domain rules.\"\n}",
            "tokensUsed": {
              "input": 8035,
              "output": 3549
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    },
    {
      "blockName": "contemplative_lean",
      "blockPath": "/Users/ajaxdavis/repos/lisa/posers/packages/motion-dsl/src/motions/contemplative-lean.ts",
      "hasErrors": true,
      "hasWarnings": true,
      "validators": [
        {
          "id": "schema.io",
          "label": "schema",
          "passed": true,
          "duration": 0,
          "issues": [],
          "context": {
            "filesAnalyzed": [
              "blocks.yml"
            ],
            "rulesApplied": [
              "input_schema_complete",
              "output_schema_complete",
              "block_has_description"
            ],
            "summary": "Schema validation passed. Validated 2 input(s) and 1 output(s). All schemas are complete with name and type fields.",
            "input": {
              "blockName": "contemplative_lean",
              "inputs": [
                {
                  "name": "rig",
                  "type": "entity.rig"
                },
                {
                  "name": "ctx",
                  "type": "entity.motion_context"
                }
              ],
              "outputs": [
                {
                  "name": "pose",
                  "type": "entity.motion_program",
                  "constraints": [
                    "DOCUMENTATION: Must describe contemplative body language",
                    "DOCUMENTATION: Must specify pose variants and their meanings",
                    "ASYMMETRY: Clear weight distribution (one leg dominant)",
                    "ASYMMETRY: Hip drop on non-weight side",
                    "ASYMMETRY: Spine S-curve to balance",
                    "VARIANTS: chin_rest, crossed_arms, akimbo, hand_on_hip",
                    "VARIANTS: Each has specific arm/hand configuration",
                    "BREATH: Deep, slow (5-6 second cycles)",
                    "BREATH: Visible chest expansion",
                    "GESTURES: Thinking micro-movements (chin tap, lip touch)",
                    "GESTURES: Very slow and deliberate",
                    "GESTURES: Phase envelopes for gesture timing",
                    "HANDS: Detailed finger pose per variant",
                    "HANDS: Subtle finger adjustments",
                    "LEGS: Weight leg straight-ish, relaxed leg bent",
                    "FEET: Weight foot flat, relaxed foot on ball",
                    "HEAD: Tilted slightly, distant gaze angle",
                    "EYES: Slow movement, distant focus",
                    "EYES: Slow, contemplative blinks",
                    "JAW: Relaxed, occasional subtle movement",
                    "MICRO: Very slow noise layer (meditative)",
                    "TRANSITIONS: Smooth between gesture variants"
                  ]
                }
              ],
              "description": "Thoughtful asymmetric pose with weight on one leg and self-reflective gestures.\nMultiple variants: chin rest, crossed arms, akimbo, hand on hip.\n\nFEEL: Lost in thought, processing something, introspective. Like solving a puzzle.\n\nTIMING: Very slow micro-movements. Breath is deep and slow (5-6 seconds).\nThinking gestures are deliberate, not fidgety. Weight shifts are rare and slow.\nEyes have \"distant\" quality - not tracking environment.\n\nASYMMETRY: Weight mostly on one leg. Opposite hip drops. Spine curves.\nArms break symmetry with gesture variant.\n",
              "path": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            "output": {
              "checksPerformed": [
                "Validating 2 input definition(s)",
                "  ✓ Input \"rig\" has valid schema (type: entity.rig)",
                "  ✓ Input \"ctx\" has valid schema (type: entity.motion_context)",
                "Validating 1 output definition(s)",
                "  ✓ Output \"pose\" has valid schema (type: entity.motion_program)",
                "✓ Block has description: \"Thoughtful asymmetric pose with weight on one leg ...\""
              ],
              "inputCount": 2,
              "outputCount": 1,
              "hasDescription": true
            }
          }
        },
        {
          "id": "domain.validation",
          "label": "domain",
          "passed": false,
          "duration": 58529,
          "issues": [
            {
              "type": "error",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "BREATH CYCLE MISMATCH — Block output constraint requires deep slow breaths of 5–6 second cycles, but the implementation uses a breathing frequency of 0.12 Hz (~8.3 s cycle). The documentation header also contains conflicting timings (5–8 s). The code uses oscBreathing(t, 0.12, ...), which does not satisfy the explicit 5–6s requirement.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "BLINK TIMING NOT GUARANTEED — The spec expects slow blinks roughly every 4–5 seconds. In code, blinking is triggered by a noise threshold (noise.noise2D(...) > 0.92) which does not explicitly enforce a 4–5s interval. This may produce different average blink timing than the stated 4–5s.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "PHASE-ENVELOPE REQUIREMENT AMBIGUITY — The Blocks philosophy asks to use overlapping phase envelopes rather than discrete state machines. The implementation uses a discrete 'thinking gesture' state (state.currentGesture) chosen by a deterministic roll, then applies a sine envelope (gestureWeight). Although envelopes are used for blending, the presence of an explicit discrete gesture state is close to a state machine rather than purely overlapping continuous envelopes. Consider using overlapping gesture envelopes or multi-channel blending instead of a single discrete gesture selector to better match the stated requirement.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            },
            {
              "type": "warning",
              "code": "DOMAIN_SEMANTIC_ISSUE",
              "message": "ASSUMED HAS-BONE HANDLING IN applyFingerCurl — The file relies on applyFingerCurl(rig, side, curls) to animate all 15 finger bones per hand and claims it handles hasBone() checks internally. That helper is imported from ../blend and is not shown here. This file does call applyFingerCurl without guarding the call with rig.hasBone checks; validation assumes the helper will do per-bone hasBone checks. If applyFingerCurl does not perform those checks, this would violate the requirement that optional bones are handled gracefully.",
              "file": "packages/motion-dsl/src/motions/contemplative-lean.ts"
            }
          ],
          "context": {
            "filesAnalyzed": [
              "contemplative-lean.ts"
            ],
            "rulesApplied": [],
            "philosophy": [
              "Human-like motion is the primary goal - believability trumps all other concerns",
              "There are no shortcuts to good animation - every detail matters",
              "Motion must feel alive, not just look correct",
              "LLMs lack embodied understanding - every numerical choice must be justified",
              "Real humans telegraph movements before executing (anticipation)",
              "Real humans have follow-through after movements complete",
              "Real humans move on a tight manifold of coordination patterns",
              "Use overlapping phase envelopes, NOT discrete state machines",
              "Shoulder leads elbow, elbow leads wrist, wrist leads fingers",
              "Breath phase couples to chest, shoulders, and subtle head motion",
              "Weight shifts propagate through spine with natural delays",
              "Every motion must consider ALL 69 VRM bones",
              "Core bones (hips, spine, chest, neck, head) are always active",
              "Fingers add life even when not the focus of motion",
              "Eyes and jaw respond to emotional state",
              "Toes grip and respond to weight shifts",
              "Motions must be anatomically accurate and biomechanically sound",
              "Joint rotations must respect anatomical limits",
              "Weight transfer follows physical laws",
              "Counter-rotations maintain balance",
              "Spring dynamics create natural acceleration/deceleration curves",
              "Secondary motion adds organic follow-through",
              "Noise functions add life but must feel intentional, not random",
              "Micro-movements distinguish living from mechanical",
              "Every motion file MUST begin with detailed documentation",
              "Documentation must describe HOW the motion should FEEL",
              "Documentation must specify timing relationships between body parts",
              "Documentation must cite biomechanical or observational research basis",
              "All motions must gracefully handle missing optional bones",
              "Motion quality degrades gracefully, never fails abruptly",
              "hasBone() checks wrap all optional bone access"
            ],
            "summary": "The block is well-documented and largely implements the domain intent: it defines pose variants, arm/hand/finger configurations, asymmetry/weight distribution, breathing, slow gestures with sine envelopes, micro-noise, and extensive hasBone guards. However, it fails a strict breathing-cycle constraint (5–6s required vs 8.3s implemented), and there are a few policy/robustness concerns (blink timing not explicitly enforced, use of a discrete gesture state vs purely overlapping envelopes, and an assumption that applyFingerCurl performs its own hasBone checks). Fixing the breathing rate to match the output constraint and clarifying/adjusting the gesture/blink implementations (or documenting why the current choices meet the intent) will bring the block into compliance."
          },
          "ai": {
            "provider": "openai",
            "model": "gpt-5-mini",
            "prompt": "Block Name: contemplative_lean\n\n\nBLOCKS PHILOSOPHY:\n- Human-like motion is the primary goal - believability trumps all other concerns\n- There are no shortcuts to good animation - every detail matters\n- Motion must feel alive, not just look correct\n- LLMs lack embodied understanding - every numerical choice must be justified\n- Real humans telegraph movements before executing (anticipation)\n- Real humans have follow-through after movements complete\n- Real humans move on a tight manifold of coordination patterns\n- Use overlapping phase envelopes, NOT discrete state machines\n- Shoulder leads elbow, elbow leads wrist, wrist leads fingers\n- Breath phase couples to chest, shoulders, and subtle head motion\n- Weight shifts propagate through spine with natural delays\n- Every motion must consider ALL 69 VRM bones\n- Core bones (hips, spine, chest, neck, head) are always active\n- Fingers add life even when not the focus of motion\n- Eyes and jaw respond to emotional state\n- Toes grip and respond to weight shifts\n- Motions must be anatomically accurate and biomechanically sound\n- Joint rotations must respect anatomical limits\n- Weight transfer follows physical laws\n- Counter-rotations maintain balance\n- Spring dynamics create natural acceleration/deceleration curves\n- Secondary motion adds organic follow-through\n- Noise functions add life but must feel intentional, not random\n- Micro-movements distinguish living from mechanical\n- Every motion file MUST begin with detailed documentation\n- Documentation must describe HOW the motion should FEEL\n- Documentation must specify timing relationships between body parts\n- Documentation must cite biomechanical or observational research basis\n- All motions must gracefully handle missing optional bones\n- Motion quality degrades gracefully, never fails abruptly\n- hasBone() checks wrap all optional bone access\n\nBlock Definition:\n{\n  \"description\": \"Thoughtful asymmetric pose with weight on one leg and self-reflective gestures.\\nMultiple variants: chin rest, crossed arms, akimbo, hand on hip.\\n\\nFEEL: Lost in thought, processing something, introspective. Like solving a puzzle.\\n\\nTIMING: Very slow micro-movements. Breath is deep and slow (5-6 seconds).\\nThinking gestures are deliberate, not fidgety. Weight shifts are rare and slow.\\nEyes have \\\"distant\\\" quality - not tracking environment.\\n\\nASYMMETRY: Weight mostly on one leg. Opposite hip drops. Spine curves.\\nArms break symmetry with gesture variant.\\n\",\n  \"path\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\",\n  \"inputs\": [\n    {\n      \"name\": \"rig\",\n      \"type\": \"entity.rig\"\n    },\n    {\n      \"name\": \"ctx\",\n      \"type\": \"entity.motion_context\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"pose\",\n      \"type\": \"entity.motion_program\",\n      \"constraints\": [\n        \"DOCUMENTATION: Must describe contemplative body language\",\n        \"DOCUMENTATION: Must specify pose variants and their meanings\",\n        \"ASYMMETRY: Clear weight distribution (one leg dominant)\",\n        \"ASYMMETRY: Hip drop on non-weight side\",\n        \"ASYMMETRY: Spine S-curve to balance\",\n        \"VARIANTS: chin_rest, crossed_arms, akimbo, hand_on_hip\",\n        \"VARIANTS: Each has specific arm/hand configuration\",\n        \"BREATH: Deep, slow (5-6 second cycles)\",\n        \"BREATH: Visible chest expansion\",\n        \"GESTURES: Thinking micro-movements (chin tap, lip touch)\",\n        \"GESTURES: Very slow and deliberate\",\n        \"GESTURES: Phase envelopes for gesture timing\",\n        \"HANDS: Detailed finger pose per variant\",\n        \"HANDS: Subtle finger adjustments\",\n        \"LEGS: Weight leg straight-ish, relaxed leg bent\",\n        \"FEET: Weight foot flat, relaxed foot on ball\",\n        \"HEAD: Tilted slightly, distant gaze angle\",\n        \"EYES: Slow movement, distant focus\",\n        \"EYES: Slow, contemplative blinks\",\n        \"JAW: Relaxed, occasional subtle movement\",\n        \"MICRO: Very slow noise layer (meditative)\",\n        \"TRANSITIONS: Smooth between gesture variants\"\n      ]\n    }\n  ]\n}\n\n\nBLOCK FILES:\n\n--- contemplative-lean.ts ---\n```\n/**\n * ============================================================================\n * CONTEMPLATIVE LEAN MOTION\n * ============================================================================\n *\n * Thoughtful asymmetric pose with weight on one leg and thinking gestures.\n * Multiple pose variants: chin rest, crossed arms, akimbo, hand on hip.\n *\n * ============================================================================\n * HOW THIS MOTION SHOULD FEEL\n * ============================================================================\n *\n * Picture a philosopher lost in thought, or someone puzzling over a difficult\n * decision. The weight shifts to one leg, creating an S-curve through the\n * body. The head tilts, eyes distant. The hands find comfortable positions -\n * supporting the chin, crossed protectively, or resting on hips.\n *\n * The key quality is STILLNESS WITH LIFE. The body is essentially static,\n * but small movements reveal the active mind: a head tilt when considering\n * a new angle, fingers tapping the chin while working through logic, eyes\n * drifting to an imagined point while the thought develops. Deep, slow\n * breathing punctuates the contemplation.\n *\n * ============================================================================\n * TIMING RELATIONSHIPS\n * ============================================================================\n *\n * BREATHING: 5-8 seconds per cycle (0.12-0.2 Hz)\n *   - Slower than normal, deeper\n *   - Chest expands 0.025-0.035 rad\n *   - Shoulders rise ~1.2% of breath amplitude\n *   - Full belly breathing, not shallow chest breathing\n *\n * THINKING GESTURES: Every 2-5 seconds\n *   - Gesture types: head_tilt, chin_tap, look_away, idle\n *   - Duration: 2-5 seconds per gesture (noise-varied)\n *   - Intensity envelope: sine wave (smooth in/out)\n *   - Selection: deterministic via seeded noise\n *\n * WEIGHT SHIFTS: Very slow (0.08 Hz noise)\n *   - Subtle hip adjustments ~0.02 rad\n *   - Spring-smoothed (stiffness 30) for glacial transitions\n *   - Never large enough to change overall pose\n *\n * HEAD MOVEMENT:\n *   - Base position: tilted toward weight-bearing side ~0.03 rad\n *   - Micro-movements at 0.15-0.2 Hz\n *   - Spring-smoothed (stiffness 50) for organic lag\n *\n * BLINKING: Every 4-5 seconds (slower than normal)\n *   - Duration: 0.2s (slower, contemplative blink)\n *   - Threshold: 0.92 noise (less frequent than neutral)\n *\n * ============================================================================\n * POSE VARIANTS\n * ============================================================================\n *\n * CHIN_REST: Classic \"thinker\" pose\n *   - Right hand supports chin\n *   - Left arm supports right elbow\n *   - Index finger extended, others curled\n *   - Enables \"chin_tap\" thinking gesture\n *\n * CROSSED_ARMS: Protective/evaluating\n *   - Arms crossed over chest\n *   - Hands tucked under opposite arms\n *   - More closed body language\n *\n * AKIMBO: Confident thinking\n *   - Both hands on hips\n *   - Open body language\n *   - \"Making a decision\" feel\n *\n * HAND_ON_HIP: Casual thinking\n *   - Right hand on hip\n *   - Left arm relaxed at side\n *   - Asymmetric, natural\n *\n * ============================================================================\n * BONE HANDLING\n * ============================================================================\n *\n * All bone access wrapped in hasBone() checks.\n * Dynamic bone names (weightLeg/relaxLeg) are cast and validated.\n *\n * CORE: hips, spine, chest, upperChest, neck, head\n * ARMS: leftShoulder, rightShoulder, leftUpperArm, rightUpperArm,\n *       leftLowerArm, rightLowerArm, leftHand, rightHand\n * FINGERS: all 30 bones via applyFingerCurl helper\n * LEGS: leftUpperLeg, rightUpperLeg, leftLowerLeg, rightLowerLeg\n * FEET: leftFoot, rightFoot, leftToes, rightToes\n * FACE: leftEye, rightEye, jaw\n *\n * ============================================================================\n * RESEARCH BASIS\n * ============================================================================\n *\n * - Pease, A. & Pease, B. (2006): \"The Definitive Book of Body Language\" -\n *   Chin stroking indicates evaluation/decision-making. Crossed arms can\n *   indicate deep thinking or self-protection.\n *\n * - McNeill, D. (1992): \"Hand and Mind\" - Self-touching gestures during\n *   thought are displacement behaviors that aid concentration.\n *\n * - Standing weight distribution: Natural contrapposto has 60-70% weight\n *   on one leg, creating characteristic hip/shoulder line.\n *\n * - Contemplative breathing: 6-8 breaths/min in relaxed thought state\n *   (vs 12-15 normal), corresponding to 0.1-0.13 Hz.\n *\n * ============================================================================\n * NUMERICAL JUSTIFICATIONS\n * ============================================================================\n *\n * hipDrop 0.06 rad = 3.4°: Visible but not exaggerated contrapposto\n * hipShift 0.04 rad = 2.3°: Lateral shift toward weight-bearing leg\n * breathRate 0.12 Hz: 8.3s cycle (deep, slow thinking breath)\n * headTiltZ 0.03 rad = 1.7°: Subtle tilt toward weighted side\n * weightSpring stiffness 30: Very slow, organic weight shifts\n * headSpring stiffness 50: Moderate response for natural movement\n * gestureInterval 2-5s: Based on self-touch research frequency\n * blinkDuration 0.2s: Slower than alert blink (0.15s)\n */\n\nimport { z } from 'zod'\nimport type { MotionProgram, MotionMeta, HumanoidRig, MotionContext, VRMHumanBoneName } from '@posers/core'\nimport {\n  osc,\n  oscBreathing,\n  quatFromAxisAngle,\n  createNoiseGenerator,\n  createSpring,\n  Easing,\n  type NoiseGenerator,\n  type Spring,\n} from '@posers/core'\nimport {\n  applyFingerCurl,\n} from '../blend'\n\n// ============================================================================\n// SCHEMA & TYPES\n// ============================================================================\n\nexport const contemplativeLeanParamsSchema = z.object({\n  /** Pose variant. Default: chin_rest */\n  poseVariant: z.enum(['chin_rest', 'crossed_arms', 'akimbo', 'hand_on_hip']).default('chin_rest'),\n  /** Intensity of thinking gestures (0-1). Default: 0.5 */\n  thoughtIntensity: z.number().min(0).max(1).default(0.5),\n  /** Depth of breathing (0-1). Default: 0.6 */\n  breathDepth: z.number().min(0).max(1).default(0.6),\n  /** Amount of subtle fidgeting (0-1). Default: 0.3 */\n  fidgetAmount: z.number().min(0).max(1).default(0.3),\n  /** Which leg bears weight. Default: right */\n  weightLeg: z.enum(['left', 'right']).default('right'),\n  /** Overall animation intensity (0-1). Default: 0.7 */\n  intensity: z.number().min(0).max(1).default(0.7),\n  /** Enable eye movement. Default: true */\n  eyeMovement: z.boolean().default(true),\n  /** Enable head tilts. Default: true */\n  headTilts: z.boolean().default(true),\n})\n\nexport type ContemplativeLeanParams = z.infer<typeof contemplativeLeanParamsSchema>\nexport type ContemplativeLeanInput = z.input<typeof contemplativeLeanParamsSchema>\n\nexport const contemplativeLeanMeta: MotionMeta = {\n  id: 'contemplative-lean',\n  name: 'Contemplative Lean',\n  description: 'Thoughtful asymmetric pose with weight on one leg and thinking gestures',\n  tags: ['contemplative', 'thinking', 'pose', 'idle'],\n  author: 'posers',\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\n\ninterface ContemplativeLeanState {\n  noise: NoiseGenerator\n  weightSpring: Spring\n  headSpring: Spring\n  armSpring: Spring\n  blinkTimer: number\n  isBlinking: boolean\n  thinkingGestureTimer: number\n  currentGesture: 'idle' | 'head_tilt' | 'chin_tap' | 'look_away'\n  gestureDuration: number\n}\n\nfunction initState(seed: number): ContemplativeLeanState {\n  return {\n    noise: createNoiseGenerator(seed),\n    weightSpring: createSpring({ stiffness: 30, damping: 12 }),\n    headSpring: createSpring({ stiffness: 50, damping: 15 }),\n    armSpring: createSpring({ stiffness: 40, damping: 10 }),\n    blinkTimer: 0,\n    isBlinking: false,\n    thinkingGestureTimer: 0,\n    currentGesture: 'idle',\n    gestureDuration: 3,\n  }\n}\n\n// ============================================================================\n// POSE VARIANTS\n// ============================================================================\n\ninterface ArmPose {\n  leftUpperArm: { x: number; y: number; z: number }\n  leftLowerArm: { x: number; y: number; z: number }\n  leftHand: { x: number; y: number; z: number }\n  rightUpperArm: { x: number; y: number; z: number }\n  rightLowerArm: { x: number; y: number; z: number }\n  rightHand: { x: number; y: number; z: number }\n  leftFingers: { thumb: number; index: number; middle: number; ring: number; little: number }\n  rightFingers: { thumb: number; index: number; middle: number; ring: number; little: number }\n}\n\nfunction getArmPose(variant: string, intensity: number): ArmPose {\n  switch (variant) {\n    case 'chin_rest':\n      return {\n        // Right arm: hand to chin\n        rightUpperArm: { x: -0.8 * intensity, y: 0, z: -0.4 * intensity },\n        rightLowerArm: { x: 0, y: 1.3 * intensity, z: 0 },\n        rightHand: { x: 0.2 * intensity, y: 0, z: 0.1 * intensity },\n        // Left arm: support under right elbow\n        leftUpperArm: { x: 0.3 * intensity, y: 0, z: 0.25 * intensity },\n        leftLowerArm: { x: 0, y: -1.0 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.15 * intensity },\n        // Fingers\n        rightFingers: { thumb: 0.2, index: 0.15, middle: 0.35, ring: 0.5, little: 0.55 },\n        leftFingers: { thumb: 0.3, index: 0.35, middle: 0.4, ring: 0.45, little: 0.5 },\n      }\n\n    case 'crossed_arms':\n      return {\n        rightUpperArm: { x: 0.5 * intensity, y: 0, z: -0.35 * intensity },\n        rightLowerArm: { x: 0, y: 1.4 * intensity, z: 0 },\n        rightHand: { x: 0, y: 0, z: -0.2 * intensity },\n        leftUpperArm: { x: 0.4 * intensity, y: 0, z: 0.25 * intensity },\n        leftLowerArm: { x: 0, y: -1.3 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.15 * intensity },\n        rightFingers: { thumb: 0.2, index: 0.25, middle: 0.3, ring: 0.35, little: 0.4 },\n        leftFingers: { thumb: 0.25, index: 0.3, middle: 0.35, ring: 0.4, little: 0.45 },\n      }\n\n    case 'akimbo':\n      return {\n        rightUpperArm: { x: 0.15 * intensity, y: 0, z: -0.5 * intensity },\n        rightLowerArm: { x: 0, y: 1.4 * intensity, z: 0 },\n        rightHand: { x: -0.3 * intensity, y: 0, z: 0 },\n        leftUpperArm: { x: 0.15 * intensity, y: 0, z: 0.5 * intensity },\n        leftLowerArm: { x: 0, y: -1.4 * intensity, z: 0 },\n        leftHand: { x: -0.3 * intensity, y: 0, z: 0 },\n        rightFingers: { thumb: 0.1, index: 0.15, middle: 0.2, ring: 0.25, little: 0.3 },\n        leftFingers: { thumb: 0.1, index: 0.15, middle: 0.2, ring: 0.25, little: 0.3 },\n      }\n\n    case 'hand_on_hip':\n      return {\n        // Right hand on hip\n        rightUpperArm: { x: 0.2 * intensity, y: 0, z: -0.45 * intensity },\n        rightLowerArm: { x: 0, y: 1.3 * intensity, z: 0 },\n        rightHand: { x: -0.25 * intensity, y: 0, z: -0.1 * intensity },\n        // Left arm relaxed\n        leftUpperArm: { x: 0.05 * intensity, y: 0, z: 0.12 * intensity },\n        leftLowerArm: { x: 0, y: -0.15 * intensity, z: 0 },\n        leftHand: { x: 0, y: 0, z: 0.1 * intensity },\n        rightFingers: { thumb: 0.15, index: 0.2, middle: 0.25, ring: 0.3, little: 0.35 },\n        leftFingers: { thumb: 0.25, index: 0.3, middle: 0.35, ring: 0.4, little: 0.45 },\n      }\n\n    default:\n      return getArmPose('chin_rest', intensity)\n  }\n}\n\n// ============================================================================\n// MOTION IMPLEMENTATION\n// ============================================================================\n\n/**\n * Creates a contemplative lean motion with multiple pose variants.\n *\n * BONE HANDLING STRATEGY:\n * - Every bone rotation is guarded by rig.hasBone() checks\n * - Missing optional bones (jaw, eyes, toes, fingers) are gracefully skipped\n * - Dynamic bone names (weightLeg, relaxLeg) are cast and checked\n * - The animation degrades gracefully when bones are unavailable\n *\n * DETERMINISM:\n * - All noise functions are seeded from ctx.seed\n * - Same seed produces identical animation every time\n * - Gesture selection is deterministic via seeded noise\n *\n * PERFORMANCE:\n * - All computations are O(1) per frame\n * - Spring physics use efficient semi-implicit Euler integration\n * - Pose variant lookup is constant time\n * - No allocations in hot path except quaternion creation\n *\n * TRANSITIONS:\n * - Thinking gesture state machine provides fluid transitions\n * - Gesture weights use sine easing for smooth in/out\n * - Springs smooth all micro-movements\n */\nexport function createContemplativeLean(params: ContemplativeLeanInput = {}): MotionProgram<ContemplativeLeanParams> {\n  const validatedParams = contemplativeLeanParamsSchema.parse(params)\n  let state: ContemplativeLeanState | null = null\n\n  return {\n    meta: contemplativeLeanMeta,\n    paramsSchema: contemplativeLeanParamsSchema,\n\n    init(_rig: HumanoidRig, ctx: MotionContext): void {\n      state = initState(ctx.seed)\n    },\n\n    update(rig: HumanoidRig, ctx: MotionContext, t: number, dt: number): void {\n      if (!state) {\n        state = initState(ctx.seed)\n      }\n\n      const {\n        poseVariant,\n        thoughtIntensity,\n        breathDepth,\n        fidgetAmount,\n        weightLeg,\n        intensity,\n        eyeMovement,\n        headTilts,\n      } = validatedParams\n\n      const noise = state.noise\n      const isRightWeight = weightLeg === 'right'\n      const weightSide = isRightWeight ? 1 : -1\n\n      // ========================================\n      // THINKING GESTURE STATE MACHINE\n      // ========================================\n\n      state.thinkingGestureTimer += dt\n      if (state.thinkingGestureTimer > state.gestureDuration) {\n        state.thinkingGestureTimer = 0\n        state.gestureDuration = 2 + noise.noise2D(t, 50) * 3\n\n        // Pick next gesture\n        const gestureRoll = noise.noise2D(t, 100)\n        if (gestureRoll > 0.7 && headTilts) {\n          state.currentGesture = 'head_tilt'\n        } else if (gestureRoll > 0.4 && poseVariant === 'chin_rest') {\n          state.currentGesture = 'chin_tap'\n        } else if (gestureRoll > 0.2) {\n          state.currentGesture = 'look_away'\n        } else {\n          state.currentGesture = 'idle'\n        }\n      }\n\n      const gestureProgress = state.thinkingGestureTimer / state.gestureDuration\n      const gestureWeight = Math.sin(gestureProgress * Math.PI) * thoughtIntensity\n\n      // ========================================\n      // LAYER 1: ASYMMETRIC WEIGHT DISTRIBUTION\n      // ========================================\n\n      // Hip drop on non-weight-bearing side\n      const hipDrop = 0.06 * intensity * weightSide\n      const hipShift = 0.04 * intensity * weightSide\n\n      // Subtle weight shift noise\n      const weightNoise = noise.noise2D(t * 0.08, 200) * fidgetAmount * 0.02\n      state.weightSpring.setTarget(hipShift + weightNoise)\n      state.weightSpring.update(dt)\n\n      if (rig.hasBone('hips')) {\n        const hipsRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, hipDrop + state.weightSpring.value)\n        hipsRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightSide * 0.03 * intensity))\n        rig.setRotation('hips', hipsRot)\n      }\n\n      // ========================================\n      // LAYER 2: SPINE (RELAXED, SLIGHT CURVE)\n      // ========================================\n\n      if (rig.hasBone('spine')) {\n        const spineRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.3)\n        spineRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.03 * intensity))\n        rig.setRotation('spine', spineRot)\n      }\n\n      if (rig.hasBone('chest')) {\n        const chestRot = quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.2)\n        chestRot.multiply(quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n        rig.setRotation('chest', chestRot)\n      }\n\n      if (rig.hasBone('upperChest')) {\n        rig.setRotation('upperChest', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -hipDrop * 0.1))\n      }\n\n      // ========================================\n      // LAYER 3: DEEP CONTEMPLATIVE BREATHING\n      // ========================================\n\n      const breathPhase = oscBreathing(t, 0.12, breathDepth)\n\n      if (rig.hasBone('chest')) {\n        rig.addRotation('chest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.025))\n      }\n      if (rig.hasBone('upperChest')) {\n        rig.addRotation('upperChest', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, breathPhase * 0.035))\n      }\n\n      // Shoulder rise with breath\n      const shoulderBreath = breathPhase * 0.012\n      if (rig.hasBone('leftShoulder')) {\n        rig.setRotation('leftShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -shoulderBreath))\n      }\n      if (rig.hasBone('rightShoulder')) {\n        rig.setRotation('rightShoulder', quatFromAxisAngle({ x: 0, y: 0, z: 1 }, shoulderBreath))\n      }\n\n      // ========================================\n      // LAYER 4: LEGS (WEIGHT DISTRIBUTION)\n      // ========================================\n\n      // Weight-bearing leg: straight\n      // Non-weight leg: bent, relaxed\n\n      const weightLegBone = isRightWeight ? 'rightUpperLeg' : 'leftUpperLeg'\n      const relaxLegBone = isRightWeight ? 'leftUpperLeg' : 'rightUpperLeg'\n      const weightKnee = isRightWeight ? 'rightLowerLeg' : 'leftLowerLeg'\n      const relaxKnee = isRightWeight ? 'leftLowerLeg' : 'rightLowerLeg'\n      const weightFoot = isRightWeight ? 'rightFoot' : 'leftFoot'\n      const relaxFoot = isRightWeight ? 'leftFoot' : 'rightFoot'\n\n      if (rig.hasBone(weightLegBone as VRMHumanBoneName)) {\n        // Slight hip flexion on weight-bearing side\n        rig.setRotation(weightLegBone as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.02 * intensity))\n      }\n\n      if (rig.hasBone(relaxLegBone as VRMHumanBoneName)) {\n        // Relaxed leg forward and slightly bent\n        const relaxRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.15 * intensity)\n        relaxRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, -weightSide * 0.08 * intensity))\n        rig.setRotation(relaxLegBone as VRMHumanBoneName, relaxRot)\n      }\n\n      if (rig.hasBone(weightKnee as VRMHumanBoneName)) {\n        // Straight-ish weight-bearing knee\n        rig.setRotation(weightKnee as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.03 * intensity))\n      }\n\n      if (rig.hasBone(relaxKnee as VRMHumanBoneName)) {\n        // Bent relaxed knee\n        rig.setRotation(relaxKnee as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.2 * intensity))\n      }\n\n      // Feet\n      if (rig.hasBone(weightFoot as VRMHumanBoneName)) {\n        rig.setRotation(weightFoot as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 0, y: 1, z: 0 }, weightSide * 0.08))\n      }\n\n      if (rig.hasBone(relaxFoot as VRMHumanBoneName)) {\n        // Relaxed foot on ball/toe\n        const relaxFootRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.15 * intensity)\n        relaxFootRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, -weightSide * 0.1))\n        rig.setRotation(relaxFoot as VRMHumanBoneName, relaxFootRot)\n      }\n\n      // Toes\n      const weightToes = isRightWeight ? 'rightToes' : 'leftToes'\n      const relaxToes = isRightWeight ? 'leftToes' : 'rightToes'\n\n      if (rig.hasBone(weightToes as VRMHumanBoneName)) {\n        rig.setRotation(weightToes as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, 0.05))\n      }\n      if (rig.hasBone(relaxToes as VRMHumanBoneName)) {\n        rig.setRotation(relaxToes as VRMHumanBoneName,\n          quatFromAxisAngle({ x: 1, y: 0, z: 0 }, -0.1 * intensity))\n      }\n\n      // ========================================\n      // LAYER 5: ARMS (POSE VARIANT)\n      // ========================================\n\n      const armPose = getArmPose(poseVariant, intensity)\n\n      // Add subtle arm movement/fidget\n      const armFidget = noise.noise2D(t * 0.15, 300) * fidgetAmount * 0.02\n      state.armSpring.setTarget(armFidget)\n      state.armSpring.update(dt)\n      const armNoise = state.armSpring.value\n\n      if (rig.hasBone('leftUpperArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftUpperArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftUpperArm.y + armNoise))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftUpperArm.z))\n        rig.setRotation('leftUpperArm', rot)\n      }\n\n      if (rig.hasBone('leftLowerArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftLowerArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftLowerArm.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftLowerArm.z))\n        rig.setRotation('leftLowerArm', rot)\n      }\n\n      if (rig.hasBone('leftHand')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.leftHand.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.leftHand.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.leftHand.z))\n        rig.setRotation('leftHand', rot)\n      }\n\n      if (rig.hasBone('rightUpperArm')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightUpperArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightUpperArm.y - armNoise))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightUpperArm.z))\n        rig.setRotation('rightUpperArm', rot)\n      }\n\n      if (rig.hasBone('rightLowerArm')) {\n        // Add chin tap gesture for chin_rest variant\n        let chinTapAdd = 0\n        if (poseVariant === 'chin_rest' && state.currentGesture === 'chin_tap') {\n          chinTapAdd = Math.sin(state.thinkingGestureTimer * 6) * 0.05 * gestureWeight\n        }\n\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightLowerArm.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightLowerArm.y + chinTapAdd))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightLowerArm.z))\n        rig.setRotation('rightLowerArm', rot)\n      }\n\n      if (rig.hasBone('rightHand')) {\n        const rot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, armPose.rightHand.x)\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, armPose.rightHand.y))\n        rot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, armPose.rightHand.z))\n        rig.setRotation('rightHand', rot)\n      }\n\n      // ========================================\n      // LAYER 5B: DETAILED FINGER ANIMATION\n      // ========================================\n      // Animate all 30 finger bones (15 per hand) with pose-specific positioning\n      // Each finger has 3 joints: Proximal, Intermediate, Distal\n      // Bones: thumb/index/middle/ring/little + Proximal/Intermediate/Distal\n      //\n      // Apply base finger curl from pose, plus subtle micro-movements\n\n      // Add subtle finger fidget for organic feel\n      const fingerFidget = noise.noise2D(t * 0.25, 1100) * fidgetAmount * 0.1\n\n      // Left hand finger bones - explicit animation of all joints\n      const leftFingerCurl = {\n        thumb: armPose.leftFingers.thumb + fingerFidget * 0.3,\n        index: armPose.leftFingers.index + fingerFidget * 0.5,\n        middle: armPose.leftFingers.middle + fingerFidget * 0.6,\n        ring: armPose.leftFingers.ring + fingerFidget * 0.7,\n        little: armPose.leftFingers.little + fingerFidget * 0.8,\n      }\n\n      // Right hand finger bones - with thinking gesture adjustments\n      const thinkingFingerMod = state.currentGesture === 'chin_tap' ? gestureWeight * 0.1 : 0\n      const rightFingerCurl = {\n        thumb: armPose.rightFingers.thumb + thinkingFingerMod,\n        index: armPose.rightFingers.index + thinkingFingerMod * 1.5, // Index finger more active in chin tap\n        middle: armPose.rightFingers.middle + thinkingFingerMod * 0.8,\n        ring: armPose.rightFingers.ring + fingerFidget * 0.5,\n        little: armPose.rightFingers.little + fingerFidget * 0.6,\n      }\n\n      // Apply finger curls - this internally handles all 15 bones per hand\n      // with hasBone() checks for each: Proximal, Intermediate, Distal joints\n      applyFingerCurl(rig, 'left', leftFingerCurl)\n      applyFingerCurl(rig, 'right', rightFingerCurl)\n\n      // ========================================\n      // LAYER 6: HEAD & NECK (THINKING)\n      // ========================================\n\n      let headTiltX = 0.02 * intensity // Slight down\n      let headTiltY = 0\n      let headTiltZ = 0.03 * intensity * weightSide // Tilt toward weight side\n\n      // Add thinking gestures\n      if (state.currentGesture === 'head_tilt') {\n        headTiltZ += gestureWeight * 0.05 * (noise.noise2D(t, 400) > 0 ? 1 : -1)\n        headTiltX += gestureWeight * 0.03\n      } else if (state.currentGesture === 'look_away') {\n        headTiltY = gestureWeight * 0.1 * (noise.noise2D(t, 500) > 0 ? 1 : -1)\n      }\n\n      // Micro-movement\n      const headMicroX = noise.noise2D(t * 0.2, 600) * 0.01 * intensity\n      const headMicroY = noise.noise2D(t * 0.15, 700) * 0.015 * intensity\n\n      state.headSpring.setTarget(headTiltY + headMicroY)\n      state.headSpring.update(dt)\n\n      if (rig.hasBone('head')) {\n        const headRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX + headMicroX)\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, state.headSpring.value))\n        headRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headTiltZ))\n        rig.setRotation('head', headRot)\n      }\n\n      if (rig.hasBone('neck')) {\n        const neckRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, headTiltX * 0.5)\n        neckRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, state.headSpring.value * 0.4))\n        neckRot.multiply(quatFromAxisAngle({ x: 0, y: 0, z: 1 }, headTiltZ * 0.3))\n        rig.setRotation('neck', neckRot)\n      }\n\n      // ========================================\n      // LAYER 7: EYES (CONTEMPLATIVE GAZE)\n      // ========================================\n\n      if (eyeMovement) {\n        // Slow, distant gaze with occasional focus shifts\n        let eyeX = 0\n        let eyeY = 0.02 * intensity // Slight downward gaze\n\n        if (state.currentGesture === 'look_away') {\n          eyeX = gestureWeight * 0.1 * (noise.noise2D(t, 800) > 0 ? 1 : -1)\n        } else {\n          eyeX = noise.noise2D(t * 0.1, 900) * 0.04 * intensity\n          eyeY += noise.noise2D(t * 0.08, 1000) * 0.02 * intensity\n        }\n\n        // Slow blinking (contemplative)\n        const blinkChance = noise.noise2D(t * 0.2, 1100)\n        if (!state.isBlinking && blinkChance > 0.92) {\n          state.isBlinking = true\n          state.blinkTimer = 0\n        }\n\n        let blinkAmount = 0\n        if (state.isBlinking) {\n          state.blinkTimer += dt\n          const blinkDuration = 0.2 // Slower, contemplative blink\n          if (state.blinkTimer > blinkDuration) {\n            state.isBlinking = false\n          } else {\n            blinkAmount = Math.sin(state.blinkTimer / blinkDuration * Math.PI) * 0.25\n          }\n        }\n\n        if (rig.hasBone('leftEye')) {\n          const leftEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n          leftEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('leftEye', leftEyeRot)\n        }\n\n        if (rig.hasBone('rightEye')) {\n          const rightEyeRot = quatFromAxisAngle({ x: 1, y: 0, z: 0 }, eyeY - blinkAmount)\n          rightEyeRot.multiply(quatFromAxisAngle({ x: 0, y: 1, z: 0 }, eyeX))\n          rig.setRotation('rightEye', rightEyeRot)\n        }\n      }\n\n      // ========================================\n      // LAYER 8: JAW (OCCASIONAL SUBTLE MOVEMENT)\n      // ========================================\n\n      if (rig.hasBone('jaw')) {\n        // Very subtle jaw movement as if thinking words\n        const jawThink = noise.noise2D(t * 0.3, 1200) > 0.85\n          ? noise.noise2D(t * 2, 1300) * 0.02 * thoughtIntensity\n          : 0\n        rig.setRotation('jaw', quatFromAxisAngle({ x: 1, y: 0, z: 0 }, jawThink))\n      }\n    },\n  }\n}\n\nexport const contemplativeLean: MotionProgram<ContemplativeLeanParams> = createContemplativeLean({})\n\n```\n\n\nVALIDATION TASK:\nAnalyze ALL files together to determine if this block:\n1. Expresses domain intent clearly in source code\n2. Uses specified inputs/outputs correctly\n3. Adheres to all domain rules\n4. For templates: Check if template SOURCE contains semantic HTML, ARIA labels, media queries, heading hierarchy\n5. Does NOT introduce undocumented concepts\n\nReturn validation issues with specific file references where possible.\nAlso provide a brief summary explaining why the block passed or failed validation.",
            "response": "{\n  \"isValid\": false,\n  \"issues\": [\n    {\n      \"message\": \"BREATH CYCLE MISMATCH — Block output constraint requires deep slow breaths of 5–6 second cycles, but the implementation uses a breathing frequency of 0.12 Hz (~8.3 s cycle). The documentation header also contains conflicting timings (5–8 s). The code uses oscBreathing(t, 0.12, ...), which does not satisfy the explicit 5–6s requirement.\",\n      \"severity\": \"error\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"BLINK TIMING NOT GUARANTEED — The spec expects slow blinks roughly every 4–5 seconds. In code, blinking is triggered by a noise threshold (noise.noise2D(...) > 0.92) which does not explicitly enforce a 4–5s interval. This may produce different average blink timing than the stated 4–5s.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"PHASE-ENVELOPE REQUIREMENT AMBIGUITY — The Blocks philosophy asks to use overlapping phase envelopes rather than discrete state machines. The implementation uses a discrete 'thinking gesture' state (state.currentGesture) chosen by a deterministic roll, then applies a sine envelope (gestureWeight). Although envelopes are used for blending, the presence of an explicit discrete gesture state is close to a state machine rather than purely overlapping continuous envelopes. Consider using overlapping gesture envelopes or multi-channel blending instead of a single discrete gesture selector to better match the stated requirement.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    },\n    {\n      \"message\": \"ASSUMED HAS-BONE HANDLING IN applyFingerCurl — The file relies on applyFingerCurl(rig, side, curls) to animate all 15 finger bones per hand and claims it handles hasBone() checks internally. That helper is imported from ../blend and is not shown here. This file does call applyFingerCurl without guarding the call with rig.hasBone checks; validation assumes the helper will do per-bone hasBone checks. If applyFingerCurl does not perform those checks, this would violate the requirement that optional bones are handled gracefully.\",\n      \"severity\": \"warning\",\n      \"file\": \"packages/motion-dsl/src/motions/contemplative-lean.ts\"\n    }\n  ],\n  \"summary\": \"The block is well-documented and largely implements the domain intent: it defines pose variants, arm/hand/finger configurations, asymmetry/weight distribution, breathing, slow gestures with sine envelopes, micro-noise, and extensive hasBone guards. However, it fails a strict breathing-cycle constraint (5–6s required vs 8.3s implemented), and there are a few policy/robustness concerns (blink timing not explicitly enforced, use of a discrete gesture state vs purely overlapping envelopes, and an assumption that applyFingerCurl performs its own hasBone checks). Fixing the breathing rate to match the output constraint and clarifying/adjusting the gesture/blink implementations (or documenting why the current choices meet the intent) will bring the block into compliance.\"\n}",
            "tokensUsed": {
              "input": 9267,
              "output": 2947
            }
          }
        }
      ],
      "cache": {
        "decision": "Force mode: running all validators",
        "skippedValidators": 0,
        "revalidationReason": "force_flag"
      }
    }
  ]
}